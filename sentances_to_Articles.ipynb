{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf8b91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from urllib.parse import unquote\n",
    "\n",
    "print(\"üì¶ Imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc945178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Qdrant client connected\n",
      "‚úÖ Model loaded on cuda\n",
      "üìä Existing collection 'wikipedia_fr': 30,208 articles\n"
     ]
    }
   ],
   "source": [
    "# Initialize Qdrant client (same as your existing setup)\n",
    "client = QdrantClient(host=\"localhost\", port=6333, prefer_grpc=True)\n",
    "\n",
    "# Load sentence transformer model (same model you're already using)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"models/multilingual-e5-large\", device=device)\n",
    "\n",
    "print(f\"‚úÖ Qdrant client connected\")\n",
    "print(f\"‚úÖ Model loaded on {device}\")\n",
    "\n",
    "# Check existing collection\n",
    "collection_name = \"wikipedia_fr\"\n",
    "total = client.count(collection_name=collection_name, exact=True).count\n",
    "print(f\"üìä Existing collection '{collection_name}': {total:,} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7103dc",
   "metadata": {},
   "source": [
    "Load the Dataset as polars dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82c5bc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Dataset loaded:\n",
      "   Total articles: 2,556,402\n",
      "   Articles with links: 2,556,402\n",
      "\n",
      "   Link structure example:\n",
      "   shape: (3,)\n",
      "Series: '' [struct[5]]\n",
      "[\n",
      "\t{\"https://fr.wikipedia.org/wiki/alg%C3%A8bre\",2,\"alg√®bre\",\"alg%C3%A8bre\",\"alg√®bre\"}\n",
      "\t{\"https://fr.wikipedia.org/wiki/math%C3%A9matiques\",100,\"math√©matiques\",\"math%C3%A9matiques\",\"math√©matiques\"}\n",
      "\t{\"https://fr.wikipedia.org/wiki/structure%20alg%C3%A9brique\",200,\"structures alg√©briques\",\"structure%20alg%C3%A9brique\",\"structure alg√©brique\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Load the merged dataset\n",
    "df = pl.scan_parquet(\"articles_fr_merged.parquet\").filter(\n",
    "    pl.col('link_count') > 0\n",
    ").collect()\n",
    "\n",
    "print(f\"üìö Dataset loaded:\")\n",
    "print(f\"   Total articles: {len(df):,}\")\n",
    "print(f\"   Articles with links: {df.filter(pl.col('link_count') > 0).height:,}\")\n",
    "print(f\"\\n   Link structure example:\")\n",
    "print(f\"   {df.select('links').to_series()[2][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449edfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (100, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>title</th><th>text</th><th>links</th><th>link_count</th><th>text_withoutHref</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>list[struct[5]]</td><td>u32</td><td>str</td></tr></thead><tbody><tr><td>3</td><td>&quot;Antoine Meillet&quot;</td><td>&quot;Antoine Meillet, n√© le √† &amp;lt;a‚Ä¶</td><td>[{&quot;https://fr.wikipedia.org/wiki/Moulins%20%28Allier%29&quot;,25,&quot;Moulins&quot;,&quot;Moulins%20%28Allier%29&quot;,&quot;Moulins (Allier)&quot;}, {&quot;https://fr.wikipedia.org/wiki/Allier%20%28d%C3%A9partement%29&quot;,83,&quot;Allier&quot;,&quot;Allier%20%28d%C3%A9partement%29&quot;,&quot;Allier (d√©partement)&quot;}, ‚Ä¶ {&quot;https://fr.wikipedia.org/wiki/Albert%20Lord&quot;,6955,&quot;Albert Lord&quot;,&quot;Albert%20Lord&quot;,&quot;Albert Lord&quot;}]</td><td>65</td><td>&quot;Antoine Meillet, n√© le √† Mouli‚Ä¶</td></tr><tr><td>7</td><td>&quot;Alg√®bre lin√©aire&quot;</td><td>&quot;L‚Äôalg√®bre lin√©aire est la bran‚Ä¶</td><td>[{&quot;https://fr.wikipedia.org/wiki/math%C3%A9matiques&quot;,38,&quot;math√©matiques&quot;,&quot;math%C3%A9matiques&quot;,&quot;math√©matiques&quot;}, {&quot;https://fr.wikipedia.org/wiki/Espace%20vectoriel&quot;,117,&quot;espaces vectoriels&quot;,&quot;Espace%20vectoriel&quot;,&quot;Espace vectoriel&quot;}, ‚Ä¶ {&quot;https://fr.wikipedia.org/wiki/Diagonalisation&quot;,17018,&quot;diagonalisables&quot;,&quot;Diagonalisation&quot;,&quot;Diagonalisation&quot;}]</td><td>111</td><td>&quot;L‚Äôalg√®bre lin√©aire est la bran‚Ä¶</td></tr><tr><td>9</td><td>&quot;Alg√®bre g√©n√©rale&quot;</td><td>&quot;L&#x27;&amp;lt;a href=&quot;alg%C3%A8bre&quot;&amp;gt‚Ä¶</td><td>[{&quot;https://fr.wikipedia.org/wiki/alg%C3%A8bre&quot;,2,&quot;alg√®bre&quot;,&quot;alg%C3%A8bre&quot;,&quot;alg√®bre&quot;}, {&quot;https://fr.wikipedia.org/wiki/math%C3%A9matiques&quot;,100,&quot;math√©matiques&quot;,&quot;math%C3%A9matiques&quot;,&quot;math√©matiques&quot;}, ‚Ä¶ {&quot;https://fr.wikipedia.org/wiki/dernier%20th%C3%A9or%C3%A8me%20de%20Fermat&quot;,2686,&quot;dernier th√©or√®me de Fermat&quot;,&quot;dernier%20th%C3%A9or%C3%A8me%20de%20Fermat&quot;,&quot;dernier th√©or√®me de Fermat&quot;}]</td><td>15</td><td>&quot;L&#x27;alg√®bre g√©n√©rale, ou alg√®bre‚Ä¶</td></tr><tr><td>10</td><td>&quot;Algorithmique&quot;</td><td>&quot;L&#x27;algorithmique est l&#x27;√©tude et‚Ä¶</td><td>[{&quot;https://fr.wikipedia.org/wiki/algorithme&quot;,127,&quot;algorithme&quot;,&quot;algorithme&quot;,&quot;algorithme&quot;}, {&quot;https://fr.wikipedia.org/wiki/probl%C3%A8me%20algorithmique&quot;,307,&quot;probl√®me algorithmique&quot;,&quot;probl%C3%A8me%20algorithmique&quot;,&quot;probl√®me algorithmique&quot;}, ‚Ä¶ {&quot;https://fr.wikipedia.org/wiki/liste%20des%20algorithmes&quot;,18655,&quot;liste des algorithmes&quot;,&quot;liste%20des%20algorithmes&quot;,&quot;liste des algorithmes&quot;}]</td><td>101</td><td>&quot;L&#x27;algorithmique est l&#x27;√©tude et‚Ä¶</td></tr><tr><td>11</td><td>&quot;Politique en Argentine&quot;</td><td>&quot;L&#x27;&amp;lt;a href=&quot;Argentine&quot;&amp;gt;Ar‚Ä¶</td><td>[{&quot;https://fr.wikipedia.org/wiki/Argentine&quot;,2,&quot;Argentine&quot;,&quot;Argentine&quot;,&quot;Argentine&quot;}, {&quot;https://fr.wikipedia.org/wiki/r%C3%A9publique&quot;,56,&quot;r√©publique&quot;,&quot;r%C3%A9publique&quot;,&quot;r√©publique&quot;}, ‚Ä¶ {&quot;https://fr.wikipedia.org/wiki/%C3%A9lections%20g%C3%A9n%C3%A9rales%20argentines%20de%202007&quot;,12114,&quot;cette derni√®re&quot;,&quot;%C3%A9lections%20g%C3%A9n%C3%A9rales%20argentines%20de%202007&quot;,&quot;√©lections g√©n√©rales argentines de 2007&quot;}]</td><td>78</td><td>&quot;L&#x27;Argentine est une r√©publique‚Ä¶</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>201</td><td>&quot;Arthur John Evans&quot;</td><td>&quot;Arthur John Evans (n√© le √† &amp;lt‚Ä¶</td><td>[{&quot;https://fr.wikipedia.org/wiki/Nash%20Mills&quot;,27,&quot;Nash Mills&quot;,&quot;Nash%20Mills&quot;,&quot;Nash Mills&quot;}, {&quot;https://fr.wikipedia.org/wiki/Hertfordshire&quot;,85,&quot;Hertfordshire&quot;,&quot;Hertfordshire&quot;,&quot;Hertfordshire&quot;}, ‚Ä¶ {&quot;https://fr.wikipedia.org/wiki/Knight%20Bachelor&quot;,3130,&quot;chevalier&quot;,&quot;Knight%20Bachelor&quot;,&quot;Knight Bachelor&quot;}]</td><td>26</td><td>&quot;Arthur John Evans (n√© le √† Nas‚Ä¶</td></tr><tr><td>206</td><td>&quot;Alfred Nobel&quot;</td><td>&quot;Alfred Bernhard Nobel /&#x27;alfr…ôd‚Ä¶</td><td>[{&quot;https://fr.wikipedia.org/wiki/Stockholm&quot;,59,&quot;Stockholm&quot;,&quot;Stockholm&quot;,&quot;Stockholm&quot;}, {&quot;https://fr.wikipedia.org/wiki/Su%C3%A8de&quot;,108,&quot;Su√®de&quot;,&quot;Su%C3%A8de&quot;,&quot;Su√®de&quot;}, ‚Ä¶ {&quot;https://fr.wikipedia.org/wiki/Cimeti%C3%A8re%20du%20Nord%20%28Solna%29&quot;,10712,&quot;cimeti√®re du Nord&quot;,&quot;Cimeti%C3%A8re%20du%20Nord%20%28Solna%29&quot;,&quot;Cimeti√®re du Nord (Solna)&quot;}]</td><td>87</td><td>&quot;Alfred Bernhard Nobel /&#x27;alfr…ôd‚Ä¶</td></tr><tr><td>208</td><td>&quot;Alc√®ne&quot;</td><td>&quot;Les alc√®nes sont des &amp;lt;a hre‚Ä¶</td><td>[{&quot;https://fr.wikipedia.org/wiki/hydrocarbure&quot;,21,&quot;hydrocarbure&quot;,&quot;hydrocarbure&quot;,&quot;hydrocarbure&quot;}, {&quot;https://fr.wikipedia.org/wiki/Compos%C3%A9%20insatur%C3%A9&quot;,74,&quot;insatur√©s&quot;,&quot;Compos%C3%A9%20insatur%C3%A9&quot;,&quot;Compos√© insatur√©&quot;}, ‚Ä¶ {&quot;https://fr.wikipedia.org/wiki/cyclohex%C3%A8ne&quot;,9887,&quot;cyclohex√®ne&quot;,&quot;cyclohex%C3%A8ne&quot;,&quot;cyclohex√®ne&quot;}]</td><td>63</td><td>&quot;Les alc√®nes sont des hydrocarb‚Ä¶</td></tr><tr><td>210</td><td>&quot;Ange (homonymie)&quot;</td><td>&quot;&amp;lt;templatestyles src=&quot;Autres‚Ä¶</td><td>[{&quot;https://fr.wikipedia.org/wiki/ange&quot;,95,&quot;ange&quot;,&quot;ange&quot;,&quot;ange&quot;}, {&quot;https://fr.wikipedia.org/wiki/Dieu&quot;,165,&quot;Dieu&quot;,&quot;Dieu&quot;,&quot;Dieu&quot;}, ‚Ä¶ {&quot;https://fr.wikipedia.org/wiki/grec%20ancien&quot;,339,&quot;grec&quot;,&quot;grec%20ancien&quot;,&quot;grec ancien&quot;}]</td><td>5</td><td>&quot;&amp;lt;templatestyles src=&quot;Autres‚Ä¶</td></tr><tr><td>213</td><td>&quot;Amstrad CPC 464&quot;</td><td>&quot;L&#x27;Amstrad CPC 464 est un &amp;lt;a‚Ä¶</td><td>[{&quot;https://fr.wikipedia.org/wiki/ordinateur%20personnel&quot;,25,&quot;ordinateur personnel&quot;,&quot;ordinateur%20personnel&quot;,&quot;ordinateur personnel&quot;}, {&quot;https://fr.wikipedia.org/wiki/Amstrad%20CPC&quot;,119,&quot;Amstrad CPC&quot;,&quot;Amstrad%20CPC&quot;,&quot;Amstrad CPC&quot;}, ‚Ä¶ {&quot;https://fr.wikipedia.org/wiki/RS-232&quot;,8065,&quot;RS-232&quot;,&quot;RS-232&quot;,&quot;RS-232&quot;}]</td><td>36</td><td>&quot;L&#x27;Amstrad CPC 464 est un ordin‚Ä¶</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (100, 6)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ id  ‚îÜ title             ‚îÜ text              ‚îÜ links             ‚îÜ link_count ‚îÜ text_withoutHref  ‚îÇ\n",
       "‚îÇ --- ‚îÜ ---               ‚îÜ ---               ‚îÜ ---               ‚îÜ ---        ‚îÜ ---               ‚îÇ\n",
       "‚îÇ i64 ‚îÜ str               ‚îÜ str               ‚îÜ list[struct[5]]   ‚îÜ u32        ‚îÜ str               ‚îÇ\n",
       "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
       "‚îÇ 3   ‚îÜ Antoine Meillet   ‚îÜ Antoine Meillet,  ‚îÜ [{\"https://fr.wik ‚îÜ 65         ‚îÜ Antoine Meillet,  ‚îÇ\n",
       "‚îÇ     ‚îÜ                   ‚îÜ n√© le √† &lt;a‚Ä¶    ‚îÜ ipedia.org/wi‚Ä¶    ‚îÜ            ‚îÜ n√© le √† Mouli‚Ä¶    ‚îÇ\n",
       "‚îÇ 7   ‚îÜ Alg√®bre lin√©aire  ‚îÜ L‚Äôalg√®bre         ‚îÜ [{\"https://fr.wik ‚îÜ 111        ‚îÜ L‚Äôalg√®bre         ‚îÇ\n",
       "‚îÇ     ‚îÜ                   ‚îÜ lin√©aire est la   ‚îÜ ipedia.org/wi‚Ä¶    ‚îÜ            ‚îÜ lin√©aire est la   ‚îÇ\n",
       "‚îÇ     ‚îÜ                   ‚îÜ bran‚Ä¶             ‚îÜ                   ‚îÜ            ‚îÜ bran‚Ä¶             ‚îÇ\n",
       "‚îÇ 9   ‚îÜ Alg√®bre g√©n√©rale  ‚îÜ L'&lt;a href=\"alg ‚îÜ [{\"https://fr.wik ‚îÜ 15         ‚îÜ L'alg√®bre         ‚îÇ\n",
       "‚îÇ     ‚îÜ                   ‚îÜ %C3%A8bre\"&gt‚Ä¶    ‚îÜ ipedia.org/wi‚Ä¶    ‚îÜ            ‚îÜ g√©n√©rale, ou      ‚îÇ\n",
       "‚îÇ     ‚îÜ                   ‚îÜ                   ‚îÜ                   ‚îÜ            ‚îÜ alg√®bre‚Ä¶          ‚îÇ\n",
       "‚îÇ 10  ‚îÜ Algorithmique     ‚îÜ L'algorithmique   ‚îÜ [{\"https://fr.wik ‚îÜ 101        ‚îÜ L'algorithmique   ‚îÇ\n",
       "‚îÇ     ‚îÜ                   ‚îÜ est l'√©tude et‚Ä¶   ‚îÜ ipedia.org/wi‚Ä¶    ‚îÜ            ‚îÜ est l'√©tude et‚Ä¶   ‚îÇ\n",
       "‚îÇ 11  ‚îÜ Politique en      ‚îÜ L'&lt;a href=\"Arg ‚îÜ [{\"https://fr.wik ‚îÜ 78         ‚îÜ L'Argentine est   ‚îÇ\n",
       "‚îÇ     ‚îÜ Argentine         ‚îÜ entine\"&gt;Ar‚Ä¶    ‚îÜ ipedia.org/wi‚Ä¶    ‚îÜ            ‚îÜ une r√©publique‚Ä¶   ‚îÇ\n",
       "‚îÇ ‚Ä¶   ‚îÜ ‚Ä¶                 ‚îÜ ‚Ä¶                 ‚îÜ ‚Ä¶                 ‚îÜ ‚Ä¶          ‚îÜ ‚Ä¶                 ‚îÇ\n",
       "‚îÇ 201 ‚îÜ Arthur John Evans ‚îÜ Arthur John Evans ‚îÜ [{\"https://fr.wik ‚îÜ 26         ‚îÜ Arthur John Evans ‚îÇ\n",
       "‚îÇ     ‚îÜ                   ‚îÜ (n√© le √† &lt‚Ä¶     ‚îÜ ipedia.org/wi‚Ä¶    ‚îÜ            ‚îÜ (n√© le √† Nas‚Ä¶     ‚îÇ\n",
       "‚îÇ 206 ‚îÜ Alfred Nobel      ‚îÜ Alfred Bernhard   ‚îÜ [{\"https://fr.wik ‚îÜ 87         ‚îÜ Alfred Bernhard   ‚îÇ\n",
       "‚îÇ     ‚îÜ                   ‚îÜ Nobel /'alfr…ôd‚Ä¶   ‚îÜ ipedia.org/wi‚Ä¶    ‚îÜ            ‚îÜ Nobel /'alfr…ôd‚Ä¶   ‚îÇ\n",
       "‚îÇ 208 ‚îÜ Alc√®ne            ‚îÜ Les alc√®nes sont  ‚îÜ [{\"https://fr.wik ‚îÜ 63         ‚îÜ Les alc√®nes sont  ‚îÇ\n",
       "‚îÇ     ‚îÜ                   ‚îÜ des &lt;a hre‚Ä¶    ‚îÜ ipedia.org/wi‚Ä¶    ‚îÜ            ‚îÜ des hydrocarb‚Ä¶    ‚îÇ\n",
       "‚îÇ 210 ‚îÜ Ange (homonymie)  ‚îÜ &lt;templatestyle ‚îÜ [{\"https://fr.wik ‚îÜ 5          ‚îÜ &lt;templatestyle ‚îÇ\n",
       "‚îÇ     ‚îÜ                   ‚îÜ s src=\"Autres‚Ä¶    ‚îÜ ipedia.org/wi‚Ä¶    ‚îÜ            ‚îÜ s src=\"Autres‚Ä¶    ‚îÇ\n",
       "‚îÇ 213 ‚îÜ Amstrad CPC 464   ‚îÜ L'Amstrad CPC 464 ‚îÜ [{\"https://fr.wik ‚îÜ 36         ‚îÜ L'Amstrad CPC 464 ‚îÇ\n",
       "‚îÇ     ‚îÜ                   ‚îÜ est un &lt;a‚Ä¶     ‚îÜ ipedia.org/wi‚Ä¶    ‚îÜ            ‚îÜ est un ordin‚Ä¶     ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter for only rows that have some links\n",
    "df.filter(pl.col('link_count') >  0).head(100) \n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c6c78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 1)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ text_withoutHref                ‚îÇ\n",
      "‚îÇ ---                             ‚îÇ\n",
      "‚îÇ str                             ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ Antoine Meillet, n√© le √† Mouli‚Ä¶ ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    df\n",
    "    .filter(pl.col(\"title\") == \"Antoine Meillet\")\n",
    "    .select(\"text_withoutHref\")\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534c474",
   "metadata": {},
   "source": [
    "Use the position of the href links from the dataframe to get the sentances that contains them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4809333b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debug info:\n",
      "   Type of test_text: <class 'str'>\n",
      "   Type of test_links: <class 'list'>\n",
      "   Number of links: 111\n",
      "\n",
      "üìù Sample extraction from 'Alg√®bre lin√©aire' (ID: 7):\n",
      "   Total links in article: 111\n",
      "   Sentences with links: 45\n",
      "\n",
      "   Example sentence:\n",
      "   'L‚Äôalg√®bre lin√©aire est la branche des math√©matiques qui s'int√©resse aux espaces vectoriels et aux transformations lin√©ai...'\n",
      "   Links (3): ['math√©matiques', 'espaces vectoriels', 'transformations lin√©aires']\n"
     ]
    }
   ],
   "source": [
    "def extract_sentences_with_links_from_positions(\n",
    "    text: str, \n",
    "    links: list\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract sentences containing hyperlinks using the position information.\n",
    "    \n",
    "    Link format: {\"full_url\": str, \"start_idx\": int, \"anchor\": str, \"href_raw\": str, \"href_decoded\": str}\n",
    "    \"\"\"\n",
    "    # Handle empty inputs\n",
    "    if not text:\n",
    "        return []\n",
    "    if not links or len(links) == 0:  # Check length instead of truthiness\n",
    "        return []\n",
    "    \n",
    "    # Sort links by position\n",
    "    sorted_links = sorted(links, key=lambda x: x.get(\"start_idx\", 0))\n",
    "    \n",
    "    # Simple sentence splitting\n",
    "    sentence_pattern = r'[.!?]+\\s+'\n",
    "    sentences = re.split(sentence_pattern, text)\n",
    "    \n",
    "    results = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if not sentence.strip():\n",
    "            continue\n",
    "        \n",
    "        # Find sentence boundaries in original text\n",
    "        sent_start = text.find(sentence, current_pos)\n",
    "        if sent_start == -1:\n",
    "            continue\n",
    "        sent_end = sent_start + len(sentence)\n",
    "        \n",
    "        # Find all links within this sentence using position info\n",
    "        links_in_sent = []\n",
    "        for link in sorted_links:\n",
    "            link_pos = link.get(\"start_idx\", -1)\n",
    "            \n",
    "            # Check if link position falls within sentence boundaries\n",
    "            if sent_start <= link_pos < sent_end:\n",
    "                links_in_sent.append({\n",
    "                    'anchor': link.get('anchor', ''),\n",
    "                    'href_decoded': link.get('href_decoded', ''),\n",
    "                    'href_raw': link.get('href_raw', ''),\n",
    "                    'position': link_pos,\n",
    "                    'full_url': link.get('full_url', '')\n",
    "                })\n",
    "        \n",
    "        if links_in_sent:  # Only keep sentences with links\n",
    "            results.append({\n",
    "                'sentence': sentence.strip(),\n",
    "                'links_in_sentence': links_in_sent,\n",
    "                'start_pos': sent_start,\n",
    "                'num_links': len(links_in_sent)\n",
    "            })\n",
    "        \n",
    "        current_pos = sent_end\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test on sample - PROPERLY convert to Python list\n",
    "test_article = df.filter(pl.col(\"id\") == 7)  # Alg√®bre lin√©aire\n",
    "\n",
    "# Get text as string\n",
    "test_text = test_article.select(\"text_withoutHref\").to_series()[0]\n",
    "\n",
    "# Get links and convert to Python list properly\n",
    "links_value = test_article.select(\"links\").to_series()[0]\n",
    "\n",
    "# Convert Polars list to Python list of dicts\n",
    "if links_value is None:\n",
    "    test_links = []\n",
    "else:\n",
    "    # This converts the Polars list to a Python list\n",
    "    test_links = list(links_value) if hasattr(links_value, '__iter__') else []\n",
    "\n",
    "print(f\"üîç Debug info:\")\n",
    "print(f\"   Type of test_text: {type(test_text)}\")\n",
    "print(f\"   Type of test_links: {type(test_links)}\")\n",
    "print(f\"   Number of links: {len(test_links)}\")\n",
    "\n",
    "sample_sentences = extract_sentences_with_links_from_positions(test_text, test_links)\n",
    "print(f\"\\nüìù Sample extraction from 'Alg√®bre lin√©aire' (ID: 7):\")\n",
    "print(f\"   Total links in article: {len(test_links)}\")\n",
    "print(f\"   Sentences with links: {len(sample_sentences)}\")\n",
    "if sample_sentences:\n",
    "    print(f\"\\n   Example sentence:\")\n",
    "    print(f\"   '{sample_sentences[0]['sentence'][:120]}...'\")\n",
    "    print(f\"   Links ({sample_sentences[0]['num_links']}): {[l['anchor'] for l in sample_sentences[0]['links_in_sentence'][:3]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45d434",
   "metadata": {},
   "source": [
    "    Build URL ‚Üí ID mapping using articles already in Qdrant.\n",
    "    This is faster than iterating through the full DataFrame.\n",
    "    Necessitate to have populated the qdrant db with some articles first (the more the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132cbac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó∫Ô∏è  Building URL ‚Üí ID mapping from Qdrant collection 'wikipedia_fr'...\n",
      "‚úÖ Created 100,088 URL mappings from 30,208 articles\n"
     ]
    }
   ],
   "source": [
    "def create_url_to_id_mapping_from_qdrant(client, collection_name: str = \"wikipedia_fr\") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Build URL ‚Üí ID mapping using articles already in Qdrant.\n",
    "    This is faster than iterating through the full DataFrame.\n",
    "    \"\"\"\n",
    "    from urllib.parse import quote\n",
    "    \n",
    "    url_to_id = {}\n",
    "    \n",
    "    print(f\"üó∫Ô∏è  Building URL ‚Üí ID mapping from Qdrant collection '{collection_name}'...\")\n",
    "    \n",
    "    # Scroll through all points in Qdrant\n",
    "    offset = None\n",
    "    batch_size = 1000\n",
    "    total_processed = 0\n",
    "    \n",
    "    while True:\n",
    "        points, offset = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=batch_size,\n",
    "            offset=offset,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        \n",
    "        if not points:\n",
    "            break\n",
    "        \n",
    "        for point in points:\n",
    "            article_id = point.payload.get(\"id\")\n",
    "            title = point.payload.get(\"title\", \"\")\n",
    "            \n",
    "            if not article_id or not title:\n",
    "                continue\n",
    "            \n",
    "            # Create URL pattern variations\n",
    "            patterns = [\n",
    "                title,\n",
    "                title.replace(\" \", \"_\"),\n",
    "                quote(title.replace(\" \", \"_\"), safe=\"\"),\n",
    "                title.lower(),\n",
    "                title.lower().replace(\" \", \"_\"),\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                url_to_id[pattern] = article_id\n",
    "        \n",
    "        total_processed += len(points)\n",
    "        \n",
    "        if offset is None:\n",
    "            break\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(url_to_id):,} URL mappings from {total_processed:,} articles\")\n",
    "    return url_to_id\n",
    "\n",
    "# Build mapping from Qdrant (faster than full DataFrame)\n",
    "url_to_id = create_url_to_id_mapping_from_qdrant(client, \"wikipedia_fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206a7f8",
   "metadata": {},
   "source": [
    "Use the struct of href with their position embedding to map sentences to articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17ce1fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Extracting linkable phrases from articles...\n",
      "   Processing 5,000 articles with links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting phrases: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:14<00:00, 356.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Extracted linkable phrases:\n",
      "   Total phrases: 111,162\n",
      "   From 4,739 articles\n",
      "   Total links: 140,198\n",
      "   Avg links per phrase: 1.26\n",
      "\n",
      "üìä Sample linkable phrases:\n",
      "shape: (3, 3)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ sentence                        ‚îÜ anchors               ‚îÜ num_links ‚îÇ\n",
      "‚îÇ ---                             ‚îÜ ---                   ‚îÜ ---       ‚îÇ\n",
      "‚îÇ str                             ‚îÜ list[str]             ‚îÜ i64       ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ Antoine Meillet, n√© le √† Mouli‚Ä¶ ‚îÜ [\"Moulins\", \"Allier\"] ‚îÜ 2         ‚îÇ\n",
      "‚îÇ Paul Jules Antoine Meillet est‚Ä¶ ‚îÜ [\"Cher\"]              ‚îÜ 1         ‚îÇ\n",
      "‚îÇ Il passe son enfance √† Ch√¢teau‚Ä¶ ‚îÜ [\"linguiste\"]         ‚îÜ 1         ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "def create_linkable_phrases_dataset_optimized(\n",
    "    df: pl.DataFrame, \n",
    "    url_to_id: Dict[str, int],\n",
    "    max_articles: int = None,\n",
    "    min_links_per_sentence: int = 1\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract linkable phrases using the structured link data.\n",
    "    Optimized to use position information from links column.\n",
    "    \"\"\"\n",
    "    print(\"üîó Extracting linkable phrases from articles...\")\n",
    "    \n",
    "    # Filter to articles with links\n",
    "    df_with_links = df.filter(pl.col(\"link_count\") > 0)\n",
    "    \n",
    "    if max_articles:\n",
    "        df_with_links = df_with_links.head(max_articles)\n",
    "    \n",
    "    print(f\"   Processing {len(df_with_links):,} articles with links...\")\n",
    "    \n",
    "    linkable_phrases = []\n",
    "    \n",
    "    for row in tqdm(df_with_links.iter_rows(named=True), total=len(df_with_links), desc=\"Extracting phrases\"):\n",
    "        article_id = row[\"id\"]\n",
    "        article_title = row[\"title\"]\n",
    "        text = row.get(\"text_withoutHref\", \"\")\n",
    "        links_raw = row.get(\"links\", [])\n",
    "        \n",
    "        # Convert links to Python list if needed\n",
    "        if links_raw is None:\n",
    "            links = []\n",
    "        else:\n",
    "            links = list(links_raw) if hasattr(links_raw, '__iter__') else []\n",
    "        \n",
    "        if not text or len(links) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Extract sentences with links using position info\n",
    "        sentences_with_links = extract_sentences_with_links_from_positions(text, links)\n",
    "        \n",
    "        for sent_data in sentences_with_links:\n",
    "            sentence = sent_data[\"sentence\"]\n",
    "            links_in_sent = sent_data[\"links_in_sentence\"]\n",
    "            \n",
    "            # Map href_decoded to article IDs\n",
    "            target_ids = []\n",
    "            anchors = []\n",
    "            href_decodeds = []\n",
    "            \n",
    "            for link in links_in_sent:\n",
    "                href_decoded = link[\"href_decoded\"]\n",
    "                anchor = link[\"anchor\"]\n",
    "                \n",
    "                # Try to find target article ID\n",
    "                target_id = url_to_id.get(href_decoded)\n",
    "                if not target_id:\n",
    "                    # Try variations\n",
    "                    for variation in [\n",
    "                        href_decoded.replace(\"_\", \" \"),\n",
    "                        href_decoded.replace(\"%20\", \" \"),\n",
    "                        href_decoded.lower(),\n",
    "                        href_decoded.lower().replace(\"_\", \" \")\n",
    "                    ]:\n",
    "                        target_id = url_to_id.get(variation)\n",
    "                        if target_id:\n",
    "                            break\n",
    "                \n",
    "                if target_id and target_id != article_id:\n",
    "                    target_ids.append(target_id)\n",
    "                    anchors.append(anchor)\n",
    "                    href_decodeds.append(href_decoded)\n",
    "            \n",
    "            # Only keep if we successfully mapped links\n",
    "            if len(target_ids) >= min_links_per_sentence:\n",
    "                linkable_phrases.append({\n",
    "                    \"source_article_id\": article_id,\n",
    "                    \"source_article_title\": article_title,\n",
    "                    \"sentence\": sentence,\n",
    "                    \"target_ids\": target_ids,\n",
    "                    \"anchors\": anchors,\n",
    "                    \"href_decodeds\": href_decodeds,\n",
    "                    \"num_links\": len(target_ids)\n",
    "                })\n",
    "    \n",
    "    linkable_df = pl.DataFrame(linkable_phrases)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extracted linkable phrases:\")\n",
    "    print(f\"   Total phrases: {len(linkable_df):,}\")\n",
    "    print(f\"   From {linkable_df.select('source_article_id').n_unique():,} articles\")\n",
    "    \n",
    "    # FIX: Extract scalar values properly from Polars\n",
    "    total_links = linkable_df.select(pl.col('num_links').sum()).item()\n",
    "    avg_links = linkable_df.select(pl.col('num_links').mean()).item()\n",
    "    \n",
    "    print(f\"   Total links: {total_links:,}\")\n",
    "    print(f\"   Avg links per phrase: {avg_links:.2f}\")\n",
    "    \n",
    "    return linkable_df\n",
    "\n",
    "# Create the dataset (use more articles since we have 26K in Qdrant)\n",
    "linkable_phrases_df = create_linkable_phrases_dataset_optimized(\n",
    "    df, \n",
    "    url_to_id, \n",
    "    max_articles=5000,  # Process 5K articles\n",
    "    min_links_per_sentence=1\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Sample linkable phrases:\")\n",
    "print(linkable_phrases_df.head(3).select([\"sentence\", \"anchors\", \"num_links\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a39565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linkable_phrases_collection(client, collection_name: str = \"linkable_phrases\", force_recreate: bool = False):\n",
    "    \"\"\"Create Qdrant collection for storing linkable phrase embeddings\"\"\"\n",
    "    \n",
    "    # Check if collection already exists\n",
    "    try:\n",
    "        collection_info = client.get_collection(collection_name)\n",
    "        existing_count = collection_info.points_count\n",
    "        \n",
    "        if not force_recreate:\n",
    "            print(f\"‚úÖ Collection '{collection_name}' already exists with {existing_count:,} points\")\n",
    "            print(f\"   Skipping creation (use force_recreate=True to rebuild)\")\n",
    "            return\n",
    "        else:\n",
    "            # User explicitly wants to recreate\n",
    "            client.delete_collection(collection_name)\n",
    "            print(f\"üóëÔ∏è  Deleted existing collection: {collection_name}\")\n",
    "    except Exception as e:\n",
    "        # Collection doesn't exist, that's fine\n",
    "        print(f\"üì¶ Collection '{collection_name}' doesn't exist, creating new one...\")\n",
    "    \n",
    "    # Create new collection with same dimension as existing wikipedia_fr collection\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(\n",
    "            size=model.get_sentence_embedding_dimension(),\n",
    "            distance=Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Created collection: {collection_name}\")\n",
    "    print(f\"   Vector dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "    print(f\"   Distance metric: COSINE\")\n",
    "\n",
    "    create_linkable_phrases_collection(client, \"linkable_phrases\", force_recreate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a78cb7",
   "metadata": {},
   "source": [
    "Put sentances with href embeddings into Qdrant DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eada336a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Resuming from phrase 105,879\n",
      "\n",
      "üöÄ Processing 5,283 phrases...\n",
      "üéÆ GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU (4.3 GB)\n",
      "   ‚ö†Ô∏è  Small GPU detected, using batch_size=32\n",
      "\n",
      "üß† Encoding 5,283 sentences in batches of 32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 166/166 [02:29<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Encoded 5,283 sentences\n",
      "\n",
      "üì§ Uploading to Qdrant with 8 parallel threads...\n",
      "   Split into 21 batches for parallel upload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading (8 threads): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:04<00:00,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Upload complete!\n",
      "   Collection size: 112,861 phrases\n",
      "   Expected: 111,162 phrases\n",
      "   ‚ö†Ô∏è  Mismatch: 112,861 != 111,162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def upload_linkable_phrases_to_qdrant(\n",
    "    client,\n",
    "    linkable_df: pl.DataFrame,\n",
    "    model,\n",
    "    collection_name: str = \"linkable_phrases\",\n",
    "    batch_size: int = 512,\n",
    "    force_recreate: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimized upload with GPU memory management and parallel Qdrant uploads.\n",
    "    \"\"\"\n",
    "    total_rows = len(linkable_df)\n",
    "    \n",
    "    # Check if collection exists\n",
    "    try:\n",
    "        collection_info = client.get_collection(collection_name)\n",
    "        existing_count = collection_info.points_count\n",
    "        \n",
    "        if not force_recreate and existing_count == total_rows:\n",
    "            print(f\"‚úÖ Collection '{collection_name}' already exists with {existing_count:,} phrases\")\n",
    "            return\n",
    "        elif not force_recreate and existing_count > 0:\n",
    "            print(f\"‚ö†Ô∏è  Resuming from phrase {existing_count:,}\")\n",
    "            start_from = existing_count\n",
    "        else:\n",
    "            start_from = 0\n",
    "    except:\n",
    "        print(f\"üì¶ Creating new collection '{collection_name}'\")\n",
    "        start_from = 0\n",
    "    \n",
    "    if start_from >= total_rows:\n",
    "        print(f\"‚úÖ Collection already complete\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüöÄ Processing {total_rows - start_from:,} phrases...\")\n",
    "    \n",
    "    # GPU memory diagnostic\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)} ({gpu_mem_gb:.1f} GB)\")\n",
    "        \n",
    "        # For 4GB GPU, use small batches to avoid OOM\n",
    "        if gpu_mem_gb < 6:\n",
    "            encode_batch_size = 32  # Very small for 4GB GPU\n",
    "            print(f\"   ‚ö†Ô∏è  Small GPU detected, using batch_size={encode_batch_size}\")\n",
    "        elif gpu_mem_gb < 12:\n",
    "            encode_batch_size = 128\n",
    "        else:\n",
    "            encode_batch_size = 512\n",
    "    else:\n",
    "        encode_batch_size = 64\n",
    "        print(f\"üíª Using CPU with batch_size={encode_batch_size}\")\n",
    "    \n",
    "    # Slice remaining data\n",
    "    remaining_df = linkable_df.slice(start_from, total_rows - start_from)\n",
    "    \n",
    "    # Extract sentences in ONE Polars operation\n",
    "    all_sentences = remaining_df.select(\"sentence\").to_series().to_list()\n",
    "    \n",
    "    print(f\"\\nüß† Encoding {len(all_sentences):,} sentences in batches of {encode_batch_size}...\")\n",
    "    \n",
    "    # Encode in small batches to avoid GPU OOM\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_sentences), encode_batch_size), desc=\"Encoding\"):\n",
    "        batch_sentences = all_sentences[i:i + encode_batch_size]\n",
    "        \n",
    "        # Encode batch\n",
    "        embeddings = model.encode(\n",
    "            batch_sentences,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=False,\n",
    "            convert_to_numpy=True,\n",
    "            batch_size=encode_batch_size,\n",
    "            device=str(model.device)\n",
    "        )\n",
    "        \n",
    "        all_embeddings.append(embeddings)\n",
    "        \n",
    "        # Clear GPU cache every 100 batches to prevent memory fragmentation\n",
    "        if torch.cuda.is_available() and i % (100 * encode_batch_size) == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    # Concatenate all embeddings\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    print(f\"‚úÖ Encoded {len(all_embeddings):,} sentences\")\n",
    "    \n",
    "    # Clear GPU memory before upload phase\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # PARALLEL QDRANT UPLOAD using ThreadPoolExecutor\n",
    "    print(f\"\\nüì§ Uploading to Qdrant with {8} parallel threads...\")\n",
    "    \n",
    "    # Convert DataFrame to dicts ONCE (Polars vectorized operation)\n",
    "    all_data = remaining_df.to_dicts()\n",
    "    \n",
    "    # Split into upload batches\n",
    "    upload_batch_size = 256  # Smaller batches for parallel uploads\n",
    "    num_batches = (len(remaining_df) + upload_batch_size - 1) // upload_batch_size\n",
    "    \n",
    "    print(f\"   Split into {num_batches} batches for parallel upload\")\n",
    "    \n",
    "    def upload_batch(batch_idx):\n",
    "        \"\"\"Upload a single batch to Qdrant (runs in parallel thread)\"\"\"\n",
    "        start_idx = batch_idx * upload_batch_size\n",
    "        end_idx = min(start_idx + upload_batch_size, len(remaining_df))\n",
    "        \n",
    "        batch_embeddings = all_embeddings[start_idx:end_idx]\n",
    "        batch_data = all_data[start_idx:end_idx]\n",
    "        \n",
    "        # Create points (list comprehension is fast)\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=start_from + start_idx + j,\n",
    "                vector=batch_embeddings[j].tolist(),\n",
    "                payload={\n",
    "                    \"phrase_id\": start_from + start_idx + j,\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"source_article_id\": row[\"source_article_id\"],\n",
    "                    \"source_article_title\": row[\"source_article_title\"],\n",
    "                    \"target_ids\": row[\"target_ids\"],\n",
    "                    \"anchors\": row[\"anchors\"],\n",
    "                    \"href_decodeds\": row[\"href_decodeds\"],\n",
    "                    \"num_links\": row[\"num_links\"]\n",
    "                }\n",
    "            )\n",
    "            for j, row in enumerate(batch_data)\n",
    "        ]\n",
    "        \n",
    "        # Upload (Qdrant client is thread-safe)\n",
    "        client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points,\n",
    "            wait=True\n",
    "        )\n",
    "        \n",
    "        return len(points)\n",
    "    \n",
    "    # Use ThreadPoolExecutor for PARALLEL uploads (this WILL use multiple threads)\n",
    "    # You'll see multiple threads active during upload phase\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        # Submit all upload jobs at once\n",
    "        futures = [executor.submit(upload_batch, i) for i in range(num_batches)]\n",
    "        \n",
    "        # Track progress as uploads complete\n",
    "        uploaded = 0\n",
    "        for future in tqdm(futures, desc=\"Uploading (8 threads)\"):\n",
    "            uploaded += future.result()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Upload complete!\")\n",
    "    \n",
    "    # Verify\n",
    "    collection_info = client.get_collection(collection_name)\n",
    "    print(f\"   Collection size: {collection_info.points_count:,} phrases\")\n",
    "    print(f\"   Expected: {total_rows:,} phrases\")\n",
    "    \n",
    "    if collection_info.points_count != total_rows:\n",
    "        print(f\"   ‚ö†Ô∏è  Mismatch: {collection_info.points_count:,} != {total_rows:,}\")\n",
    "\n",
    "# Run with proper memory management\n",
    "upload_linkable_phrases_to_qdrant(\n",
    "    client,\n",
    "    linkable_phrases_df,\n",
    "    model,\n",
    "    \"linkable_phrases\",\n",
    "    batch_size=512,\n",
    "    force_recreate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296d6ce",
   "metadata": {},
   "source": [
    "Run actual prediction by querying Qdrant DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b046a86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Collection 'linkable_phrases' already exists with 105,879 phrases\n"
     ]
    }
   ],
   "source": [
    "def predict_links_for_article_hybrid(\n",
    "    client,\n",
    "    linkable_df: pl.DataFrame,\n",
    "    model,\n",
    "    collection_name: str = \"linkable_phrases\",\n",
    "    batch_size: int = 128,\n",
    "    force_recreate: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Embed linkable phrases and upload to Qdrant.\n",
    "    Uses same encoding approach as existing wikipedia_fr collection.\n",
    "    Supports resuming and parallel encoding.\n",
    "    \"\"\"\n",
    "    total_rows = len(linkable_df)\n",
    "    \n",
    "    # Check if collection exists and has the right size\n",
    "    try:\n",
    "        collection_info = client.get_collection(collection_name)\n",
    "        existing_count = collection_info.points_count\n",
    "        \n",
    "        if not force_recreate and existing_count == total_rows:\n",
    "            print(f\"‚úÖ Collection '{collection_name}' already exists with {existing_count:,} phrases\")\n",
    "            print(f\"   Skipping upload (use force_recreate=True to rebuild)\")\n",
    "            return\n",
    "        elif not force_recreate and existing_count > 0:\n",
    "            print(f\"‚ö†Ô∏è  Collection exists with {existing_count:,} phrases (expected {total_rows:,})\")\n",
    "            user_input = input(\"   Continue from where it left off? (y/n): \")\n",
    "            if user_input.lower() == 'y':\n",
    "                start_from = existing_count\n",
    "                print(f\"   Resuming from phrase {start_from:,}\")\n",
    "            else:\n",
    "                print(\"   Aborting. Use force_recreate=True to rebuild from scratch.\")\n",
    "                return\n",
    "        else:\n",
    "            start_from = 0\n",
    "    except:\n",
    "        print(f\"üì¶ Collection '{collection_name}' doesn't exist, will create it\")\n",
    "        start_from = 0\n",
    "    \n",
    "    print(f\"üöÄ Uploading {total_rows - start_from:,} linkable phrases to Qdrant...\")\n",
    "    print(f\"   Using parallel encoding with batch_size={batch_size}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(start_from, total_rows, batch_size), desc=\"Uploading batches\"):\n",
    "        batch = linkable_df.slice(i, min(batch_size, total_rows - i))\n",
    "        \n",
    "        # Get sentences\n",
    "        sentences = batch.select(\"sentence\").to_series().to_list()\n",
    "        \n",
    "        # Embed sentences with parallel processing\n",
    "        # The model.encode already uses multi-threading internally\n",
    "        embeddings = model.encode(\n",
    "            sentences,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=False,\n",
    "            convert_to_numpy=True,\n",
    "            batch_size=64,  # Internal batch size for encoding\n",
    "            device=device,\n",
    "            convert_to_tensor=False\n",
    "        )\n",
    "        \n",
    "        # Create points in parallel using list comprehension (faster than loop)\n",
    "        batch_rows = list(batch.iter_rows(named=True))\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=i + j,\n",
    "                vector=embeddings[j].tolist(),\n",
    "                payload={\n",
    "                    \"phrase_id\": i + j,\n",
    "                    \"sentence\": row[\"sentence\"],\n",
    "                    \"source_article_id\": row[\"source_article_id\"],\n",
    "                    \"source_article_title\": row[\"source_article_title\"],\n",
    "                    \"target_ids\": row[\"target_ids\"],\n",
    "                    \"anchors\": row[\"anchors\"],\n",
    "                    \"href_decodeds\": row[\"href_decodeds\"],\n",
    "                    \"num_links\": row[\"num_links\"]\n",
    "                }\n",
    "            )\n",
    "            for j, row in enumerate(batch_rows)\n",
    "        ]\n",
    "        \n",
    "        # Upload to Qdrant (this is already async internally)\n",
    "        client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points,\n",
    "            wait=True\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Upload complete!\")\n",
    "    \n",
    "    # Verify\n",
    "    collection_info = client.get_collection(collection_name)\n",
    "    print(f\"   Collection '{collection_name}': {collection_info.points_count:,} phrases\")\n",
    "    print(f\"   Existing 'wikipedia_fr': {client.count('wikipedia_fr', exact=True).count:,} articles\")\n",
    "\n",
    "# Upload the phrases (will skip if already exists with same size)\n",
    "upload_linkable_phrases_to_qdrant(\n",
    "    client,\n",
    "    linkable_phrases_df,\n",
    "    model,\n",
    "    \"linkable_phrases\",\n",
    "    batch_size=256,  # Increased batch size for better parallelism\n",
    "    force_recreate=False  # Set to True to force rebuild\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "113f571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Predicting links for 5 sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9f42174ab04edb89cf5330f83913d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/tmp/ipykernel_3784/148963073.py:30: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = client.search(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 39.27it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict_links_for_sentences(\n",
    "    client,\n",
    "    model,\n",
    "    test_sentences: List[str],\n",
    "    source_article_id: int,  # ‚Üê Add this parameter\n",
    "    collection_name: str = \"linkable_phrases\",\n",
    "    top_k: int = 5,  # ‚Üê Increase to get more candidates\n",
    "    min_similarity: float = 0.7\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Predict links for new sentences by finding similar linkable phrases.\n",
    "    Excludes matches from the same source article.\n",
    "    \"\"\"\n",
    "    print(f\"üîç Predicting links for {len(test_sentences)} sentences...\")\n",
    "    \n",
    "    # Encode test sentences\n",
    "    test_embeddings = model.encode(\n",
    "        test_sentences,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Query Qdrant for each sentence\n",
    "    for sentence, embedding in tqdm(zip(test_sentences, test_embeddings), total=len(test_sentences)):\n",
    "        # Search for similar phrases in the collection\n",
    "        search_results = client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=embedding.tolist(),\n",
    "            limit=top_k,\n",
    "            score_threshold=min_similarity\n",
    "        )\n",
    "        \n",
    "        # Extract results, EXCLUDING same article\n",
    "        similar_phrases = []\n",
    "        for result in search_results:\n",
    "            # ‚Üê ADD THIS CHECK\n",
    "            if result.payload['source_article_id'] == source_article_id:\n",
    "                continue  # Skip matches from same article\n",
    "            \n",
    "            similar_phrases.append({\n",
    "                'similarity_score': result.score,\n",
    "                'similar_sentence': result.payload['sentence'],\n",
    "                'source_article': result.payload['source_article_title'],\n",
    "                'source_article_id': result.payload['source_article_id'],\n",
    "                'target_ids': result.payload['target_ids'],\n",
    "                'anchors': result.payload['anchors'],\n",
    "                'href_decodeds': result.payload['href_decodeds'],\n",
    "                'num_links': result.payload['num_links']\n",
    "            })\n",
    "        \n",
    "        predictions.append({\n",
    "            'sentence': sentence,\n",
    "            'num_matches': len(similar_phrases),\n",
    "            'similar_linkable_phrases': similar_phrases\n",
    "        })\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Usage\n",
    "test_article = df.filter(pl.col(\"id\") == 7)\n",
    "test_article_id = test_article.select(\"id\").to_series()[0]  # ‚Üê Get article ID\n",
    "test_text = test_article.select(\"text_withoutHref\").to_series()[0]\n",
    "test_links = list(test_article.select(\"links\").to_series()[0])\n",
    "\n",
    "test_sentences_data = extract_sentences_with_links_from_positions(test_text, test_links)\n",
    "test_sentences = [s['sentence'] for s in test_sentences_data[:5]]\n",
    "\n",
    "# Generate predictions, excluding self-matches\n",
    "predictions = predict_links_for_sentences(\n",
    "    client,\n",
    "    model,\n",
    "    test_sentences,\n",
    "    source_article_id=test_article_id,  # ‚Üê Pass article ID\n",
    "    collection_name=\"linkable_phrases\",\n",
    "    top_k=5,  # Increased to get enough non-self matches\n",
    "    min_similarity=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef78cca0",
   "metadata": {},
   "source": [
    "Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adbc01ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéØ PREDICTED LINKS (Phrase-Level Similarity)\n",
      "================================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Sentence 1:\n",
      "'L‚Äôalg√®bre lin√©aire est la branche des math√©matiques qui s'int√©resse aux espaces vectoriels et aux transformations lin√©aires, formalisation g√©n√©rale des th√©ories des syst√®mes d'√©quations lin√©aires...'\n",
      "\n",
      "üí° 4 similar linkable phrase(s) found:\n",
      "\n",
      "   Match 1 | Similarity: 0.909 | Links: 1\n",
      "   Similar phrase: 'L'alg√®bre g√©n√©rale, ou alg√®bre abstraite, est la branche des math√©matiques qui porte principalement sur l'√©tude des structures alg√©briques et de leurs...'\n",
      "   From article: 'Alg√®bre g√©n√©rale'\n",
      "   Suggested links:\n",
      "      üîó 'alg√®bre' ‚Üí Alg√®bre\n",
      "\n",
      "   Match 2 | Similarity: 0.892 | Links: 2\n",
      "   Similar phrase: 'En math√©matiques, plus pr√©cis√©ment en alg√®bre lin√©aire, un espace vectoriel est un ensemble d'objets, appel√©s vecteurs, que l'on peut additionner entr...'\n",
      "   From article: 'Espace vectoriel'\n",
      "   Suggested links:\n",
      "      üîó 'alg√®bre lin√©aire' ‚Üí Alg√®bre lin√©aire\n",
      "      üîó 'vecteurs' ‚Üí Vecteur\n",
      "\n",
      "   Match 3 | Similarity: 0.866 | Links: 2\n",
      "   Similar phrase: 'L'alg√®bre de Boole, ou calcul bool√©en, est la partie des math√©matiques qui s'int√©resse √† une approche alg√©brique de la logique, vue en termes de varia...'\n",
      "   From article: 'Alg√®bre de Boole (logique)'\n",
      "   Suggested links:\n",
      "      üîó 'alg√©brique' ‚Üí Alg√®bre\n",
      "      üîó 'logique' ‚Üí Logique\n",
      "\n",
      "   Match 4 | Similarity: 0.859 | Links: 1\n",
      "   Similar phrase: 'En math√©matiques, et plus pr√©cis√©ment en alg√®bre, la th√©orie de Galois est l'√©tude des extensions de corps commutatifs, par le biais d'une corresponda...'\n",
      "   From article: 'Th√©orie de Galois'\n",
      "   Suggested links:\n",
      "      üîó 'alg√®bre' ‚Üí Alg√®bre\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Sentence 2:\n",
      "'L'alg√®bre lin√©aire est initi√©e dans son principe par le math√©maticien perse Al-Khw√¢rizm√Æ qui s'est inspir√© des textes de math√©matiques indiens et qui a compl√©t√© les travaux de l'√©cole grecque, laquell...'\n",
      "\n",
      "üí° 3 similar linkable phrase(s) found:\n",
      "\n",
      "   Match 1 | Similarity: 0.863 | Links: 2\n",
      "   Similar phrase: 'Le mot ¬´ algorithme ¬ª vient du nom du math√©maticien Al-Khw√¢rizm√Æ (latinis√© au Moyen √Çge en ), qui, au √©crivit le premier ouvrage syst√©matique donnant ...'\n",
      "   From article: 'Algorithmique'\n",
      "   Suggested links:\n",
      "      üîó 'math√©maticien' ‚Üí Math√©maticien\n",
      "      üîó 'Al-Khw√¢rizm√Æ' ‚Üí Al-Khw√¢rizm√Æ\n",
      "\n",
      "   Match 2 | Similarity: 0.851 | Links: 1\n",
      "   Similar phrase: 'Vers le VIII¬†si√®cle, Mohamed Ybn Moussa al-Khawarezmi fonde suppos√©ment la th√©orie des algorithmes et forge le terme \"alg√®bre\" (de l'arabe \"Al-jabr\" s...'\n",
      "   From article: 'Histoire des ordinateurs'\n",
      "   Suggested links:\n",
      "      üîó 'physicien' ‚Üí Physicien\n",
      "\n",
      "   Match 3 | Similarity: 0.850 | Links: 1\n",
      "   Similar phrase: 'Cette th√©orie est n√©e de l'√©tude par √âvariste Galois des √©quations alg√©briques...'\n",
      "   From article: 'Th√©orie de Galois'\n",
      "   Suggested links:\n",
      "      üîó 'groupes de Galois' ‚Üí Groupe de Galois\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Sentence 3:\n",
      "'Elle a √©t√© reprise par Ren√© Descartes qui pose des probl√®mes de g√©om√©trie, comme la d√©termination de l'intersection de deux droites, en termes d'√©quation lin√©aire, √©tablissant d√®s lors un pont entre d...'\n",
      "\n",
      "üí° 3 similar linkable phrase(s) found:\n",
      "\n",
      "   Match 1 | Similarity: 0.880 | Links: 1\n",
      "   Similar phrase: 'L'introduction des coordonn√©es par Ren√© Descartes permet de faire un lien entre certains objets g√©om√©triques et certaines √©quations alg√©briques, ne fa...'\n",
      "   From article: 'G√©om√©trie alg√©brique'\n",
      "   Suggested links:\n",
      "      üîó 'Andr√© Weil' ‚Üí Andr√© Weil\n",
      "\n",
      "   Match 2 | Similarity: 0.873 | Links: 2\n",
      "   Similar phrase: 'Ren√© Descartes et Pierre de Fermat introduisent √©galement ce que l'on appelle toujours dans les coll√®ges et les lyc√©es la ¬´ g√©om√©trie analytique ¬ª, au...'\n",
      "   From article: 'Alg√®bre'\n",
      "   Suggested links:\n",
      "      üîó 'suite de Fibonacci' ‚Üí Suite de Fibonacci\n",
      "      üîó 'chiffres arabes' ‚Üí Chiffres arabes\n",
      "\n",
      "   Match 3 | Similarity: 0.845 | Links: 1\n",
      "   Similar phrase: 'Descartes, alert√©, r√©pond aussit√¥t √† Mersenne :\n",
      "La querelle qui s'ensuit permet alors √† Fermat de faire montre de rigueur et de sang-froid :\n",
      "Pour auta...'\n",
      "   From article: 'Pierre de Fermat'\n",
      "   Suggested links:\n",
      "      üîó 'La Haye' ‚Üí Descartes (Indre-et-Loire)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Sentence 4:\n",
      "'S'il ne d√©finit pas la notion de base de l'alg√®bre lin√©aire qu'est celle d'espace vectoriel, il l'utilise d√©j√† avec succ√®s, et cette utilisation naturelle des aspects lin√©aires des √©quations manipul√©e...'\n",
      "\n",
      "üí° 4 similar linkable phrase(s) found:\n",
      "\n",
      "   Match 1 | Similarity: 0.867 | Links: 1\n",
      "   Similar phrase: 'L'id√©e principale est qu'un espace au sens de la g√©om√©trie usuelle peut √™tre d√©crit par l'ensemble des fonctions num√©riques d√©finies sur cet espace...'\n",
      "   From article: 'G√©om√©trie non commutative'\n",
      "   Suggested links:\n",
      "      üîó 'Alexandre Grothendieck' ‚Üí Alexandre Grothendieck\n",
      "\n",
      "   Match 2 | Similarity: 0.863 | Links: 1\n",
      "   Similar phrase: 'Il d√©gage, dans son c√©l√®bre programme d'Erlangen, le principe g√©n√©ral qu'une g√©om√©trie est d√©finie par un espace et un groupe op√©rant sur cet espace, ...'\n",
      "   From article: 'Th√©orie de Galois'\n",
      "   Suggested links:\n",
      "      üîó 'corps fini' ‚Üí Corps fini\n",
      "\n",
      "   Match 3 | Similarity: 0.863 | Links: 1\n",
      "   Similar phrase: 'On utilisera de plus constamment que si \"K\" est archim√©dien alors les Œµ qui interviennent dans ces d√©finitions peuvent toujours √™tre pris dans ‚Ñö+*...'\n",
      "   From article: 'Construction des nombres r√©els'\n",
      "   Suggested links:\n",
      "      üîó 'espace m√©trique complet' ‚Üí Espace complet\n",
      "\n",
      "   Match 4 | Similarity: 0.862 | Links: 1\n",
      "   Similar phrase: 'Si les calculs sont justes, ils sont exprim√©s dans un langage d'une grande complexit√© et les \"preuves\" proc√®dent plus de l'intuition g√©om√©trique que d...'\n",
      "   From article: 'Nombre r√©el'\n",
      "   Suggested links:\n",
      "      üîó 'irrationnel' ‚Üí Nombre irrationnel\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Sentence 5:\n",
      "'Apr√®s cette d√©couverte, les progr√®s en alg√®bre lin√©aire vont se limiter √† des √©tudes ponctuelles comme la d√©finition et l'analyse des premi√®res propri√©t√©s des d√©terminants par Jean d'Alembert...'\n",
      "\n",
      "üí° 4 similar linkable phrase(s) found:\n",
      "\n",
      "   Match 1 | Similarity: 0.895 | Links: 1\n",
      "   Similar phrase: 'Jusqu'√† cette √©poque, l'alg√®bre s'identifie √† la th√©orie des √©quations polynomiales et de leur r√©solution...'\n",
      "   From article: 'Corps commutatif'\n",
      "   Suggested links:\n",
      "      üîó 'classes de congruences modulo \"p\"' ‚Üí Congruence sur les entiers\n",
      "\n",
      "   Match 2 | Similarity: 0.877 | Links: 1\n",
      "   Similar phrase: 'L'introduction des coordonn√©es par Ren√© Descartes permet de faire un lien entre certains objets g√©om√©triques et certaines √©quations alg√©briques, ne fa...'\n",
      "   From article: 'G√©om√©trie alg√©brique'\n",
      "   Suggested links:\n",
      "      üîó 'Andr√© Weil' ‚Üí Andr√© Weil\n",
      "\n",
      "   Match 3 | Similarity: 0.869 | Links: 1\n",
      "   Similar phrase: 'Par lin√©arit√©, il suffit de d√©terminer les ant√©c√©dents des vecteurs de la base canonique de \"K\"\"n \"+ 1...'\n",
      "   From article: '√âquation lin√©aire'\n",
      "   Suggested links:\n",
      "      üîó 'physique' ‚Üí Physique\n",
      "\n",
      "   Match 4 | Similarity: 0.868 | Links: 2\n",
      "   Similar phrase: 'La physique quantique est d√©sormais r√©ductible aux math√©matiques des op√©rateurs hermitiens lin√©aires dans un espace de Hilbert...'\n",
      "   From article: 'John von Neumann'\n",
      "   Suggested links:\n",
      "      üîó 'Giuseppe Peano' ‚Üí Giuseppe Peano\n",
      "      üîó 'g√©om√©trie' ‚Üí G√©om√©trie\n"
     ]
    }
   ],
   "source": [
    "def display_predictions_detailed(predictions: List[Dict], df: pl.DataFrame, max_sentences: int = 5):\n",
    "    \"\"\"Display predicted links with full article information\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ PREDICTED LINKS (Phrase-Level Similarity)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, pred in enumerate(predictions[:max_sentences]):\n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"Sentence {i+1}:\")\n",
    "        print(f\"'{pred['sentence'][:200]}...'\")\n",
    "        print(f\"\\nüí° {pred['num_matches']} similar linkable phrase(s) found:\")\n",
    "        \n",
    "        for j, similar in enumerate(pred['similar_linkable_phrases']):\n",
    "            print(f\"\\n   Match {j+1} | Similarity: {similar['similarity_score']:.3f} | Links: {similar['num_links']}\")\n",
    "            print(f\"   Similar phrase: '{similar['similar_sentence'][:150]}...'\")\n",
    "            print(f\"   From article: '{similar['source_article']}'\")\n",
    "            print(f\"   Suggested links:\")\n",
    "            \n",
    "            for target_id, anchor in zip(similar['target_ids'][:3], similar['anchors'][:3]):\n",
    "                target_title = df.filter(pl.col(\"id\") == target_id).select(\"title\").to_series()\n",
    "                target_name = target_title[0] if len(target_title) > 0 else f\"ID {target_id}\"\n",
    "                print(f\"      üîó '{anchor}' ‚Üí {target_name}\")\n",
    "\n",
    "# Display predictions\n",
    "display_predictions_detailed(predictions, df, max_sentences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c5fe1",
   "metadata": {},
   "source": [
    "Create a test set with articles that are not already embedded in the qdrant db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Creating test set from unseen articles (checking link validity)...\n",
      "üìö Training set: 4,739 unique articles\n",
      "üìñ Total articles in database: 2,556,402\n",
      "üóÑÔ∏è  Articles in Qdrant (valid link targets): 30,208\n",
      "üîç Available unseen articles: 2,551,663\n",
      "‚úÖ Selected 500 unseen articles for testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting test sentences: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 1140.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Created test set:\n",
      "   Total test sentences: 508\n",
      "   From 199 unseen articles\n",
      "   Total ground truth links: 695\n",
      "   Avg links per sentence: 1.37\n",
      "\n",
      "üìä Statistics:\n",
      "   Articles processed: 500\n",
      "   Sentences extracted: 7,589\n",
      "   Sentences with empty anchors: 14\n",
      "   Sentences with unmappable links: 823\n",
      "   Sentences with valid links: 508\n",
      "\n",
      "üìù Sample test sentences:\n",
      "\n",
      "   1. 'Messenger proposait des environnements (d√©cors), un grand nombre d'√©motic√¥nes, l...'\n",
      "      From: Yahoo! Messenger\n",
      "      Ground truth: ['Yahoo!']\n",
      "\n",
      "   2. 'Il donnait √©galement acc√®s aux services Yahoo (m√©t√©o, bourse, information, r√©sul...'\n",
      "      From: Yahoo! Messenger\n",
      "      Ground truth: ['Windows']\n",
      "\n",
      "   3. '\"La Dame √† l'hermine\" (Portrait de Cecilia Gallerani), de L√©onard de Vinci, pein...'\n",
      "      From: 1488\n",
      "      Ground truth: ['L√©onard de Vinci']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell: Create Test Set with Valid Links Only \n",
    "\n",
    "print(\"üß™ Creating test set from unseen articles (checking link validity)...\")\n",
    "\n",
    "# Get IDs of articles used in training\n",
    "training_article_ids = set(linkable_phrases_df.select(\"source_article_id\").to_series().unique().to_list())\n",
    "print(f\"üìö Training set: {len(training_article_ids):,} unique articles\")\n",
    "\n",
    "# Get set of all article IDs in our database (for validation)\n",
    "all_article_ids = set(df.select(\"id\").to_series().to_list())\n",
    "print(f\"üìñ Total articles in database: {len(all_article_ids):,}\")\n",
    "\n",
    "# Get articles that exist in Qdrant (these are the only valid link targets)\n",
    "qdrant_article_ids = set(url_to_id.values())\n",
    "print(f\"üóÑÔ∏è  Articles in Qdrant (valid link targets): {len(qdrant_article_ids):,}\")\n",
    "\n",
    "# Filter to articles NOT in training set\n",
    "unseen_articles = df.filter(\n",
    "    ~pl.col(\"id\").is_in(training_article_ids) & \n",
    "    (pl.col(\"link_count\") > 0)\n",
    ")\n",
    "\n",
    "print(f\"üîç Available unseen articles: {len(unseen_articles):,}\")\n",
    "\n",
    "# Sample some test articles\n",
    "test_sample_size = 500  # Increased to get more valid sentences\n",
    "test_articles = unseen_articles.head(test_sample_size)\n",
    "\n",
    "print(f\"‚úÖ Selected {len(test_articles)} unseen articles for testing\")\n",
    "\n",
    "# Extract test sentences from these unseen articles\n",
    "test_data = []\n",
    "stats = {\n",
    "    'articles_processed': 0,\n",
    "    'sentences_extracted': 0,\n",
    "    'sentences_with_empty_anchors': 0,\n",
    "    'sentences_with_unmappable_links': 0,\n",
    "    'sentences_with_valid_links': 0\n",
    "}\n",
    "\n",
    "for row in tqdm(test_articles.iter_rows(named=True), total=len(test_articles), desc=\"Extracting test sentences\"):\n",
    "    stats['articles_processed'] += 1\n",
    "    article_id = row[\"id\"]\n",
    "    article_title = row[\"title\"]\n",
    "    text = row.get(\"text_withoutHref\", \"\")\n",
    "    links_raw = row.get(\"links\", [])\n",
    "    \n",
    "    # Convert links\n",
    "    if links_raw is None:\n",
    "        links = []\n",
    "    else:\n",
    "        links = list(links_raw) if hasattr(links_raw, '__iter__') else []\n",
    "    \n",
    "    if not text or len(links) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Extract sentences with links\n",
    "    sentences_with_links = extract_sentences_with_links_from_positions(text, links)\n",
    "    stats['sentences_extracted'] += len(sentences_with_links)\n",
    "    \n",
    "    # Take first 5 sentences from each article\n",
    "    for sent_data in sentences_with_links[:5]:\n",
    "        sentence = sent_data[\"sentence\"]\n",
    "        links_in_sent = sent_data[\"links_in_sentence\"]\n",
    "        \n",
    "        # Map href_decoded to article IDs (ground truth)\n",
    "        ground_truth_links = []\n",
    "        has_empty_anchor = False\n",
    "        \n",
    "        for link in links_in_sent:\n",
    "            href_decoded = link[\"href_decoded\"]\n",
    "            anchor = link[\"anchor\"]\n",
    "            \n",
    "            # Skip links with empty anchors\n",
    "            if not anchor or anchor.strip() == \"\":\n",
    "                has_empty_anchor = True\n",
    "                continue\n",
    "            \n",
    "            # Try to find target article ID\n",
    "            target_id = url_to_id.get(href_decoded)\n",
    "            \n",
    "            if not target_id:\n",
    "                # Try variations\n",
    "                variations = [\n",
    "                    href_decoded.replace(\"_\", \" \"),\n",
    "                    href_decoded.replace(\"%20\", \" \"),\n",
    "                    href_decoded.lower(),\n",
    "                    href_decoded.lower().replace(\"_\", \" \"),\n",
    "                    href_decoded.replace(\"_\", \" \").lower(),\n",
    "                ]\n",
    "                \n",
    "                for variation in variations:\n",
    "                    target_id = url_to_id.get(variation)\n",
    "                    if target_id:\n",
    "                        break\n",
    "            \n",
    "            # Only include if target exists in Qdrant and is not self-link\n",
    "            if target_id and target_id != article_id and target_id in qdrant_article_ids:\n",
    "                ground_truth_links.append({\n",
    "                    'anchor': anchor,\n",
    "                    'target_id': target_id,\n",
    "                    'href_decoded': href_decoded\n",
    "                })\n",
    "        \n",
    "        # Track statistics\n",
    "        if has_empty_anchor:\n",
    "            stats['sentences_with_empty_anchors'] += 1\n",
    "        \n",
    "        if ground_truth_links:\n",
    "            stats['sentences_with_valid_links'] += 1\n",
    "            test_data.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'sentence': sentence,\n",
    "                'ground_truth_links': ground_truth_links,\n",
    "                'num_ground_truth': len(ground_truth_links)\n",
    "            })\n",
    "        else:\n",
    "            stats['sentences_with_unmappable_links'] += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Created test set:\")\n",
    "print(f\"   Total test sentences: {len(test_data):,}\")\n",
    "print(f\"   From {len(set(t['article_id'] for t in test_data)):,} unseen articles\")\n",
    "print(f\"   Total ground truth links: {sum(t['num_ground_truth'] for t in test_data):,}\")\n",
    "print(f\"   Avg links per sentence: {sum(t['num_ground_truth'] for t in test_data) / len(test_data):.2f}\" if test_data else \"   Avg links per sentence: 0\")\n",
    "\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(f\"   Articles processed: {stats['articles_processed']:,}\")\n",
    "print(f\"   Sentences extracted: {stats['sentences_extracted']:,}\")\n",
    "print(f\"   Sentences with empty anchors: {stats['sentences_with_empty_anchors']:,}\")\n",
    "print(f\"   Sentences with unmappable links: {stats['sentences_with_unmappable_links']:,}\")\n",
    "print(f\"   Sentences with valid links: {stats['sentences_with_valid_links']:,}\")\n",
    "\n",
    "if test_data:\n",
    "    print(f\"\\nüìù Sample test sentences:\")\n",
    "    for i, sample in enumerate(test_data[:3]):\n",
    "        print(f\"\\n   {i+1}. '{sample['sentence'][:80]}...'\")\n",
    "        print(f\"      From: {sample['article_title']}\")\n",
    "        print(f\"      Ground truth: {[gt['anchor'] for gt in sample['ground_truth_links']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27db1e3",
   "metadata": {},
   "source": [
    "Upload them into Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "074c0da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Running predictions on 508 unseen sentences...\n",
      "üîç Predicting links for 508 sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbecf80f093f4a8c862a927bd677e0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/508 [00:00<?, ?it/s]/tmp/ipykernel_3784/284731192.py:29: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = client.search(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 508/508 [00:02<00:00, 226.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 508 predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell: Run Predictions on Unseen Test Set\n",
    "\n",
    "# Extract just the sentences for prediction\n",
    "test_sentences = [t['sentence'] for t in test_data]\n",
    "\n",
    "print(f\"üîÆ Running predictions on {len(test_sentences)} unseen sentences...\")\n",
    "\n",
    "# Generate predictions\n",
    "test_predictions = predict_links_for_sentences(\n",
    "    client,\n",
    "    model,\n",
    "    test_sentences,\n",
    "    collection_name=\"linkable_phrases\",\n",
    "    top_k=5,  # Get more candidates\n",
    "    min_similarity=0.65  # Lower threshold to see more results\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Generated {len(test_predictions)} predictions\")\n",
    "\n",
    "# Combine predictions with ground truth\n",
    "for i, pred in enumerate(test_predictions):\n",
    "    pred['article_id'] = test_data[i]['article_id']\n",
    "    pred['article_title'] = test_data[i]['article_title']\n",
    "    pred['ground_truth_links'] = test_data[i]['ground_truth_links']\n",
    "    pred['num_ground_truth'] = test_data[i]['num_ground_truth']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867a592",
   "metadata": {},
   "source": [
    "Evaluate sentances vs unseen articles from qdrant\n",
    "Results are terrible as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75316bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä EVALUATION RESULTS (Unseen Test Set)\n",
      "================================================================================\n",
      "\n",
      "üìà Overall Metrics:\n",
      "   Precision: 0.092 (212/2307)\n",
      "   Recall:    0.305 (212/694)\n",
      "   F1 Score:  0.141\n",
      "\n",
      "üìù Test Set Size:\n",
      "   Sentences: 508\n",
      "   Avg ground truth links per sentence: 1.37\n",
      "   Avg predicted links per sentence: 4.54\n",
      "\n",
      "‚úÖ Top 5 Best Predictions (by F1):\n",
      "shape: (5, 5)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ sentence                        ‚îÜ article                ‚îÜ precision ‚îÜ recall ‚îÜ f1       ‚îÇ\n",
      "‚îÇ ---                             ‚îÜ ---                    ‚îÜ ---       ‚îÜ ---    ‚îÜ ---      ‚îÇ\n",
      "‚îÇ str                             ‚îÜ str                    ‚îÜ f64       ‚îÜ f64    ‚îÜ f64      ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ D√©put√© pour la 3e¬†circonscript‚Ä¶ ‚îÜ Herv√© Morin            ‚îÜ 1.0       ‚îÜ 1.0    ‚îÜ 1.0      ‚îÇ\n",
      "‚îÇ Fille d'Eustase Rimane, ancien‚Ä¶ ‚îÜ Juliana Rimane         ‚îÜ 1.0       ‚îÜ 1.0    ‚îÜ 1.0      ‚îÇ\n",
      "‚îÇ Membre du Rassemblement pour l‚Ä¶ ‚îÜ Jean Marsaudon         ‚îÜ 0.75      ‚îÜ 1.0    ‚îÜ 0.857143 ‚îÇ\n",
      "‚îÇ Il est ministre de l'√âcologie,‚Ä¶ ‚îÜ Philippe Martin (homme ‚îÜ 0.666667  ‚îÜ 1.0    ‚îÜ 0.8      ‚îÇ\n",
      "‚îÇ                                 ‚îÜ politiq‚Ä¶               ‚îÜ           ‚îÜ        ‚îÜ          ‚îÇ\n",
      "‚îÇ Il est maire de Belfort depuis‚Ä¶ ‚îÜ Damien Meslot          ‚îÜ 0.666667  ‚îÜ 1.0    ‚îÜ 0.8      ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "‚ùå Top 5 Worst Predictions (by F1):\n",
      "shape: (5, 5)\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ sentence                        ‚îÜ article        ‚îÜ precision ‚îÜ recall ‚îÜ f1  ‚îÇ\n",
      "‚îÇ ---                             ‚îÜ ---            ‚îÜ ---       ‚îÜ ---    ‚îÜ --- ‚îÇ\n",
      "‚îÇ str                             ‚îÜ str            ‚îÜ f64       ‚îÜ f64    ‚îÜ f64 ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ Ce dernier mena les r√©voltes d‚Ä¶ ‚îÜ Andr√© Samitier ‚îÜ 0.0       ‚îÜ 0.0    ‚îÜ 0.0 ‚îÇ\n",
      "‚îÇ Il est Instituteur √† Gargenvil‚Ä¶ ‚îÜ Andr√© Samitier ‚îÜ 0.0       ‚îÜ 0.0    ‚îÜ 0.0 ‚îÇ\n",
      "‚îÇ Son assistante parlementaire, ‚Ä¶ ‚îÜ Andr√© Samitier ‚îÜ 0.0       ‚îÜ 0.0    ‚îÜ 0.0 ‚îÇ\n",
      "‚îÇ Pierre-Jean Samot, n√© le √† For‚Ä¶ ‚îÜ Pierre Samot   ‚îÜ 0.0       ‚îÜ 0.0    ‚îÜ 0.0 ‚îÇ\n",
      "‚îÇ Pierre Samot est un artisan ma‚Ä¶ ‚îÜ Pierre Samot   ‚îÜ 0.0       ‚îÜ 0.0    ‚îÜ 0.0 ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "# Cell: Evaluate Predictions vs Ground Truth\n",
    "\n",
    "def evaluate_predictions(predictions, df):\n",
    "    \"\"\"\n",
    "    Evaluate predicted links against ground truth.\n",
    "    Computes precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    total_predicted = 0\n",
    "    total_ground_truth = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for pred in predictions:\n",
    "        ground_truth_ids = set(link['target_id'] for link in pred['ground_truth_links'])\n",
    "        \n",
    "        # Collect all predicted target IDs from similar phrases\n",
    "        predicted_ids = set()\n",
    "        for similar in pred['similar_linkable_phrases']:\n",
    "            predicted_ids.update(similar['target_ids'])\n",
    "        \n",
    "        # Calculate matches\n",
    "        correct = ground_truth_ids & predicted_ids\n",
    "        \n",
    "        total_ground_truth += len(ground_truth_ids)\n",
    "        total_predicted += len(predicted_ids)\n",
    "        total_correct += len(correct)\n",
    "        \n",
    "        # Per-sentence metrics\n",
    "        precision = len(correct) / len(predicted_ids) if predicted_ids else 0\n",
    "        recall = len(correct) / len(ground_truth_ids) if ground_truth_ids else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'sentence': pred['sentence'][:100],\n",
    "            'article': pred['article_title'],\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'num_predicted': len(predicted_ids),\n",
    "            'num_ground_truth': len(ground_truth_ids),\n",
    "            'num_correct': len(correct)\n",
    "        })\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_precision = total_correct / total_predicted if total_predicted > 0 else 0\n",
    "    overall_recall = total_correct / total_ground_truth if total_ground_truth > 0 else 0\n",
    "    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä EVALUATION RESULTS (Unseen Test Set)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüìà Overall Metrics:\")\n",
    "    print(f\"   Precision: {overall_precision:.3f} ({total_correct}/{total_predicted})\")\n",
    "    print(f\"   Recall:    {overall_recall:.3f} ({total_correct}/{total_ground_truth})\")\n",
    "    print(f\"   F1 Score:  {overall_f1:.3f}\")\n",
    "    print(f\"\\nüìù Test Set Size:\")\n",
    "    print(f\"   Sentences: {len(predictions):,}\")\n",
    "    print(f\"   Avg ground truth links per sentence: {total_ground_truth/len(predictions):.2f}\")\n",
    "    print(f\"   Avg predicted links per sentence: {total_predicted/len(predictions):.2f}\")\n",
    "    \n",
    "    # Show best and worst examples\n",
    "    results_df = pl.DataFrame(results).sort(\"f1\", descending=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Top 5 Best Predictions (by F1):\")\n",
    "    print(results_df.head(5).select(['sentence', 'article', 'precision', 'recall', 'f1']))\n",
    "    \n",
    "    print(f\"\\n‚ùå Top 5 Worst Predictions (by F1):\")\n",
    "    print(results_df.tail(5).select(['sentence', 'article', 'precision', 'recall', 'f1']))\n",
    "    \n",
    "    return results_df, {\n",
    "        'precision': overall_precision,\n",
    "        'recall': overall_recall,\n",
    "        'f1': overall_f1\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "results_df, metrics = evaluate_predictions(test_predictions, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f24e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç DETAILED PREDICTION ANALYSIS (Unseen Articles)\n",
      "================================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 1: Article 'Yahoo! Messenger'\n",
      "Sentence: 'Messenger proposait des environnements (d√©cors), un grand nombre d'√©motic√¥nes, la possibilit√© de communiquer oralement avec un microphone et de pouvoi...'\n",
      "\n",
      "‚úÖ Ground Truth Links (1):\n",
      "   üîó 'Yahoo!' ‚Üí Yahoo!\n",
      "\n",
      "üîÆ Predicted Links (5 similar phrases found):\n",
      "\n",
      "   Match 1 | Similarity: 0.874\n",
      "   From: 'Liste de logiciels libres'\n",
      "   Similar: 'Les mod√®les les plus perfectionn√©s permettent de jouer tout en discutant par oral en se voyant gr√¢ce...'\n",
      "   Suggests:\n",
      "      ‚úó 'bases de donn√©es' ‚Üí Base de donn√©es\n",
      "\n",
      "   Match 2 | Similarity: 0.872\n",
      "   From: 'Webcam'\n",
      "   Similar: 'L'utilisation de la webcam pour la visiophonie se diffuse pour les communications personnelles entre...'\n",
      "   Suggests:\n",
      "      ‚úó '1991' ‚Üí 1991\n",
      "\n",
      "   Match 3 | Similarity: 0.870\n",
      "   From: 'Communication humaine'\n",
      "   Similar: 'Les messageries √©lectroniques, l'internet‚Ä¶ permettent d'atteindre des groupes de personnes, et de fa...'\n",
      "   Suggests:\n",
      "      ‚úó 'sommet de la Terre' ‚Üí ID 27722\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 2: Article 'Yahoo! Messenger'\n",
      "Sentence: 'Il donnait √©galement acc√®s aux services Yahoo (m√©t√©o, bourse, information, r√©sultats sportifs, etc.)...'\n",
      "\n",
      "‚úÖ Ground Truth Links (1):\n",
      "   üîó 'Windows' ‚Üí Microsoft Windows\n",
      "\n",
      "üîÆ Predicted Links (5 similar phrases found):\n",
      "\n",
      "   Match 1 | Similarity: 0.859\n",
      "   From: 'Natural'\n",
      "   Similar: 'Ce langage fut cr√©√© au d√©part pour permettre l'acc√®s aux bases de donn√©es Adabas (du m√™me √©diteur)...'\n",
      "   Suggests:\n",
      "      ‚úì 'Windows' ‚Üí Microsoft Windows\n",
      "\n",
      "   Match 2 | Similarity: 0.858\n",
      "   From: 'Serveur informatique'\n",
      "   Similar: 'Il offre √† l'informateur le moyen de mettre √† disposition ses informations, et offre aux usagers les...'\n",
      "   Suggests:\n",
      "      ‚úó 'proxy' ‚Üí Proxy\n",
      "\n",
      "   Match 3 | Similarity: 0.856\n",
      "   From: 'Bible'\n",
      "   Similar: 'Ce syst√®me permet de faire correspondre les versions h√©bra√Øque, grecque, latine et autres (pour auta...'\n",
      "   Suggests:\n",
      "      ‚úó 'Pentateuque' ‚Üí Pentateuque\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 3: Article '1488'\n",
      "Sentence: '\"La Dame √† l'hermine\" (Portrait de Cecilia Gallerani), de L√©onard de Vinci, peinte vers 1488-1490 [ la peinture en 1488] sur \n",
      "[ modifier] \n",
      "L'ann√©e 148...'\n",
      "\n",
      "‚úÖ Ground Truth Links (1):\n",
      "   üîó 'L√©onard de Vinci' ‚Üí L√©onard de Vinci\n",
      "\n",
      "üîÆ Predicted Links (5 similar phrases found):\n",
      "\n",
      "   Match 1 | Similarity: 0.872\n",
      "   From: '1508'\n",
      "   Similar: 'Plafond de la chapelle Sixtine, de Michel-Ange \n",
      "[ la peinture en 1508] sur \n",
      "[ modifier] \n",
      "L'ann√©e 150...'\n",
      "   Suggests:\n",
      "      ‚úó 'Michel-Ange' ‚Üí Michel-Ange\n",
      "\n",
      "   Match 2 | Similarity: 0.872\n",
      "   From: '1508'\n",
      "   Similar: 'Plafond de la chapelle Sixtine, de Michel-Ange \n",
      "[ la peinture en 1508] sur \n",
      "[ modifier] \n",
      "L'ann√©e 150...'\n",
      "   Suggests:\n",
      "      ‚úó 'Michel-Ange' ‚Üí Michel-Ange\n",
      "\n",
      "   Match 3 | Similarity: 0.861\n",
      "   From: '1848'\n",
      "   Similar: 'Lithographie de Marie-C√©cile Goldsmid, imprim√©e le 29 novembre 1849.[ modifier] \n",
      "L'ann√©e 1848 est un...'\n",
      "   Suggests:\n",
      "      ‚úó 'ann√©e bissextile' ‚Üí Ann√©e bissextile\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 4: Article '1487'\n",
      "Sentence: '[ la peinture en 1487] sur \n",
      "[ modifier] \n",
      "L'ann√©e 1487 est une ann√©e commune qui commence un lundi....'\n",
      "\n",
      "‚úÖ Ground Truth Links (1):\n",
      "   üîó 'Venise' ‚Üí Venise\n",
      "\n",
      "üîÆ Predicted Links (5 similar phrases found):\n",
      "\n",
      "   Match 1 | Similarity: 0.902\n",
      "   From: '1877'\n",
      "   Similar: '[ modifier] \n",
      "L'ann√©e 1877 est une ann√©e commune qui commence un lundi...'\n",
      "   Suggests:\n",
      "      ‚úó '24 septembre' ‚Üí 24 septembre\n",
      "\n",
      "   Match 2 | Similarity: 0.901\n",
      "   From: '1687'\n",
      "   Similar: '12 ao√ªt : bataille de Moh√°cs, sch√©ma d'√©poque\n",
      "[ modifier] \n",
      "L'ann√©e 1687 est une ann√©e commune qui co...'\n",
      "   Suggests:\n",
      "      ‚úó '12 ao√ªt' ‚Üí 12 ao√ªt\n",
      "\n",
      "   Match 3 | Similarity: 0.899\n",
      "   From: '1787'\n",
      "   Similar: '17 septembre : constitution des √âtats-Unis\n",
      "[ modifier] \n",
      "L'ann√©e 1787 est une ann√©e commune qui comme...'\n",
      "   Suggests:\n",
      "      ‚úó '17 septembre' ‚Üí 17 septembre\n",
      "      ‚úó 'constitution des √âtats-Unis' ‚Üí Constitution des √âtats-Unis\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 5: Article '1484'\n",
      "Sentence: '[ la peinture en 1484] sur \n",
      "[ modifier] \n",
      "L'ann√©e 1484 est une ann√©e bissextile qui commence un jeudi....'\n",
      "\n",
      "‚úÖ Ground Truth Links (1):\n",
      "   üîó 'Bruges' ‚Üí Bruges\n",
      "\n",
      "üîÆ Predicted Links (5 similar phrases found):\n",
      "\n",
      "   Match 1 | Similarity: 0.923\n",
      "   From: '1504'\n",
      "   Similar: '[ la peinture en 1504] sur \n",
      "[ modifier] \n",
      "L'ann√©e 1504 est une ann√©e bissextile qui commence un lundi...'\n",
      "   Suggests:\n",
      "      ‚úó 'Michel-Ange' ‚Üí Michel-Ange\n",
      "\n",
      "   Match 2 | Similarity: 0.923\n",
      "   From: '1504'\n",
      "   Similar: '[ la peinture en 1504] sur \n",
      "[ modifier] \n",
      "L'ann√©e 1504 est une ann√©e bissextile qui commence un lundi...'\n",
      "   Suggests:\n",
      "      ‚úó 'Michel-Ange' ‚Üí Michel-Ange\n",
      "\n",
      "   Match 3 | Similarity: 0.904\n",
      "   From: '1688'\n",
      "   Similar: '[ modifier] \n",
      "L'ann√©e 1688 est une ann√©e bissextile qui commence un jeudi....'\n",
      "   Suggests:\n",
      "      ‚úó 'Londres' ‚Üí Londres\n"
     ]
    }
   ],
   "source": [
    "# Cell: Detailed Analysis of Sample Predictions\n",
    "\n",
    "def display_detailed_comparison(predictions, df, num_samples=3):\n",
    "    \"\"\"Show detailed comparison of predictions vs ground truth\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üîç DETAILED PREDICTION ANALYSIS (Unseen Articles)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, pred in enumerate(predictions[:num_samples]):\n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"Example {i+1}: Article '{pred['article_title']}'\")\n",
    "        print(f\"Sentence: '{pred['sentence'][:150]}...'\")\n",
    "        \n",
    "        # Ground truth\n",
    "        print(f\"\\n‚úÖ Ground Truth Links ({pred['num_ground_truth']}):\")\n",
    "        for gt_link in pred['ground_truth_links']:\n",
    "            target_title = df.filter(pl.col(\"id\") == gt_link['target_id']).select(\"title\").to_series()\n",
    "            target_name = target_title[0] if len(target_title) > 0 else f\"ID {gt_link['target_id']}\"\n",
    "            print(f\"   üîó '{gt_link['anchor']}' ‚Üí {target_name}\")\n",
    "        \n",
    "        # Predictions\n",
    "        print(f\"\\nüîÆ Predicted Links ({pred['num_matches']} similar phrases found):\")\n",
    "        if pred['num_matches'] == 0:\n",
    "            print(\"   ‚ö†Ô∏è  No predictions (no similar phrases above threshold)\")\n",
    "        else:\n",
    "            for j, similar in enumerate(pred['similar_linkable_phrases'][:3]):\n",
    "                print(f\"\\n   Match {j+1} | Similarity: {similar['similarity_score']:.3f}\")\n",
    "                print(f\"   From: '{similar['source_article']}'\")\n",
    "                print(f\"   Similar: '{similar['similar_sentence'][:100]}...'\")\n",
    "                print(f\"   Suggests:\")\n",
    "                for target_id, anchor in zip(similar['target_ids'][:3], similar['anchors'][:3]):\n",
    "                    target_title = df.filter(pl.col(\"id\") == target_id).select(\"title\").to_series()\n",
    "                    target_name = target_title[0] if len(target_title) > 0 else f\"ID {target_id}\"\n",
    "                    \n",
    "                    # Check if this matches ground truth\n",
    "                    is_correct = target_id in [gt['target_id'] for gt in pred['ground_truth_links']]\n",
    "                    marker = \"‚úì\" if is_correct else \"‚úó\"\n",
    "                    print(f\"      {marker} '{anchor}' ‚Üí {target_name}\")\n",
    "\n",
    "# Display detailed analysis\n",
    "display_detailed_comparison(test_predictions, df, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4ee4d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved test results to 'test_results_unseen_articles.parquet'\n"
     ]
    }
   ],
   "source": [
    "# Cell: Optional - Save Test Results for Later Analysis\n",
    "\n",
    "# Save test predictions to parquet for later analysis\n",
    "test_results = []\n",
    "for pred in test_predictions:\n",
    "    test_results.append({\n",
    "        'article_id': pred['article_id'],\n",
    "        'article_title': pred['article_title'],\n",
    "        'sentence': pred['sentence'],\n",
    "        'num_ground_truth': pred['num_ground_truth'],\n",
    "        'num_predictions': pred['num_matches'],\n",
    "        'ground_truth_anchors': [gt['anchor'] for gt in pred['ground_truth_links']],\n",
    "        'ground_truth_targets': [gt['target_id'] for gt in pred['ground_truth_links']]\n",
    "    })\n",
    "\n",
    "test_results_df = pl.DataFrame(test_results)\n",
    "test_results_df.write_parquet(\"test_results_unseen_articles.parquet\")\n",
    "print(f\"üíæ Saved test results to 'test_results_unseen_articles.parquet'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
