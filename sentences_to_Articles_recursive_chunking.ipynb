{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ca5a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/ltran/.conda/envs/DCL_WIKI_RAG/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import polars as pl\n",
    "#If you already saved the model locally and are using docker\n",
    "client = QdrantClient(host=\"localhost\", port=6333, prefer_grpc=True, timeout=1000)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"models/multilingual-e5-large\", device=device)\n",
    "\n",
    "# Run docker container before\n",
    "# docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v /home/Loris/EPFL/MA3/ML/project2/project2Rag/qdrant_storage:/qdrant/storage qdrant/qdrant:latest\n",
    "\n",
    "# VERY IMPORTANT : to query the remote qdrand docker container over ssh, run the following in a terminale before:\n",
    "# ssh -L 6333:localhost:6333 -L 6334:localhost:6334 <your username>@dclgpusrv.epfl.ch -N\n",
    "#Make sure to have your ssh keys before\n",
    "#This forwards the qdrant ports to your local machine\n",
    "#If you are outside EPFL network, use VPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9929b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles stored: 2800041\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http import models as rest\n",
    "#get articles count from Qdrant DB \n",
    "\n",
    "collection_name = \"wikipedia_fr_chunks\"\n",
    "total = client.count(collection_name=collection_name, exact=True).count\n",
    "print(f\"Total articles stored: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2cb083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Dataset loaded as LazyFrame (not in memory)\n",
      "   Schema: Schema({'id': Int64, 'title': String, 'text': String, 'links': List(Struct({'full_url': String, 'start_idx': Int64, 'anchor': String, 'href_raw': String, 'href_decoded': String})), 'link_count': UInt32, 'text_withoutHref': String})\n",
      "\n",
      "   Link structure example:\n",
      "Antoine Meillet, n√© le √† &lt;a href=\"Moulins%20%28Allier%29\"&gt;Moulins&lt;/a&gt; (&lt;a href=\"Allier%20%28d%C3%A9partement%29\"&gt;Allier&lt;/a&gt;) et mort le √† &lt;a href=\"Ch%C3%A2teaumeillant\"&gt;Ch√¢teaumeillant&lt;/a&gt; (&lt;a href=\"Cher%20%28d%C3%A9partement%29\"&gt;Cher&lt;/a&gt;), est un &lt;a href=\"Philologie\"&gt;philologue&lt;/a&gt; fran√ßais, le principal &lt;a href=\"liste%20de%20linguistes\"&gt;linguiste&lt;/a&gt; fran√ßais des premi√®res d√©cennies du XX¬†si√®cle.\n",
      "Biographie.\n",
      "Enfance et formation.\n",
      "Paul Jules Antoine Meillet est d'origine &lt;a href=\"Allier%20%28d%C3%A9partement%29\"&gt;bourbonnaise&lt;/a&gt;, fils d'un &lt;a href=\"notaire\"&gt;notaire&lt;/a&gt; de &lt;a href=\"Ch%C3%A2teaumeillant\"&gt;Ch√¢teaumeillant&lt;/a&gt; (&lt;a href=\"Cher%20%28d%C3%A9partement%29\"&gt;Cher&lt;/a&gt;). Il na√Æt √† &lt;a href=\"Moulins%20%28Allier%29\"&gt;Moulins&lt;/a&gt; le 11 novembre 1866.\n",
      "Il passe son enfance √† Ch√¢teaumeillant, puis fait ses √©tudes secondaires au &lt;a href=\"lyc%C3%A9e%20Th%C3%A9odore-de-Banville\"&gt;lyc√©e&lt;/a&gt; de Moulins.\n",
      "√âtudiant √† partir de 1885 √† la &lt;a href=\"facult%C3%A9%20des%20lettres%20de%20Paris\"&gt;facult√© des lettres de Paris&lt;/a&gt; o√π il suit notamment les cours de &lt;a href=\"Louis%20Havet\"&gt;Louis Havet&lt;/a&gt;, il assiste √©galement √† ceux de &lt;a href=\"Michel%20Br%C3%A9al\"&gt;Michel Br√©al&lt;/a&gt; au &lt;a href=\"Coll%C3%A8ge%20de%20France\"&gt;Coll√®ge de France&lt;/a&gt; et de &lt;a href=\"Ferdinand%20de%20Saussure\"&gt;Ferdinand de Saussure&lt;/a&gt; √† l'&lt;a href=\"%C3%89cole%20pratique%20des%20hautes%20%C3%A9tudes\"&gt;√âcole pratique des hautes √©tudes&lt;/a&gt;.\n",
      "En 1889, il est major de l'&lt;a href=\"agr%C3%A9gation%20de%20grammaire\"&gt;agr√©gation de grammaire&lt;/a&gt;. En 1891, il fait son premier s√©jour en &lt;a href=\"Arm%C3%A9nie\"&gt;Arm√©nie&lt;/a&gt;, notamment √† &lt;a href=\"Etchmiadzin\"&gt;Etchmiadzin&lt;/a&gt; ; son projet est d'apprendre l'arm√©nien moderne et d'√©tudier d'anciens manuscrits.\n",
      "√Ä son retour, il assure √† la suite de Saussure le cours de &lt;a href=\"grammaire%20compar%C3%A9e\"&gt;grammaire compar√©e&lt;/a&gt;, qu'il compl√®te √† partir de 1894 par une conf√©rence sur les &lt;a href=\"langues%20persanes\"&gt;langues persanes&lt;/a&gt;.\n",
      "En 1897, il soutient sa th√®se pour le &lt;a href=\"Doctorat%20%C3%A8s%20lettres%20%28France%29\"&gt;doctorat √®s lettres&lt;/a&gt; \"(Recherches sur l'emploi du g√©nitif-accusatif en &lt;a href=\"vieux-slave\"&gt;vieux-slave&lt;/a&gt;)\".\n",
      "Carri√®re.\n",
      "En 1902, il succ√®de au linguiste &lt;a href=\"Auguste%20Carri%C3%A8re\"&gt;Auguste Carri√®re&lt;/a&gt; √† la chaire d'&lt;a href=\"arm%C3%A9nien\"&gt;arm√©nien&lt;/a&gt; de l'&lt;a href=\"Institut%20national%20des%20langues%20et%20civilisations%20orientales\"&gt;√âcole des langues orientales&lt;/a&gt;. En 1906, √† la suite de &lt;a href=\"Michel%20Br%C3%A9al\"&gt;Michel Br√©al&lt;/a&gt;, il prend la chaire de grammaire compar√©e du &lt;a href=\"Coll%C3%A8ge%20de%20France\"&gt;Coll√®ge de France&lt;/a&gt;, o√π il consacre ses cours √† l'histoire et √† la structure des &lt;a href=\"langues%20indo-europ%C3%A9ennes\"&gt;langues indo-europ√©ennes&lt;/a&gt; ; il abandonne alors son enseignement √† l'√âcole des langues orientales et se consacre d√©sormais √† la linguistique compar√©e au Coll√®ge de France, ainsi qu'√† l'&lt;a href=\"%C3%89cole%20pratique%20des%20hautes%20%C3%A9tudes\"&gt;√âcole pratique des hautes √©tudes&lt;/a&gt;.\n",
      "Secr√©taire de la &lt;a href=\"Soci%C3%A9t%C3%A9%20de%20linguistique%20de%20Paris\"&gt;Soci√©t√© de linguistique de Paris&lt;/a&gt;, il est √©lu √† l'&lt;a href=\"Acad%C3%A9mie%20des%20inscriptions%20et%20belles-lettres\"&gt;Acad√©mie des inscriptions et belles-lettres&lt;/a&gt; en 1924. Il pr√©side √©galement l'&lt;a href=\"Institut%20d%27%C3%A9tudes%20slaves\"&gt;Institut d'√©tudes slaves&lt;/a&gt; de 1921 √† sa mort.\n",
      "Il a form√© toute une g√©n√©ration de linguistes fran√ßais, parmi lesquels &lt;a href=\"%C3%89mile%20Benveniste\"&gt;√âmile Benveniste&lt;/a&gt;, &lt;a href=\"Marcel%20Cohen\"&gt;Marcel Cohen&lt;/a&gt;, &lt;a href=\"Georges%20Dum%C3%A9zil\"&gt;Georges Dum√©zil&lt;/a&gt;, &lt;a href=\"Lilias%20Homburger\"&gt;Lilias Homburger&lt;/a&gt;, &lt;a href=\"Andr%C3%A9%20Martinet\"&gt;Andr√© Martinet&lt;/a&gt;, &lt;a href=\"Aur%C3%A9lien%20Sauvageot\"&gt;Aur√©lien Sauvageot&lt;/a&gt;, &lt;a href=\"Lucien%20Tesni%C3%A8re\"&gt;Lucien Tesni√®re&lt;/a&gt;, le &lt;a href=\"Japonisation\"&gt;japonisant&lt;/a&gt; &lt;a href=\"Charles%20Haguenauer\"&gt;Charles Haguenauer&lt;/a&gt; ou &lt;a href=\"Joseph%20Vendryes\"&gt;Joseph Vendryes&lt;/a&gt;. Antoine Meillet devait diriger la th√®se de &lt;a href=\"Jean%20Paulhan\"&gt;Jean Paulhan&lt;/a&gt; sur la s√©mantique du proverbe et c'est lui qui d√©couvrit &lt;a href=\"Gustave%20Guillaume\"&gt;Gustave Guillaume&lt;/a&gt;.\n",
      "Il a influenc√© aussi un certain nombre de linguistes √©trangers. Il a √©galement √©t√© le premier √† identifier le ph√©nom√®ne de la &lt;a href=\"grammaticalisation\"&gt;grammaticalisation&lt;/a&gt;.\n",
      "Selon le &lt;a href=\"Linguistique\"&gt;linguiste&lt;/a&gt; allemand &lt;a href=\"Walter%20Porzig\"&gt;Walter Porzig&lt;/a&gt;, Meillet est un ¬´ grand pr√©curseur ¬ª. Il montre, par exemple, que, dans les &lt;a href=\"Dialecte\"&gt;dialectes&lt;/a&gt; indo-europ√©ens, les groupes indo-europ√©ens sont le r√©sultat historique d'une &lt;a href=\"Variation%20linguistique\"&gt;variation diatopique&lt;/a&gt;.\n",
      "L‚Äôacte de naissance de la &lt;a href=\"sociolinguistique\"&gt;sociolinguistique&lt;/a&gt; est sign√© par Antoine Meillet fondateur de la sociolinguistique qui s‚Äôest oppos√© au &lt;a href=\"Cours%20de%20linguistique%20g%C3%A9n%C3%A9rale\"&gt;Cours de linguistique g√©n√©rale&lt;/a&gt; de &lt;a href=\"Ferdinand%20de%20Saussure\"&gt;Ferdinand de Saussure&lt;/a&gt; d√®s son apparition en 1916 en le critiquant sur plusieurs plans.\n",
      "Il meurt en 1936 √† &lt;a href=\"Ch%C3%A2teaumeillant\"&gt;Ch√¢teaumeillant&lt;/a&gt; et est enterr√© au cimeti√®re de &lt;a href=\"Moulins%20%28Allier%29\"&gt;Moulins&lt;/a&gt; dans le caveau familial.\n",
      "√âtudes hom√©riques.\n",
      "√Ä la Sorbonne, Meillet supervise le travail de &lt;a href=\"Milman%20Parry\"&gt;Milman Parry&lt;/a&gt;. Meillet offre √† son √©tudiant l'opinion, nouvelle √† cette √©poque, que la structure formula√Øque de \"&lt;a href=\"l%27Iliade\"&gt;l'Iliade&lt;/a&gt;\" serait une cons√©quence directe de sa transmission orale. Ainsi, il le dirige vers l'√©tude de l'oralit√© dans son cadre natif et lui sugg√®re d'observer les m√©canismes d'une tradition orale vivante √† c√¥t√© du texte classique (\"&lt;a href=\"l%27Iliade\"&gt;l'Iliade&lt;/a&gt;\") qui est cens√© r√©sulter d'une telle tradition. En cons√©quence, Meillet pr√©sente Parry √† &lt;a href=\"Matija%20Murko\"&gt;Matija Murko&lt;/a&gt;, savant originaire de &lt;a href=\"Slov%C3%A9nie\"&gt;Slov√©nie&lt;/a&gt; qui avait longuement √©crit sur la tradition h√©ro√Øque √©pique dans les &lt;a href=\"Balkans\"&gt;Balkans&lt;/a&gt;, surtout en &lt;a href=\"Bosnie-Herz%C3%A9govine\"&gt;Bosnie-Herz√©govine&lt;/a&gt;. Par leurs recherches, dont les r√©sultats sont √† pr√©sent h√©berg√©s par l'universit√© de Harvard, Parry et son √©l√®ve, &lt;a href=\"Albert%20Lord\"&gt;Albert Lord&lt;/a&gt;, ont profond√©ment renouvel√© les √©tudes hom√©riques.\n",
      "Voir aussi.\n",
      "&lt;templatestyles src=\"Autres projets/styles.css\" /&gt;\n",
      "Sur les autres projets Wikimedia :\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Keep as LazyFrame - don't collect the full dataset!\n",
    "df_lazy = pl.scan_parquet(\"articles_fr_merged.parquet\").filter(\n",
    "    pl.col('link_count') > 0\n",
    ")\n",
    "\n",
    "# Only collect a small sample to verify structure\n",
    "sample = df_lazy.head(1).collect()\n",
    "\n",
    "print(f\"üìö Dataset loaded as LazyFrame (not in memory)\")\n",
    "print(f\"   Schema: {df_lazy.collect_schema()}\")\n",
    "print(f\"\\n   Link structure example:\")\n",
    "\n",
    "\n",
    "full_text = df_lazy.select(\"text\").head(1).collect().item()\n",
    "\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1f17e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['paragraph_index', 'word_count', 'source_article_id', 'chunk_type', 'sentence_group_index', 'source_article_title', 'global_chunk_id', 'text_preview', 'text'])\n"
     ]
    }
   ],
   "source": [
    "# Check payload structure of new collection\n",
    "sample = client.scroll(\n",
    "    collection_name=\"wikipedia_fr_chunks\",\n",
    "    limit=1,\n",
    "    with_payload=True,\n",
    "    with_vectors=False\n",
    ")[0]\n",
    "print(sample[0].payload.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81ce5997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó∫Ô∏è  Building URL ‚Üí ID mapping from 'wikipedia_fr_chunks'...\n",
      "‚úÖ Created 11,095,075 URL mappings from 2,230,038 unique articles (2,800,041 chunks)\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import re\n",
    "# from typing import List, Dict, Tuple\n",
    "# from urllib.parse import unquote\n",
    "\n",
    "# def create_url_to_id_mapping_from_qdrant(client, collection_name: str = \"wikipedia_fr_chunks\") -> Dict[str, int]:\n",
    "#     \"\"\"Build URL ‚Üí ID mapping using articles in Qdrant.\"\"\"\n",
    "#     from urllib.parse import quote\n",
    "    \n",
    "#     url_to_id = {}\n",
    "#     id_to_title = {}\n",
    "    \n",
    "#     print(f\"üó∫Ô∏è  Building URL ‚Üí ID mapping from '{collection_name}'...\")\n",
    "    \n",
    "#     offset = None\n",
    "#     batch_size = 1000\n",
    "#     total_processed = 0\n",
    "    \n",
    "#     while True:\n",
    "#         points, offset = client.scroll(\n",
    "#             collection_name=collection_name,\n",
    "#             limit=batch_size,\n",
    "#             offset=offset,\n",
    "#             with_payload=True,\n",
    "#             with_vectors=False\n",
    "#         )\n",
    "        \n",
    "#         if not points:\n",
    "#             break\n",
    "        \n",
    "#         for point in points:\n",
    "#             article_id = point.payload.get(\"id\")\n",
    "#             title = point.payload.get(\"title\", \"\")\n",
    "            \n",
    "#             if not article_id or not title:\n",
    "#                 continue\n",
    "            \n",
    "#             id_to_title[article_id] = title\n",
    "            \n",
    "#             # Create URL pattern variations\n",
    "#             patterns = [\n",
    "#                 title,\n",
    "#                 title.replace(\" \", \"_\"),\n",
    "#                 quote(title.replace(\" \", \"_\"), safe=\"\"),\n",
    "#                 title.lower(),\n",
    "#                 title.lower().replace(\" \", \"_\"),\n",
    "#                 title.lower().replace(\" \", \"%20\"),\n",
    "#             ]\n",
    "            \n",
    "#             for pattern in patterns:\n",
    "#                 url_to_id[pattern] = article_id\n",
    "        \n",
    "#         total_processed += len(points)\n",
    "        \n",
    "#         if offset is None:\n",
    "#             break\n",
    "    \n",
    "#     print(f\"‚úÖ Created {len(url_to_id):,} URL mappings from {total_processed:,} articles\")\n",
    "#     return url_to_id, id_to_title\n",
    "\n",
    "# # Build mappings\n",
    "# url_to_id, id_to_title = create_url_to_id_mapping_from_qdrant(client, \"wikipedia_fr_chunks\")\n",
    "\n",
    "import torch\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def create_url_to_id_mapping_from_qdrant(client, collection_name: str = \"wikipedia_fr_chunks\") -> Dict[str, int]:\n",
    "    \"\"\"Build URL ‚Üí ID mapping using articles in Qdrant.\"\"\"\n",
    "    from urllib.parse import quote\n",
    "    \n",
    "    url_to_id = {}\n",
    "    id_to_title = {}\n",
    "    \n",
    "    print(f\"üó∫Ô∏è  Building URL ‚Üí ID mapping from '{collection_name}'...\")\n",
    "    \n",
    "    offset = None\n",
    "    batch_size = 1000\n",
    "    total_processed = 0\n",
    "    seen_article_ids = set()  # Track unique articles (chunks share same source)\n",
    "    \n",
    "    while True:\n",
    "        points, offset = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=batch_size,\n",
    "            offset=offset,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        \n",
    "        if not points:\n",
    "            break\n",
    "        \n",
    "        for point in points:\n",
    "            # === FIX: Use correct field names for chunked collection ===\n",
    "            article_id = point.payload.get(\"source_article_id\")  # Changed from \"id\"\n",
    "            title = point.payload.get(\"source_article_title\", \"\")  # Changed from \"title\"\n",
    "            \n",
    "            if not article_id or not title:\n",
    "                continue\n",
    "            \n",
    "            # Skip if we've already processed this article (multiple chunks per article)\n",
    "            if article_id in seen_article_ids:\n",
    "                continue\n",
    "            seen_article_ids.add(article_id)\n",
    "            \n",
    "            id_to_title[article_id] = title\n",
    "            \n",
    "            # Create URL pattern variations\n",
    "            patterns = [\n",
    "                title,\n",
    "                title.replace(\" \", \"_\"),\n",
    "                quote(title.replace(\" \", \"_\"), safe=\"\"),\n",
    "                title.lower(),\n",
    "                title.lower().replace(\" \", \"_\"),\n",
    "                title.lower().replace(\" \", \"%20\"),\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                url_to_id[pattern] = article_id\n",
    "        \n",
    "        total_processed += len(points)\n",
    "        \n",
    "        if offset is None:\n",
    "            break\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(url_to_id):,} URL mappings from {len(seen_article_ids):,} unique articles ({total_processed:,} chunks)\")\n",
    "    return url_to_id, id_to_title\n",
    "\n",
    "# Build mappings\n",
    "url_to_id, id_to_title = create_url_to_id_mapping_from_qdrant(client, \"wikipedia_fr_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52156768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 11,095,075 URL mappings to url_to_id_mapping.pkl\n",
      "‚úÖ Saved 2,230,038 ID‚Üítitle mappings to id_to_title_mapping.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save mappings to disk\n",
    "with open(\"url_to_id_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(url_to_id, f)\n",
    "\n",
    "with open(\"id_to_title_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id_to_title, f)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(url_to_id):,} URL mappings to url_to_id_mapping.pkl\")\n",
    "print(f\"‚úÖ Saved {len(id_to_title):,} ID‚Üítitle mappings to id_to_title_mapping.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f693234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading cached mappings...\n",
      "‚úÖ Loaded 11,095,075 URL mappings, 2,230,038 ID‚Üítitle mappings\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Load from disk if available, otherwise rebuild\n",
    "if Path(\"url_to_id_mapping.pkl\").exists() and Path(\"id_to_title_mapping.pkl\").exists():\n",
    "    print(\"üìÇ Loading cached mappings...\")\n",
    "    with open(\"url_to_id_mapping.pkl\", \"rb\") as f:\n",
    "        url_to_id = pickle.load(f)\n",
    "    with open(\"id_to_title_mapping.pkl\", \"rb\") as f:\n",
    "        id_to_title = pickle.load(f)\n",
    "    print(f\"‚úÖ Loaded {len(url_to_id):,} URL mappings, {len(id_to_title):,} ID‚Üítitle mappings\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No cached mappings found, rebuilding...\")\n",
    "    url_to_id, id_to_title = create_url_to_id_mapping_from_qdrant(client, \"wikipedia_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15be6759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample result:\n",
      "Original text (first 200 chars): Antoine Meillet, n√© le √† &lt;a href=\"Moulins%20%28Allier%29\"&gt;Moulins&lt;/a&gt; (&lt;a href=\"Allier%20%28d%C3%A9partement%29\"&gt;Allier&lt;/a&gt;) et mort le √† &lt;a href=\"Ch%C3%A2teaumeillant\"&gt;C...\n",
      "\n",
      "Clean text (first 200 chars): Antoine Meillet, n√© le √† Moulins (Allier) et mort le √† Ch√¢teaumeillant (Cher), est un philologue fran√ßais, le principal linguiste fran√ßais des premi√®res d√©cennies du XX¬†si√®cle.\n",
      "Biographie.\n",
      "Enfance et ...\n",
      "\n",
      "Extracted links (65 total):\n",
      "  - 'Moulins' ‚Üí Moulins (Allier) (pos: 25)\n",
      "  - 'Allier' ‚Üí Allier (d√©partement) (pos: 34)\n",
      "  - 'Ch√¢teaumeillant' ‚Üí Ch√¢teaumeillant (pos: 55)\n",
      "  - 'Cher' ‚Üí Cher (d√©partement) (pos: 72)\n",
      "  - 'philologue' ‚Üí Philologie (pos: 86)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import polars as pl\n",
    "from urllib.parse import unquote\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def extract_links_and_clean_text(text: str) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Extract href links from text and return clean text with anchors only.\n",
    "    \n",
    "    Input text format:\n",
    "    'Antoine Meillet, n√© le √† <a href=\"Moulins%20%28Allier%29\">Moulins</a>...'\n",
    "    \n",
    "    Returns:\n",
    "    - clean_text: 'Antoine Meillet, n√© le √† Moulins...'\n",
    "    - links: [{'anchor': 'Moulins', 'href_raw': 'Moulins%20%28Allier%29', \n",
    "               'href_decoded': 'Moulins (Allier)', 'start_idx': 25, 'end_idx': 32}]\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\", []\n",
    "    \n",
    "    # Pattern to match <a href=\"...\">anchor</a>\n",
    "    # Handles both &lt;a href=... and <a href=... formats\n",
    "    href_pattern = r'(?:&lt;|<)a\\s+href=\"([^\"]*)\"(?:&gt;|>)(.*?)(?:&lt;|<)/a(?:&gt;|>)'\n",
    "    \n",
    "    links = []\n",
    "    clean_text = text\n",
    "    offset = 0  # Track position offset as we replace tags\n",
    "    \n",
    "    for match in re.finditer(href_pattern, text, re.IGNORECASE | re.DOTALL):\n",
    "        href_raw = match.group(1)\n",
    "        anchor = match.group(2)\n",
    "        \n",
    "        # Decode URL-encoded href\n",
    "        try:\n",
    "            href_decoded = unquote(href_raw)\n",
    "        except:\n",
    "            href_decoded = href_raw\n",
    "        \n",
    "        # Calculate position in clean text\n",
    "        # Original position minus the offset from previous replacements\n",
    "        original_start = match.start()\n",
    "        clean_start = original_start - offset\n",
    "        \n",
    "        # Store link info\n",
    "        links.append({\n",
    "            'anchor': anchor,\n",
    "            'href_raw': href_raw,\n",
    "            'href_decoded': href_decoded,\n",
    "            'start_idx': clean_start,\n",
    "            'end_idx': clean_start + len(anchor)\n",
    "        })\n",
    "        \n",
    "        # Update offset: we're removing the full tag and keeping only anchor\n",
    "        tag_length = match.end() - match.start()\n",
    "        anchor_length = len(anchor)\n",
    "        offset += tag_length - anchor_length\n",
    "    \n",
    "    # Replace all href tags with just the anchor text\n",
    "    clean_text = re.sub(href_pattern, r'\\2', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    return clean_text, links\n",
    "\n",
    "\n",
    "def process_text_column(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a single text entry and return clean text + links.\n",
    "    For use with Polars map_elements.\n",
    "    \"\"\"\n",
    "    clean_text, links = extract_links_and_clean_text(text)\n",
    "    return {\n",
    "        'text_clean': clean_text,\n",
    "        'extracted_links': links\n",
    "    }\n",
    "\n",
    "\n",
    "# Create new LazyFrame with extracted links and clean text\n",
    "# This processes the 'text' column which contains the href tags\n",
    "\n",
    "df_with_links = (\n",
    "    df_lazy\n",
    "    .with_columns([\n",
    "        # Extract links and clean text using map_elements\n",
    "        pl.col('text').map_elements(\n",
    "            lambda x: extract_links_and_clean_text(x)[0] if x else \"\",\n",
    "            return_dtype=pl.Utf8\n",
    "        ).alias('text_clean'),\n",
    "        \n",
    "        pl.col('text').map_elements(\n",
    "            lambda x: extract_links_and_clean_text(x)[1] if x else [],\n",
    "            return_dtype=pl.List(pl.Struct([\n",
    "                pl.Field('anchor', pl.Utf8),\n",
    "                pl.Field('href_raw', pl.Utf8),\n",
    "                pl.Field('href_decoded', pl.Utf8),\n",
    "                pl.Field('start_idx', pl.Int64),\n",
    "                pl.Field('end_idx', pl.Int64)\n",
    "            ]))\n",
    "        ).alias('extracted_links')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Test on a sample\n",
    "sample = df_with_links.head(1).collect()\n",
    "print(\"Sample result:\")\n",
    "print(f\"Original text (first 200 chars): {sample['text'][0][:200]}...\")\n",
    "print(f\"\\nClean text (first 200 chars): {sample['text_clean'][0][:200]}...\")\n",
    "print(f\"\\nExtracted links ({len(sample['extracted_links'][0])} total):\")\n",
    "for link in sample['extracted_links'][0][:5]:\n",
    "    print(f\"  - '{link['anchor']}' ‚Üí {link['href_decoded']} (pos: {link['start_idx']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a569b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85d9fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# def extract_sentences_with_links_by_anchor(\n",
    "#     text: str,\n",
    "#     links: list\n",
    "# ) -> List[Dict]:\n",
    "#     \"\"\"Extract sentences and match links by anchor text presence (whole word match).\"\"\"\n",
    "#     if not text or not links or len(links) == 0:\n",
    "#         return []\n",
    "    \n",
    "#     # Split into sentences\n",
    "#     sentence_pattern = r'(?<=[.!?])\\s+'\n",
    "#     sentences = re.split(sentence_pattern, text)\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         if not sentence.strip() or len(sentence) < 20:\n",
    "#             continue\n",
    "        \n",
    "#         links_in_sent = []\n",
    "#         seen_links = set()\n",
    "        \n",
    "#         for link in links:\n",
    "#             anchor = link.get('anchor', '')\n",
    "#             href_decoded = link.get('href_decoded', '')\n",
    "            \n",
    "#             if not anchor or len(anchor) < 2:\n",
    "#                 continue\n",
    "            \n",
    "#             # Use word boundary matching to avoid substring matches\n",
    "#             # Escape special regex characters in anchor\n",
    "#             escaped_anchor = re.escape(anchor)\n",
    "#             # Match as whole word (with word boundaries)\n",
    "#             pattern = r'\\b' + escaped_anchor + r'\\b'\n",
    "            \n",
    "#             if re.search(pattern, sentence, re.IGNORECASE):\n",
    "#                 link_key = (anchor.lower(), href_decoded.lower())\n",
    "#                 if link_key not in seen_links:\n",
    "#                     seen_links.add(link_key)\n",
    "#                     links_in_sent.append({\n",
    "#                         'anchor': anchor,\n",
    "#                         'href_decoded': href_decoded,\n",
    "#                     })\n",
    "        \n",
    "#         if links_in_sent:\n",
    "#             results.append({\n",
    "#                 'sentence': sentence.strip(),\n",
    "#                 'links_in_sentence': links_in_sent,\n",
    "#                 'num_links': len(links_in_sent)\n",
    "#             })\n",
    "    \n",
    "#     return results\n",
    "\n",
    "def extract_sentences_with_links_by_position(\n",
    "    clean_text: str,\n",
    "    extracted_links: List[Dict]\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract sentences and match links by their character position in clean text.\n",
    "    \n",
    "    This ensures only links that were ACTUALLY in that sentence (in the original\n",
    "    Wikipedia markup) are included as ground truth.\n",
    "    \"\"\"\n",
    "    if not clean_text or not extracted_links or len(extracted_links) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Sort links by position\n",
    "    sorted_links = sorted(extracted_links, key=lambda x: x.get('start_idx', 0))\n",
    "    \n",
    "    # Split into sentences\n",
    "    sentence_pattern = r'(?<=[.!?])\\s+'\n",
    "    sentences = re.split(sentence_pattern, clean_text)\n",
    "    \n",
    "    results = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if not sentence.strip() or len(sentence) < 20:\n",
    "            # Still need to track position even for skipped sentences\n",
    "            idx = clean_text.find(sentence, current_pos)\n",
    "            if idx != -1:\n",
    "                current_pos = idx + len(sentence)\n",
    "            continue\n",
    "        \n",
    "        # Find sentence boundaries in clean text\n",
    "        sent_start = clean_text.find(sentence, current_pos)\n",
    "        if sent_start == -1:\n",
    "            continue\n",
    "        sent_end = sent_start + len(sentence)\n",
    "        current_pos = sent_end\n",
    "        \n",
    "        # Find all links whose position falls within this sentence\n",
    "        links_in_sent = []\n",
    "        for link in sorted_links:\n",
    "            link_start = link.get('start_idx', -1)\n",
    "            link_end = link.get('end_idx', -1)\n",
    "            \n",
    "            # Check if link position falls within sentence boundaries\n",
    "            if sent_start <= link_start < sent_end:\n",
    "                links_in_sent.append({\n",
    "                    'anchor': link.get('anchor', ''),\n",
    "                    'href_decoded': link.get('href_decoded', ''),\n",
    "                    'href_raw': link.get('href_raw', ''),\n",
    "                    'position': link_start\n",
    "                })\n",
    "        \n",
    "        if links_in_sent:\n",
    "            results.append({\n",
    "                'sentence': sentence.strip(),\n",
    "                'links_in_sentence': links_in_sent,\n",
    "                'num_links': len(links_in_sent),\n",
    "                'start_pos': sent_start,\n",
    "                'end_pos': sent_end\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08af577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')  # Run once if needed\n",
    "\n",
    "def paragraph_chunk(text: str, min_length: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text on paragraph breaks (double newlines).\n",
    "    Filter out very short paragraphs.\n",
    "    \"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    # Also split on single newlines followed by headers (common in Wikipedia)\n",
    "    result = []\n",
    "    for p in paragraphs:\n",
    "        # Split further on section headers (lines ending with period or colon followed by newline)\n",
    "        sub_parts = re.split(r'\\n(?=[A-Z])', p)\n",
    "        for part in sub_parts:\n",
    "            if len(part.strip()) >= min_length:\n",
    "                result.append(part.strip())\n",
    "    return result\n",
    "\n",
    "\n",
    "def sentence_chunk(text: str, max_words: int = 150, min_words: int = 20) -> List[str]:\n",
    "    \"\"\"\n",
    "    Group sentences into chunks under a specified word count.\n",
    "    Preserves complete sentences.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sentences = sent_tokenize(text, language='french')\n",
    "    except:\n",
    "        # Fallback to simple regex if NLTK fails\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    buffer = []\n",
    "    length = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        count = len(sent.split())\n",
    "        if length + count > max_words and buffer:\n",
    "            chunk_text = \" \".join(buffer)\n",
    "            if len(chunk_text.split()) >= min_words:\n",
    "                chunks.append(chunk_text)\n",
    "            buffer = []\n",
    "            length = 0\n",
    "        buffer.append(sent)\n",
    "        length += count\n",
    "    \n",
    "    # Don't forget the last buffer\n",
    "    if buffer:\n",
    "        chunk_text = \" \".join(buffer)\n",
    "        if len(chunk_text.split()) >= min_words:\n",
    "            chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def recursive_chunk(text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursive chunking: try paragraphs first, then sentences, then words.\n",
    "    \"\"\"\n",
    "    # First try paragraph-based\n",
    "    paragraphs = paragraph_chunk(text, min_length=50)\n",
    "    \n",
    "    chunks = []\n",
    "    for para in paragraphs:\n",
    "        word_count = len(para.split())\n",
    "        if word_count <= chunk_size:\n",
    "            chunks.append(para)\n",
    "        else:\n",
    "            # Paragraph too long, split into sentences\n",
    "            sentence_chunks = sentence_chunk(para, max_words=chunk_size)\n",
    "            chunks.extend(sentence_chunks)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Creating test dataset with position-based matching...\n",
      "   Sampling 1,000 articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2698231/569278588.py:147: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n",
      "  .collect(streaming=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 1,000 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:04<00:00, 200.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Created test dataset:\n",
      "   Sentences: 51956\n",
      "   Articles processed: 1000\n",
      "   Total ground truth links: 117577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # import torch\n",
    "# # import re\n",
    "# # from typing import List, Dict, Tuple\n",
    "# # from urllib.parse import unquote\n",
    "\n",
    "# # def create_test_dataset_optimized(\n",
    "# #     df_lazy: pl.LazyFrame,\n",
    "# #     url_to_id: Dict[str, int],\n",
    "# #     max_sentences: int = 100,\n",
    "# #     min_links_per_sentence: int = 1,\n",
    "# #     sample_articles: int = 5000\n",
    "# # ) -> List[Dict]:\n",
    "# #     \"\"\"\n",
    "# #     Create test dataset using LazyFrame streaming.\n",
    "# #     \"\"\"\n",
    "# #     print(f\"üîç Creating test dataset (max {max_sentences} sentences)...\")\n",
    "# #     print(f\"   Sampling {sample_articles:,} articles from LazyFrame...\")\n",
    "    \n",
    "# #     sampled_df = (\n",
    "# #         df_lazy\n",
    "# #         .filter(pl.col('link_count') > 0)\n",
    "# #         .filter(pl.col('text_withoutHref').is_not_null())\n",
    "# #         .filter(pl.col('text_withoutHref').str.len_chars() > 100)\n",
    "# #         .select(['id', 'title', 'text_withoutHref', 'links'])\n",
    "# #         .head(sample_articles * 3)\n",
    "# #         .collect(streaming=True)\n",
    "# #         .sample(n=min(sample_articles, sample_articles * 3), shuffle=True, seed=42)\n",
    "# #     )\n",
    "    \n",
    "# #     print(f\"   Loaded {len(sampled_df):,} articles into memory\")\n",
    "    \n",
    "# #     test_data = []\n",
    "# #     articles_processed = 0\n",
    "    \n",
    "# #     for row in tqdm(sampled_df.iter_rows(named=True), total=len(sampled_df), desc=\"Processing\"):\n",
    "# #         if len(test_data) >= max_sentences:\n",
    "# #             break\n",
    "        \n",
    "# #         article_id = row[\"id\"]\n",
    "# #         article_title = row[\"title\"]\n",
    "# #         text = row.get(\"text_withoutHref\", \"\")\n",
    "# #         links_raw = row.get(\"links\", [])\n",
    "        \n",
    "# #         if links_raw is None:\n",
    "# #             continue\n",
    "# #         links = list(links_raw) if hasattr(links_raw, '__iter__') else []\n",
    "        \n",
    "# #         if not text or len(links) == 0:\n",
    "# #             continue\n",
    "        \n",
    "# #         # Use anchor-based matching instead of position-based\n",
    "# #         sentences_with_links = extract_sentences_with_links_by_anchor(text, links)\n",
    "        \n",
    "# #         for sent_data in sentences_with_links:\n",
    "# #             if len(test_data) >= max_sentences:\n",
    "# #                 break\n",
    "            \n",
    "# #             sentence = sent_data[\"sentence\"]\n",
    "# #             links_in_sent = sent_data[\"links_in_sentence\"]\n",
    "            \n",
    "# #             # Verify anchor is actually in sentence\n",
    "# #             ground_truth_links = []\n",
    "# #             for link in links_in_sent:\n",
    "# #                 href_decoded = link[\"href_decoded\"]\n",
    "# #                 anchor = link[\"anchor\"]\n",
    "                \n",
    "# #                 # Double-check anchor is in sentence\n",
    "# #                 if anchor.lower() not in sentence.lower():\n",
    "# #                     continue\n",
    "                \n",
    "# #                 if not anchor or not anchor.strip():\n",
    "# #                     continue\n",
    "                \n",
    "# #                 target_id = url_to_id.get(href_decoded)\n",
    "# #                 if not target_id:\n",
    "# #                     for variation in [\n",
    "# #                         href_decoded.replace(\"_\", \" \"),\n",
    "# #                         href_decoded.replace(\"%20\", \" \"),\n",
    "# #                         href_decoded.lower(),\n",
    "# #                         href_decoded.lower().replace(\"_\", \" \")\n",
    "# #                     ]:\n",
    "# #                         target_id = url_to_id.get(variation)\n",
    "# #                         if target_id:\n",
    "# #                             break\n",
    "                \n",
    "# #                 if target_id and target_id != article_id:\n",
    "# #                     ground_truth_links.append({\n",
    "# #                         'anchor': anchor,\n",
    "# #                         'target_id': target_id,\n",
    "# #                         'href_decoded': href_decoded\n",
    "# #                     })\n",
    "            \n",
    "# #             if len(ground_truth_links) >= min_links_per_sentence:\n",
    "# #                 test_data.append({\n",
    "# #                     'source_article_id': article_id,\n",
    "# #                     'source_article_title': article_title,\n",
    "# #                     'sentence': sentence,\n",
    "# #                     'ground_truth_links': ground_truth_links,\n",
    "# #                     'num_ground_truth': len(ground_truth_links)\n",
    "# #                 })\n",
    "        \n",
    "# #         articles_processed += 1\n",
    "    \n",
    "# #     del sampled_df\n",
    "# #     import gc\n",
    "# #     gc.collect()\n",
    "    \n",
    "# #     print(f\"\\n‚úÖ Created test dataset:\")\n",
    "# #     print(f\"   Sentences: {len(test_data)}\")\n",
    "# #     print(f\"   Articles processed: {articles_processed}\")\n",
    "# #     total_gt_links = sum(t['num_ground_truth'] for t in test_data)\n",
    "# #     print(f\"   Total ground truth links: {total_gt_links}\")\n",
    "    \n",
    "# #     return test_data\n",
    "\n",
    "# # # Create test dataset - pass LazyFrame, not collected DataFrame\n",
    "# # test_data = create_test_dataset_optimized(\n",
    "# #     df_lazy,  # LazyFrame, not df\n",
    "# #     url_to_id, \n",
    "# #     max_sentences=1000000,\n",
    "# #     min_links_per_sentence=20,\n",
    "# #     sample_articles=1000  # Only load 5K articles into memory\n",
    "# # )\n",
    "\n",
    "\n",
    "# def create_test_dataset_with_position_matching(\n",
    "#     df_lazy: pl.LazyFrame,\n",
    "#     url_to_id: Dict[str, int],\n",
    "#     max_sentences: int = 100,\n",
    "#     min_links_per_sentence: int = 1,\n",
    "#     sample_articles: int = 5000\n",
    "# ) -> List[Dict]:\n",
    "#     \"\"\"\n",
    "#     Create test dataset using position-based link matching from raw text.\n",
    "#     \"\"\"\n",
    "#     print(f\"üîç Creating test dataset with position-based matching...\")\n",
    "#     print(f\"   Sampling {sample_articles:,} articles...\")\n",
    "    \n",
    "#     # Sample articles and process text column\n",
    "#     sampled_df = (\n",
    "#         df_lazy\n",
    "#         .filter(pl.col('link_count') > 0)\n",
    "#         .filter(pl.col('text').is_not_null())\n",
    "#         .filter(pl.col('text').str.len_chars() > 100)\n",
    "#         .select(['id', 'title', 'text'])\n",
    "#         .head(sample_articles * 3)\n",
    "#         .collect(streaming=True)\n",
    "#         .sample(n=min(sample_articles, sample_articles * 3), shuffle=True, seed=42)\n",
    "#     )\n",
    "    \n",
    "#     print(f\"   Loaded {len(sampled_df):,} articles\")\n",
    "    \n",
    "#     test_data = []\n",
    "#     articles_processed = 0\n",
    "    \n",
    "#     for row in tqdm(sampled_df.iter_rows(named=True), total=len(sampled_df), desc=\"Processing\"):\n",
    "#         if len(test_data) >= max_sentences:\n",
    "#             break\n",
    "        \n",
    "#         article_id = row[\"id\"]\n",
    "#         article_title = row[\"title\"]\n",
    "#         raw_text = row.get(\"text\", \"\")\n",
    "        \n",
    "#         if not raw_text:\n",
    "#             continue\n",
    "        \n",
    "#         # Extract links and get clean text\n",
    "#         clean_text, extracted_links = extract_links_and_clean_text(raw_text)\n",
    "        \n",
    "#         if not clean_text or len(extracted_links) == 0:\n",
    "#             continue\n",
    "        \n",
    "#         # Extract sentences with position-based link matching\n",
    "#         sentences_with_links = extract_sentences_with_links_by_position(clean_text, extracted_links)\n",
    "        \n",
    "#         for sent_data in sentences_with_links:\n",
    "#             if len(test_data) >= max_sentences:\n",
    "#                 break\n",
    "            \n",
    "#             sentence = sent_data[\"sentence\"]\n",
    "#             links_in_sent = sent_data[\"links_in_sentence\"]\n",
    "            \n",
    "#             # Map links to article IDs\n",
    "#             ground_truth_links = []\n",
    "#             for link in links_in_sent:\n",
    "#                 href_decoded = link[\"href_decoded\"]\n",
    "#                 anchor = link[\"anchor\"]\n",
    "                \n",
    "#                 if not anchor or not anchor.strip():\n",
    "#                     continue\n",
    "                \n",
    "#                 target_id = url_to_id.get(href_decoded)\n",
    "#                 if not target_id:\n",
    "#                     for variation in [\n",
    "#                         href_decoded.replace(\"_\", \" \"),\n",
    "#                         href_decoded.replace(\"%20\", \" \"),\n",
    "#                         href_decoded.lower(),\n",
    "#                         href_decoded.lower().replace(\"_\", \" \")\n",
    "#                     ]:\n",
    "#                         target_id = url_to_id.get(variation)\n",
    "#                         if target_id:\n",
    "#                             break\n",
    "                \n",
    "#                 if target_id and target_id != article_id:\n",
    "#                     ground_truth_links.append({\n",
    "#                         'anchor': anchor,\n",
    "#                         'target_id': target_id,\n",
    "#                         'href_decoded': href_decoded\n",
    "#                     })\n",
    "            \n",
    "#             if len(ground_truth_links) >= min_links_per_sentence:\n",
    "#                 test_data.append({\n",
    "#                     'source_article_id': article_id,\n",
    "#                     'source_article_title': article_title,\n",
    "#                     'sentence': sentence,\n",
    "#                     'ground_truth_links': ground_truth_links,\n",
    "#                     'num_ground_truth': len(ground_truth_links)\n",
    "#                 })\n",
    "        \n",
    "#         articles_processed += 1\n",
    "    \n",
    "#     print(f\"\\n‚úÖ Created test dataset:\")\n",
    "#     print(f\"   Sentences: {len(test_data)}\")\n",
    "#     print(f\"   Articles processed: {articles_processed}\")\n",
    "#     total_gt_links = sum(t['num_ground_truth'] for t in test_data)\n",
    "#     print(f\"   Total ground truth links: {total_gt_links}\")\n",
    "    \n",
    "#     return test_data\n",
    "\n",
    "\n",
    "# # Create test dataset with position-based matching\n",
    "# test_data = create_test_dataset_with_position_matching(\n",
    "#     df_lazy,\n",
    "#     url_to_id, \n",
    "#     max_sentences=10000000,\n",
    "#     min_links_per_sentence=1,\n",
    "#     sample_articles=1000\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b6b88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Creating test dataset with paragraph-based chunking...\n",
      "   Sampling 1,000 articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3605256/1855920295.py:30: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n",
      "  .collect(streaming=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 1,000 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02<00:00, 431.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Created test dataset:\n",
      "   Chunks: 25815\n",
      "   Articles processed: 1000\n",
      "   Total ground truth links: 106321\n",
      "   Avg chunk length: 78.9 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_test_dataset_with_chunking(\n",
    "    df_lazy: pl.LazyFrame,\n",
    "    url_to_id: Dict[str, int],\n",
    "    max_chunks: int = 100000,\n",
    "    min_links_per_chunk: int = 1,\n",
    "    sample_articles: int = 5000,\n",
    "    chunking_strategy: str = \"paragraph\"  # \"paragraph\", \"sentence\", or \"recursive\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create test dataset using paragraph/sentence-based chunking instead of \n",
    "    individual sentence extraction.\n",
    "    \n",
    "    Args:\n",
    "        chunking_strategy: \n",
    "            - \"paragraph\": Split on paragraph breaks\n",
    "            - \"sentence\": Group sentences up to max_words\n",
    "            - \"recursive\": Try paragraphs first, then sentences\n",
    "    \"\"\"\n",
    "    print(f\"üîç Creating test dataset with {chunking_strategy}-based chunking...\")\n",
    "    print(f\"   Sampling {sample_articles:,} articles...\")\n",
    "    \n",
    "    # Sample articles\n",
    "    sampled_df = (\n",
    "        df_lazy\n",
    "        .filter(pl.col('link_count') > 0)\n",
    "        .filter(pl.col('text').is_not_null())\n",
    "        .filter(pl.col('text').str.len_chars() > 100)\n",
    "        .select(['id', 'title', 'text'])\n",
    "        .head(sample_articles * 3)\n",
    "        .collect(streaming=True)\n",
    "        .sample(n=min(sample_articles, sample_articles * 3), shuffle=True, seed=42)\n",
    "    )\n",
    "    \n",
    "    print(f\"   Loaded {len(sampled_df):,} articles\")\n",
    "    \n",
    "    test_data = []\n",
    "    articles_processed = 0\n",
    "    \n",
    "    for row in tqdm(sampled_df.iter_rows(named=True), total=len(sampled_df), desc=\"Processing\"):\n",
    "        if len(test_data) >= max_chunks:\n",
    "            break\n",
    "        \n",
    "        article_id = row[\"id\"]\n",
    "        article_title = row[\"title\"]\n",
    "        raw_text = row.get(\"text\", \"\")\n",
    "        \n",
    "        if not raw_text:\n",
    "            continue\n",
    "        \n",
    "        # Extract links and get clean text\n",
    "        clean_text, extracted_links = extract_links_and_clean_text(raw_text)\n",
    "        \n",
    "        if not clean_text or len(extracted_links) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Apply chunking strategy to clean text\n",
    "        if chunking_strategy == \"paragraph\":\n",
    "            chunks = paragraph_chunk(clean_text, min_length=50)\n",
    "        elif chunking_strategy == \"sentence\":\n",
    "            chunks = sentence_chunk(clean_text, max_words=150, min_words=20)\n",
    "        elif chunking_strategy == \"recursive\":\n",
    "            chunks = recursive_chunk(clean_text, chunk_size=300)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown chunking strategy: {chunking_strategy}\")\n",
    "        \n",
    "        # For each chunk, find links that fall within it\n",
    "        current_pos = 0\n",
    "        for chunk in chunks:\n",
    "            if len(test_data) >= max_chunks:\n",
    "                break\n",
    "            \n",
    "            # Find chunk position in clean text\n",
    "            chunk_start = clean_text.find(chunk, current_pos)\n",
    "            if chunk_start == -1:\n",
    "                continue\n",
    "            chunk_end = chunk_start + len(chunk)\n",
    "            current_pos = chunk_end\n",
    "            \n",
    "            # Find links within this chunk's boundaries\n",
    "            links_in_chunk = []\n",
    "            for link in extracted_links:\n",
    "                link_start = link.get('start_idx', -1)\n",
    "                if chunk_start <= link_start < chunk_end:\n",
    "                    links_in_chunk.append(link)\n",
    "            \n",
    "            if not links_in_chunk:\n",
    "                continue\n",
    "            \n",
    "            # Map links to article IDs\n",
    "            ground_truth_links = []\n",
    "            for link in links_in_chunk:\n",
    "                href_decoded = link[\"href_decoded\"]\n",
    "                anchor = link[\"anchor\"]\n",
    "                \n",
    "                if not anchor or not anchor.strip():\n",
    "                    continue\n",
    "                \n",
    "                target_id = url_to_id.get(href_decoded)\n",
    "                if not target_id:\n",
    "                    for variation in [\n",
    "                        href_decoded.replace(\"_\", \" \"),\n",
    "                        href_decoded.replace(\"%20\", \" \"),\n",
    "                        href_decoded.lower(),\n",
    "                        href_decoded.lower().replace(\"_\", \" \")\n",
    "                    ]:\n",
    "                        target_id = url_to_id.get(variation)\n",
    "                        if target_id:\n",
    "                            break\n",
    "                \n",
    "                if target_id and target_id != article_id:\n",
    "                    ground_truth_links.append({\n",
    "                        'anchor': anchor,\n",
    "                        'target_id': target_id,\n",
    "                        'href_decoded': href_decoded\n",
    "                    })\n",
    "            \n",
    "            if len(ground_truth_links) >= min_links_per_chunk:\n",
    "                test_data.append({\n",
    "                    'source_article_id': article_id,\n",
    "                    'source_article_title': article_title,\n",
    "                    'sentence': chunk,  # Keep key name for compatibility\n",
    "                    'chunk_type': chunking_strategy,\n",
    "                    'ground_truth_links': ground_truth_links,\n",
    "                    'num_ground_truth': len(ground_truth_links)\n",
    "                })\n",
    "        \n",
    "        articles_processed += 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created test dataset:\")\n",
    "    print(f\"   Chunks: {len(test_data)}\")\n",
    "    print(f\"   Articles processed: {articles_processed}\")\n",
    "    total_gt_links = sum(t['num_ground_truth'] for t in test_data)\n",
    "    print(f\"   Total ground truth links: {total_gt_links}\")\n",
    "    avg_chunk_len = np.mean([len(t['sentence'].split()) for t in test_data]) if test_data else 0\n",
    "    print(f\"   Avg chunk length: {avg_chunk_len:.1f} words\")\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "\n",
    "# Create test dataset with paragraph-based chunking\n",
    "test_data = create_test_dataset_with_chunking(\n",
    "    df_lazy,\n",
    "    url_to_id, \n",
    "    max_chunks=100000,\n",
    "    min_links_per_chunk=1,\n",
    "    sample_articles=1000,\n",
    "    chunking_strategy=\"paragraph\"  # Try \"sentence\" or \"recursive\" too\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd64c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Predicting links for 25815 sentences...\n",
      "   Encoding sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 807/807 [05:26<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Querying Qdrant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25815/25815 [07:30<00:00, 57.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated predictions for 25815 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# def predict_links_for_sentences(\n",
    "#     client,\n",
    "#     model,\n",
    "#     test_data: List[Dict],\n",
    "#     collection_name: str = \"wikipedia_fr_chunks\",\n",
    "#     top_k: int = 10,\n",
    "#     min_similarity: float = 0.5\n",
    "# ) -> List[Dict]:\n",
    "#     \"\"\"\n",
    "#     Predict links by finding similar articles in Qdrant.\n",
    "    \n",
    "#     Approach: Encode sentence ‚Üí Find similar articles ‚Üí Suggest those articles as links\n",
    "#     \"\"\"\n",
    "#     print(f\"üîÆ Predicting links for {len(test_data)} sentences...\")\n",
    "    \n",
    "#     # Extract all sentences\n",
    "#     sentences = [t['sentence'] for t in test_data]\n",
    "#     source_ids = [t['source_article_id'] for t in test_data]\n",
    "    \n",
    "#     # Encode all sentences\n",
    "#     print(\"   Encoding sentences...\")\n",
    "#     embeddings = model.encode(\n",
    "#         sentences,\n",
    "#         normalize_embeddings=True,\n",
    "#         show_progress_bar=True,\n",
    "#         convert_to_numpy=True,\n",
    "#         device=device\n",
    "#     )\n",
    "    \n",
    "#     predictions = []\n",
    "    \n",
    "#     print(\"   Querying Qdrant...\")\n",
    "#     for i, (sentence, embedding, source_id) in enumerate(tqdm(\n",
    "#         zip(sentences, embeddings, source_ids), \n",
    "#         total=len(sentences)\n",
    "#     )):\n",
    "#         # Search for similar articles\n",
    "#         search_results = client.query_points(\n",
    "#             collection_name=collection_name,\n",
    "#             query=embedding.tolist(),\n",
    "#             limit=top_k + 1,  # Extra to account for self-match\n",
    "#             score_threshold=min_similarity\n",
    "#         ).points\n",
    "        \n",
    "#         # Filter out source article and collect predictions\n",
    "#         predicted_articles = []\n",
    "#         for result in search_results:\n",
    "#             article_id = result.payload.get('id')\n",
    "            \n",
    "#             # Skip self-match\n",
    "#             if article_id == source_id:\n",
    "#                 continue\n",
    "            \n",
    "#             predicted_articles.append({\n",
    "#                 'article_id': article_id,\n",
    "#                 'article_title': result.payload.get('title', f'ID {article_id}'),\n",
    "#                 'similarity_score': result.score,\n",
    "#                 'text_preview': result.payload.get('text_withoutHref', '')[:150] + '...'\n",
    "#             })\n",
    "        \n",
    "#         # Limit to top_k after filtering\n",
    "#         predicted_articles = predicted_articles[:top_k]\n",
    "        \n",
    "#         predictions.append({\n",
    "#             **test_data[i],\n",
    "#             'predicted_articles': predicted_articles,\n",
    "#             'num_predictions': len(predicted_articles)\n",
    "#         })\n",
    "    \n",
    "#     print(f\"‚úÖ Generated predictions for {len(predictions)} sentences\")\n",
    "#     return predictions\n",
    "\n",
    "# # Generate predictions\n",
    "# predictions = predict_links_for_sentences(\n",
    "#     client,\n",
    "#     model,\n",
    "#     test_data,\n",
    "#     collection_name=\"wikipedia_fr_chunks\",\n",
    "#     top_k=20,\n",
    "#     min_similarity=0.5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce9bb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_links_for_sentences(\n",
    "    client,\n",
    "    model,\n",
    "    test_data: List[Dict],\n",
    "    collection_name: str = \"wikipedia_fr_chunks\",  # Updated default\n",
    "    top_k: int = 10,\n",
    "    min_similarity: float = 0.5\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Predict links by finding similar chunks in Qdrant, then aggregating by source article.\n",
    "    \"\"\"\n",
    "    print(f\"üîÆ Predicting links for {len(test_data)} sentences...\")\n",
    "    \n",
    "    # Extract all sentences\n",
    "    sentences = [t['sentence'] for t in test_data]\n",
    "    source_ids = [t['source_article_id'] for t in test_data]\n",
    "    \n",
    "    # Encode all sentences\n",
    "    print(\"   Encoding sentences...\")\n",
    "    embeddings = model.encode(\n",
    "        sentences,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    print(\"   Querying Qdrant...\")\n",
    "    for i, (sentence, embedding, source_id) in enumerate(tqdm(\n",
    "        zip(sentences, embeddings, source_ids), \n",
    "        total=len(sentences)\n",
    "    )):\n",
    "        # Search for similar chunks (not articles)\n",
    "        search_results = client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            query=embedding.tolist(),\n",
    "            limit=top_k * 3,  # Get more chunks to aggregate by article\n",
    "            score_threshold=min_similarity\n",
    "        ).points\n",
    "        \n",
    "        # === FIX: Aggregate chunks by source article ===\n",
    "        article_scores = {}\n",
    "        for result in search_results:\n",
    "            # Use correct field names for chunked collection\n",
    "            article_id = result.payload.get('source_article_id')  # Changed from 'id'\n",
    "            article_title = result.payload.get('source_article_title', f'ID {article_id}')  # Changed from 'title'\n",
    "            \n",
    "            # Skip self-match\n",
    "            if article_id == source_id:\n",
    "                continue\n",
    "            \n",
    "            # Aggregate scores by article (take max score across chunks)\n",
    "            if article_id not in article_scores:\n",
    "                article_scores[article_id] = {\n",
    "                    'article_id': article_id,\n",
    "                    'article_title': article_title,\n",
    "                    'max_similarity_score': result.score,\n",
    "                    'chunk_count': 1,\n",
    "                    'best_chunk_preview': result.payload.get('text_preview', '')[:150] + '...'\n",
    "                }\n",
    "            else:\n",
    "                # Update with better score if found\n",
    "                if result.score > article_scores[article_id]['max_similarity_score']:\n",
    "                    article_scores[article_id]['max_similarity_score'] = result.score\n",
    "                    article_scores[article_id]['best_chunk_preview'] = result.payload.get('text_preview', '')[:150] + '...'\n",
    "                article_scores[article_id]['chunk_count'] += 1\n",
    "        \n",
    "        # Sort by max similarity and take top_k articles\n",
    "        predicted_articles = sorted(\n",
    "            article_scores.values(), \n",
    "            key=lambda x: x['max_similarity_score'], \n",
    "            reverse=True\n",
    "        )[:top_k]\n",
    "        \n",
    "        # Format for compatibility with existing evaluation code\n",
    "        formatted_predictions = []\n",
    "        for pred in predicted_articles:\n",
    "            formatted_predictions.append({\n",
    "                'article_id': pred['article_id'],\n",
    "                'article_title': pred['article_title'],\n",
    "                'similarity_score': pred['max_similarity_score'],\n",
    "                'text_preview': pred['best_chunk_preview'],\n",
    "                'chunk_count': pred['chunk_count']  # New: how many chunks matched\n",
    "            })\n",
    "        \n",
    "        predictions.append({\n",
    "            **test_data[i],\n",
    "            'predicted_articles': formatted_predictions,\n",
    "            'num_predictions': len(formatted_predictions)\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Generated predictions for {len(predictions)} sentences\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e82f81d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Predicting links for 25815 sentences...\n",
      "   Encoding sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 807/807 [10:59<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Querying Qdrant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25815/25815 [16:49<00:00, 25.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated predictions for 25815 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions using chunked collection\n",
    "predictions = predict_links_for_sentences(\n",
    "    client,\n",
    "    model,\n",
    "    test_data,\n",
    "    collection_name=\"wikipedia_fr_chunks\",  # Use chunked collection\n",
    "    top_k=20,\n",
    "    min_similarity=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b13458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä EVALUATION RESULTS (Recall@K)\n",
      "============================================================\n",
      "\n",
      "üìà Recall@K Metrics:\n",
      "   Recall@ 1: 0.061 (6,372/104,555)\n",
      "   Recall@ 5: 0.163 (17,074/104,555)\n",
      "   Recall@10: 0.219 (22,939/104,555)\n",
      "   Recall@20: 0.283 (29,560/104,555)\n",
      "\n",
      "üìã Dataset Stats:\n",
      "   Test sentences: 25815\n",
      "   Total ground truth links: 104,555\n",
      "\n",
      "üìä Mean Reciprocal Rank (MRR): 0.109\n"
     ]
    }
   ],
   "source": [
    "def evaluate_predictions_recall_at_k(predictions: List[Dict], k_values: List[int] = [1, 5, 10, 20]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate predictions using Recall@K metrics.\n",
    "    \n",
    "    Recall@K = What fraction of ground truth links appear in the top-K predictions?\n",
    "    \"\"\"\n",
    "    # Initialize counters for each K\n",
    "    recall_at_k = {k: {'hits': 0, 'total': 0} for k in k_values}\n",
    "    \n",
    "    results_per_sentence = []\n",
    "    \n",
    "    for pred in predictions:\n",
    "        sentence_lower = pred['sentence'].lower()\n",
    "        \n",
    "        # Filter ground truth to only anchors actually in this sentence\n",
    "        filtered_gt = [link for link in pred['ground_truth_links']\n",
    "                       if link['anchor'].lower() in sentence_lower]\n",
    "        \n",
    "        gt_ids = set(link['target_id'] for link in filtered_gt)\n",
    "        \n",
    "        # Get predicted IDs in ranked order\n",
    "        pred_ids_ranked = [p['article_id'] for p in pred['predicted_articles']]\n",
    "        \n",
    "        # Calculate Recall@K for each K value\n",
    "        sentence_recall_at_k = {}\n",
    "        for k in k_values:\n",
    "            top_k_preds = set(pred_ids_ranked[:k])\n",
    "            hits = len(gt_ids & top_k_preds)\n",
    "            sentence_recall_at_k[k] = hits / len(gt_ids) if gt_ids else 0\n",
    "            \n",
    "            # Accumulate for overall metrics\n",
    "            recall_at_k[k]['hits'] += hits\n",
    "            recall_at_k[k]['total'] += len(gt_ids)\n",
    "        \n",
    "        results_per_sentence.append({\n",
    "            **pred,\n",
    "            'ground_truth_links': filtered_gt,\n",
    "            'num_ground_truth': len(filtered_gt),\n",
    "            'recall_at_k': sentence_recall_at_k\n",
    "        })\n",
    "    \n",
    "    # Calculate overall Recall@K\n",
    "    overall_recall_at_k = {}\n",
    "    for k in k_values:\n",
    "        if recall_at_k[k]['total'] > 0:\n",
    "            overall_recall_at_k[k] = recall_at_k[k]['hits'] / recall_at_k[k]['total']\n",
    "        else:\n",
    "            overall_recall_at_k[k] = 0\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä EVALUATION RESULTS (Recall@K)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nüìà Recall@K Metrics:\")\n",
    "    for k in k_values:\n",
    "        hits = recall_at_k[k]['hits']\n",
    "        total = recall_at_k[k]['total']\n",
    "        recall = overall_recall_at_k[k]\n",
    "        print(f\"   Recall@{k:2d}: {recall:.3f} ({hits:,}/{total:,})\")\n",
    "    \n",
    "    print(f\"\\nüìã Dataset Stats:\")\n",
    "    print(f\"   Test sentences: {len(predictions)}\")\n",
    "    total_gt = recall_at_k[k_values[0]]['total']\n",
    "    print(f\"   Total ground truth links: {total_gt:,}\")\n",
    "    \n",
    "    # Also compute MRR (Mean Reciprocal Rank)\n",
    "    mrr_sum = 0\n",
    "    mrr_count = 0\n",
    "    for pred in predictions:\n",
    "        gt_ids = set(link['target_id'] for link in pred.get('ground_truth_links', [])\n",
    "                     if link['anchor'].lower() in pred['sentence'].lower())\n",
    "        pred_ids_ranked = [p['article_id'] for p in pred['predicted_articles']]\n",
    "        \n",
    "        for gt_id in gt_ids:\n",
    "            mrr_count += 1\n",
    "            if gt_id in pred_ids_ranked:\n",
    "                rank = pred_ids_ranked.index(gt_id) + 1\n",
    "                mrr_sum += 1.0 / rank\n",
    "    \n",
    "    mrr = mrr_sum / mrr_count if mrr_count > 0 else 0\n",
    "    print(f\"\\nüìä Mean Reciprocal Rank (MRR): {mrr:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'recall_at_k': overall_recall_at_k,\n",
    "        'mrr': mrr,\n",
    "        'results_per_sentence': results_per_sentence\n",
    "    }\n",
    "\n",
    "# Evaluate with Recall@K\n",
    "eval_results = evaluate_predictions_recall_at_k(predictions, k_values=[1, 5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ecaef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéØ BEST PREDICTIONS (Highest Recall@5)\n",
      "================================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 1: Article 'Les Verts (France)'\n",
      "Sentence: 'Lors des √©lections r√©gionales de 1992, la r√©gion Nord-Pas-de-Calais voit √©lue √† la Pr√©sidence de son conseil r√©gional une femme ‚Äì la premi√®re en France ‚Äì adh√©rente des Verts - Marie-Christine Blandin, apr√®s un accord avec le P.S qui avait une majorit√© relative en nombre de conseillers r√©gionaux.'\n",
      "Recall@1: 0.50 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (2):\n",
      "   [‚úì] '√©lections r√©gionales de 1992' ‚Üí √âlections r√©gionales fran√ßaises de 1992 (rank: 2)\n",
      "   [‚úì] 'Marie-Christine Blandin' ‚Üí Marie-Christine Blandin (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 Marie-Christine Blandin (sim: 0.893)\n",
      "   [‚úì] #2 √âlections r√©gionales fran√ßaises de 1992 (sim: 0.872)\n",
      "   [‚úó] #3 Alliances √©lectorales du Front national (sim: 0.872)\n",
      "   [‚úó] #4 Liste des membres du conseil r√©gional du Nord-Pas-de-Calais (1998-2004) (sim: 0.871)\n",
      "   [‚úó] #5 Conseil r√©gional du Nord-Pas-de-Calais (sim: 0.864)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 2: Article 'Les Verts (France)'\n",
      "Sentence: 'La le√ßon est tir√©e lors de leur Assembl√©e f√©d√©rale de Nantes qui a lieu fin 2002 et o√π les Verts font leur autocritique. Ils estiment qu'ils auraient d√ª sortir du gouvernement d√®s 2000 quand le contrat qu'ils avaient pass√© avec le Parti socialiste avait cess√© d'√™tre respect√©, et r√©affirmer leur radicalit√© politique. Le courant incarn√© par Dominique Voynet, qui avait en juin pr√©c√©dent √©voqu√© la possibilit√© d'un grand parti unique de la gauche dans lequel se fondrait les Verts, est mis en minorit√©. En janvier 2003, Gilles Lemaire devient secr√©taire national, succ√©dant √† Dominique Voynet.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'Gilles Lemaire' ‚Üí Gilles Lemaire (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 Gilles Lemaire (sim: 0.877)\n",
      "   [‚úó] #2 Jean-Luc Bennahmias (sim: 0.863)\n",
      "   [‚úó] #3 Alliance 90/Les Verts (sim: 0.861)\n",
      "   [‚úó] #4 √âlections l√©gislatives n√©erlandaises de 2023 (sim: 0.860)\n",
      "   [‚úó] #5 Les Jeunes √âcologistes (sim: 0.859)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 3: Article 'Les Verts (France)'\n",
      "Sentence: 'Apr√®s les bons r√©sultats √©lectoraux obtenus par le rassemblement Europe √âcologie, Daniel Cohn-Bendit appelle, au lendemain des √©lections r√©gionales de 2010, √† la dissolution des Verts au sein d'¬´ une nouvelle formation politique √† inventer ¬ª. De m√™me, son fr√®re Gabriel souhaite qu'Europe √âcologie ¬´ devienne une force qui ne d√©pende plus de l'appareil des Verts ¬ª, mais cette hypoth√®se re√ßoit un accueil mitig√© chez les Verts, √† l'image du num√©ro deux du parti, Jean-Vincent Plac√©.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'Daniel Cohn-Bendit' ‚Üí Daniel Cohn-Bendit (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 Daniel Cohn-Bendit (sim: 0.867)\n",
      "   [‚úó] #2 Les √âcologistes (sim: 0.859)\n",
      "   [‚úó] #3 C√©cile Duflot (sim: 0.853)\n",
      "   [‚úó] #4 Jean-Luc M√©lenchon (sim: 0.852)\n",
      "   [‚úó] #5 Mouvement √©cologiste ind√©pendant (sim: 0.849)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 4: Article 'Les Verts (France)'\n",
      "Sentence: 'D'un point de vue g√©n√©ral, les Verts militent pour anticiper la transition (qu'ils pensent de toute fa√ßon in√©luctable) de notre mod√®le de production √©nerg√©tique vers le tout renouvelable. Consid√©rant que la production de l'√©nergie est presque toujours destructrice pour l'environnement, les Verts militent pour une soci√©t√© √©nerg√©tiquement plus sobre.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'transition' ‚Üí Transition √©nerg√©tique (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 Transition √©nerg√©tique (sim: 0.846)\n",
      "   [‚úó] #2 √ânergie durable (sim: 0.843)\n",
      "   [‚úó] #3 √ânergie (√©conomie) (sim: 0.840)\n",
      "   [‚úó] #4 Transition juste (sim: 0.837)\n",
      "   [‚úó] #5 Soci√©t√© √† 2000 watts (sim: 0.836)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 5: Article 'Boussole (constellation)'\n",
      "Sentence: 'Lacaille a attribu√© des d√©signations de Bayer √† dix √©toiles, catalogu√©es de Œ± (Alpha) √† Œª (Lambda) Pyxidis, tout en ignorant les lettres grecques iota et kappa.\n",
      "Œ± Pyxidis.'\n",
      "Recall@1: 0.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'd√©signations de Bayer' ‚Üí D√©signation de Bayer (rank: 5)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Psi Aurigae (sim: 0.870)\n",
      "   [‚úó] #2 Alpha Capricorni (sim: 0.855)\n",
      "   [‚úó] #3 D√©signation stellaire (sim: 0.853)\n",
      "   [‚úó] #4 Kappa Ceti (sim: 0.852)\n",
      "   [‚úì] #5 D√©signation de Bayer (sim: 0.850)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 6: Article '√âcole pratique des hautes √©tudes'\n",
      "Sentence: 'Enfin, les auditeurs libres sont toutes les personnes qui souhaitent suivre les s√©minaires d'un enseignant de l'EPHE mais qui ne sont pas engag√©es dans un cursus universitaire ou qui ne se sentent pas la capacit√© d'effectuer les recherches attendues pour valider le s√©minaire.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'auditeurs libres' ‚Üí Auditeur libre (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 Auditeur libre (sim: 0.837)\n",
      "   [‚úó] #2 Universit√© ouverte (type d'universit√©) (sim: 0.829)\n",
      "   [‚úó] #3 S√©minaire protestant (sim: 0.823)\n",
      "   [‚úó] #4 Acc√®s universel √† l'√©ducation (sim: 0.817)\n",
      "   [‚úó] #5 Pratiques √©ducatives ouvertes (sim: 0.816)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 7: Article 'G√©ographie'\n",
      "Sentence: 'Aujourd'hui, une division de la g√©ographie en deux branches principales s'est impos√©e √† l'usage, la g√©ographie humaine et la g√©ographie physique.'\n",
      "Recall@1: 0.50 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (2):\n",
      "   [‚úì] 'g√©ographie humaine' ‚Üí G√©ographie humaine (rank: 2)\n",
      "   [‚úì] 'g√©ographie physique' ‚Üí G√©ographie physique (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 G√©ographie physique (sim: 0.844)\n",
      "   [‚úì] #2 G√©ographie humaine (sim: 0.836)\n",
      "   [‚úó] #3 Sciences physiques (sim: 0.834)\n",
      "   [‚úó] #4 G√©ographie √©conomique (sim: 0.830)\n",
      "   [‚úó] #5 Discipline scientifique (sim: 0.829)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 8: Article 'G√©ographie'\n",
      "Sentence: 'Entre le XIX et le XX¬†si√®cle, plusieurs courants se d√©veloppent tentant de d√©montrer l'interaction entre l'homme et la nature, avec plus ou moins de succ√®s et de rigueur d'approche :'\n",
      "Recall@1: 0.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'nature' ‚Üí Nature (rank: 5)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 D√©terminisme historique (sim: 0.839)\n",
      "   [‚úó] #2 Environnement (sim: 0.833)\n",
      "   [‚úó] #3 Protection de l'environnement (sim: 0.832)\n",
      "   [‚úó] #4 Naturalit√© (environnement) (sim: 0.829)\n",
      "   [‚úì] #5 Nature (sim: 0.827)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 9: Article 'G√©ographie'\n",
      "Sentence: 'Seul s√©l√©nographie semble utilis√©. Le terme \"ar√©ographie\" pour Mars, par exemple, a bien √©t√© propos√©, mais n'a rencontr√© que tr√®s peu de succ√®s.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 's√©l√©nographie' ‚Üí S√©l√©nographie (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 S√©l√©nographie (sim: 0.835)\n",
      "   [‚úó] #2 Sortie extrav√©hiculaire (sim: 0.829)\n",
      "   [‚úó] #3 Programme Viking (sim: 0.828)\n",
      "   [‚úó] #4 Liste de l√©gendes urbaines (sim: 0.826)\n",
      "   [‚úó] #5 Mars Pathfinder (sim: 0.822)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 10: Article 'G√©ographie'\n",
      "Sentence: 'Colette Cauvin, est une g√©ographe des transformations cartographiques en France.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'Colette Cauvin' ‚Üí Colette Cauvin (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 Colette Cauvin (sim: 0.879)\n",
      "   [‚úó] #2 Camille Toubkis (sim: 0.840)\n",
      "   [‚úó] #3 Catherine Sauvat (sim: 0.835)\n",
      "   [‚úó] #4 Catherine Castel (r√©alisatrice) (sim: 0.832)\n",
      "   [‚úó] #5 Pierre Cottereau (directeur de la photographie) (sim: 0.829)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 11: Article 'G√©ographie'\n",
      "Sentence: 'L'urbaniste Pierre Merlin pr√©cise que ¬´ \"les g√©ographes ont souvent eu tendance √† consid√©rer, en France notamment, l'am√©nagement (et en particulier l'am√©nagement urbain, voire l'urbanisme) comme un prolongement naturel de leur discipline. Il s'agit en fait de champs d'action pluridisciplinaires par nature qui ne sauraient √™tre l'apanage d'une seule discipline quelle qu'elle soit. Mais la g√©ographie, discipline de l'espace √† diff√©rentes √©chelles, est concern√©e au premier chef\" ¬ª.'\n",
      "Recall@1: 0.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (2):\n",
      "   [‚úì] 'am√©nagement' ‚Üí Am√©nagement du territoire (rank: 2)\n",
      "   [‚úì] 'urbanisme' ‚Üí Urbanisme (rank: 3)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Dictionnaire de l'urbanisme et de l'am√©nagement (sim: 0.872)\n",
      "   [‚úì] #2 Am√©nagement du territoire (sim: 0.862)\n",
      "   [‚úì] #3 Urbanisme (sim: 0.862)\n",
      "   [‚úó] #4 Architecte urbaniste (sim: 0.859)\n",
      "   [‚úó] #5 Planification urbaine (sim: 0.859)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 12: Article 'G√©ographie'\n",
      "Sentence: 'L'enseignement de la g√©ographie a fait l'objet de plusieurs √©tudes, notamment de la part de Jacques Scheibling ou d'Isabelle Lefort, montrant, depuis son apparition en tant que v√©ritable discipline scolaire en France dans les ann√©es 1870 jusqu'√† nos jours, son √©volution en parall√®le avec celle de la g√©ographie savante.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'Jacques Scheibling' ‚Üí Jacques Scheibling (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 Jacques Scheibling (sim: 0.862)\n",
      "   [‚úó] #2 G√©opolitique (sim: 0.844)\n",
      "   [‚úó] #3 G√©ographie spectacle (sim: 0.842)\n",
      "   [‚úó] #4 Classes pr√©paratoires √©conomiques et commerciales (sim: 0.840)\n",
      "   [‚úó] #5 G√©oconfluences (sim: 0.838)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 13: Article 'Totem'\n",
      "Sentence: 'Nous consid√©rons alors √† travers ces d√©finitions combien Sigmund Freud se r√©approprie un terme de nature sociologique. Les lectures de la part du psychanalyste des textes contemporains aux deux ann√©es 1910-1912, de l‚Äôanthropologie anglo-saxonne, sont multiples. Il lit \"La religion des S√©mites\" de W.R. Smith (1889), les quatre volumes du \"Rameau d‚Äôor\" de James George Frazer paraissant jusqu‚Äôen 1915, mais aussi \"Les Formes √©l√©mentaires de la vie religieuse\" d‚ÄôE. Durkheim. Freud y puise des concepts et trouve une inspiration d√©terminante √† l‚Äô√©criture du texte qui insistera aussi sur l‚Äôassociation au tabou, comme interdiction rituelle.'\n",
      "Recall@1: 0.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (2):\n",
      "   [‚úì] 'Sigmund Freud' ‚Üí Sigmund Freud (rank: 5)\n",
      "   [‚úì] 'James George Frazer' ‚Üí James George Frazer (rank: 2)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Totem et Tabou (sim: 0.889)\n",
      "   [‚úì] #2 James George Frazer (sim: 0.880)\n",
      "   [‚úó] #3 Mythologie (sim: 0.878)\n",
      "   [‚úó] #4 Religion (histoire des id√©es) (sim: 0.878)\n",
      "   [‚úì] #5 Sigmund Freud (sim: 0.874)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 14: Article 'Licence publique g√©n√©rale GNU'\n",
      "Sentence: 'Cette condition est connue sous le nom de \"copyleft\", et il obtient son origine l√©gale du fait que le programme est ¬´ copyright√© ¬ª. Puisqu'il est copyright√©, l'utilisateur n'a aucun droit de le modifier ou de le redistribuer, sauf sous les termes du \"copyleft\". On est oblig√© d'adh√©rer √† la GPL si on souhaite exercer des droits normalement limit√©s (voire interdits) par le \"copyright\", comme la redistribution. Ainsi, si on distribue des copies du \"travail\" sans respecter les termes de la GPL (en gardant le code source secret par exemple), on peut √™tre poursuivi par l'auteur original en vertu du \"copyright\".'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'copyleft' ‚Üí Copyleft (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 Copyleft (sim: 0.900)\n",
      "   [‚úó] #2 Logiciel libre (sim: 0.897)\n",
      "   [‚úó] #3 Partage dans les m√™mes conditions (sim: 0.886)\n",
      "   [‚úó] #4 Licence de logiciel (sim: 0.882)\n",
      "   [‚úó] #5 Licence publique g√©n√©rale limit√©e GNU (sim: 0.877)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 15: Article 'Licence publique g√©n√©rale GNU'\n",
      "Sentence: 'Harald Welte, fondateur du projet gpl-violations.org, poursuit les soci√©t√©s et les programmeurs coupables, selon lui, d'une violation de la GPL. Il a d√©j√† obtenu, depuis 2004, une trentaine de conciliations, apr√®s avoir engag√© des poursuites dans certains cas.'\n",
      "Recall@1: 0.50 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (2):\n",
      "   [‚úì] 'Harald Welte' ‚Üí Harald Welte (rank: 2)\n",
      "   [‚úì] 'gpl-violations.org' ‚Üí Gpl-violations.org (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 Gpl-violations.org (sim: 0.908)\n",
      "   [‚úì] #2 Harald Welte (sim: 0.870)\n",
      "   [‚úó] #3 Netfilter (sim: 0.844)\n",
      "   [‚úó] #4 Government Accountability Project (sim: 0.836)\n",
      "   [‚úó] #5 Software Freedom Law Center (sim: 0.835)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 16: Article 'Licence publique g√©n√©rale GNU'\n",
      "Sentence: 'Les licences CeCILL ont √©t√© mises en place afin de permettre √† des √©tablissements publics de publier leurs travaux logiciels sous licence libre r√©dig√©e selon le droit fran√ßais. La licence CeCILL est compatible, depuis sa version 2, avec la licence publique g√©n√©rale GNU.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'CeCILL' ‚Üí Licence CeCILL (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 Licence CeCILL (sim: 0.907)\n",
      "   [‚úó] #2 Open Database License (sim: 0.872)\n",
      "   [‚úó] #3 Licence de documentation libre GNU (sim: 0.870)\n",
      "   [‚úó] #4 Educational Community License (sim: 0.861)\n",
      "   [‚úó] #5 Socle interminist√©riel des logiciels libres (sim: 0.856)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 17: Article 'Licence publique g√©n√©rale GNU'\n",
      "Sentence: 'Les identifiants SPDX sont maintenant GPL-1.0-only, GPL-1.0-or-later, GPL-2.0-only, GPL-2.0-or-later, GPL-3.0-only et GPL-3.0-or-later. Les anciens identifiants SPDX GPL-1.0, GPL-1.0+, GPL-2.0, GPL-2.0+, GPL-3.0 et GPL-3.0+ sont d√©pr√©ci√©s.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'SPDX' ‚Üí SPDX (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 SPDX (sim: 0.899)\n",
      "   [‚úó] #2 Licence publique de l'Union europ√©enne (sim: 0.851)\n",
      "   [‚úó] #3 Copyleft (sim: 0.843)\n",
      "   [‚úó] #4 Akai MPC (sim: 0.843)\n",
      "   [‚úó] #5 Oracle Database (sim: 0.840)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 18: Article '4 juin'\n",
      "Sentence: 'Les noms de plusieurs voies, places, sites ou √©difices de pays ou r√©gions francophones contiennent cette date sous des graphies diverses : voir Quatre-Juin.'\n",
      "Recall@1: 0.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'Quatre-Juin' ‚Üí Quatre-Juin (rank: 5)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Septembre (odonymie) (sim: 0.912)\n",
      "   [‚úó] #2 D√©cembre (odonymie) (sim: 0.911)\n",
      "   [‚úó] #3 Juillet (odonymie) (sim: 0.911)\n",
      "   [‚úó] #4 Octobre (odonymie) (sim: 0.910)\n",
      "   [‚úì] #5 Quatre-Juin (sim: 0.899)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 19: Article 'Ordinateur personnel'\n",
      "Sentence: 'En 1981, IBM produit l'IBM PC, qui √©tait vendu tr√®s cher par rapport aux autres micro-ordinateurs et n√©cessitait son propre √©cran, alors que les autres micro-ordinateurs permettaient d'utiliser l'√©cran de la t√©l√©vision familiale. La carte graphique CGA, sortie en 1981, avait aussi un prix assez √©lev√© et √©tait limit√©e √† quatre couleurs. En 1984, IBM sort une carte Enhanced Graphics Adapter supportant 16 couleurs.'\n",
      "Recall@1: 0.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (3):\n",
      "   [‚úì] 'IBM PC' ‚Üí IBM PC (rank: 5)\n",
      "   [‚úì] 'CGA' ‚Üí Color Graphics Adapter (rank: 2)\n",
      "   [‚úì] 'Enhanced Graphics Adapter' ‚Üí Enhanced Graphics Adapter (rank: 3)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Hercules Graphics Card (sim: 0.889)\n",
      "   [‚úì] #2 Color Graphics Adapter (sim: 0.885)\n",
      "   [‚úì] #3 Enhanced Graphics Adapter (sim: 0.877)\n",
      "   [‚úó] #4 1958 en informatique (sim: 0.875)\n",
      "   [‚úì] #5 IBM PC (sim: 0.872)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 20: Article 'Ordinateur personnel'\n",
      "Sentence: 'Les descendants de l'ordinateur personnel d'IBM, les compatibles PC, fabriqu√©s par des entreprises asiatiques, ont exerc√© une pression sur le march√© qui a fait diminuer le prix de vente des ordinateurs personnels. √Ä partir des ann√©es 1990, .'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   [‚úì] 'compatibles PC' ‚Üí Compatible PC (rank: 1)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úì] #1 Compatible PC (sim: 0.856)\n",
      "   [‚úó] #2 Histoire des ordinateurs personnels (sim: 0.850)\n",
      "   [‚úó] #3 Ordinateur domestique (sim: 0.849)\n",
      "   [‚úó] #4 Macintosh (sim: 0.844)\n",
      "   [‚úó] #5 Liste de produits IBM (sim: 0.841)\n",
      "\n",
      "================================================================================\n",
      "‚ùå WORST PREDICTIONS (Lowest Recall@5, with predictions)\n",
      "================================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 1: Article 'Charlemagne'\n",
      "Sentence: 'En 1861, des scientifiques ont ouvert le tombeau de Charlemagne pour analyser son squelette ; sa taille fut estim√©e √† . En 1988, l'analyse de la suture osseuse de son cr√¢ne permet d'estimer un √¢ge √† sa mort de 66 ans, soit 37 ans de plus que l'esp√©rance de vie moyenne de ses contemporains. En 2010, une radiographie et une scannographie de son tibia a estim√© sa taille √† . Charlemagne faisait donc partie des rares personnes de grande taille de son √©poque, √©tant donn√© que la hauteur moyenne des hommes de son temps √©tait de . La largeur de l'os laisse penser qu'il √©tait gracile et n'avait pas une construction corporelle robuste.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (2):\n",
      "   'radiographie' ‚Üí Radiographie (not found)\n",
      "   'tibia' ‚Üí Tibia (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Tombe double d'Oberkassel (sim: 0.859)\n",
      "   [‚úó] #2 D√©couverte du corps de Richard III (sim: 0.856)\n",
      "   [‚úó] #3 Site arch√©ologique de Windover (sim: 0.855)\n",
      "   [‚úó] #4 Grotte du Bichon (sim: 0.853)\n",
      "   [‚úó] #5 G√©ant de Castelnau (sim: 0.853)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 2: Article 'Charlemagne'\n",
      "Sentence: 'L'historien Jean Favier pr√©cise que l'historiographie de Charlemagne ne commence qu'au XVII¬†si√®cle avec en 1677 la premi√®re publication des capitulaires par le biblioth√©caire royal √âtienne Baluze et √† la m√™me √©poque son √©vocation dans le \"Discours sur l'histoire universelle\" de Bossuet, lequel connaissait le texte d'Eginhard qui n'√©tait pas encore imprim√©.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (3):\n",
      "   'Jean Favier' ‚Üí Jean Favier (rank: 15)\n",
      "   '√âtienne Baluze' ‚Üí √âtienne Baluze (not found)\n",
      "   'Bossuet' ‚Üí Jacques-B√©nigne Bossuet (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Karoli M. capitulare Primum (sim: 0.844)\n",
      "   [‚úó] #2 Julius Pollux (historien byzantin) (sim: 0.842)\n",
      "   [‚úó] #3 Archives parlementaires de 1787 √† 1860 (sim: 0.841)\n",
      "   [‚úó] #4 Histoire litt√©raire de la France (sim: 0.840)\n",
      "   [‚úó] #5 Bataille de Roncevaux (778) (sim: 0.839)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 3: Article 'Charlemagne'\n",
      "Sentence: 'On peut remarquer que ces textes avaient d√©j√† √©t√© imprim√©s et traduits plusieurs fois avant 1677, et que l'int√©r√™t pour l'histoire de sa vie est plus ancien : la \"Vita Karoli Magni\" d'Eginhart est imprim√©e √† Cologne en 1521, √† Utrecht en 1711 ; la fausse chronique romanc√©e \"De Vite Caroli et Rolandi\", attribu√©e au moine Jean Turpin et pleine d'√©pisodes invent√©s, est publi√©e √† Paris, d'abord sans date, puis en 1527, puis √† Lyon en 1583. Le recueil de ses capitulaires est publi√© √† Ingolstadt en 1548, avec des notes d'Amerbach, et la m√™me ann√©e √† Paris, mais avec des retranchements, par Jean du Tillet, √©v√™que de Meaux, √©dition termin√©e en 1588 par Pierre Pithou, avec des notes de Fran√ßois Pithou. Des √©ditions compl√®tes paraissent en 1603 et 1620, cette derni√®re avec la publication \"in-folio\" de la carte de l'empire de Charlemagne par P. Bertius. Sa f√™te avait √©t√© fix√©e le par le roi , en 1661 l'Universit√© de Paris l'avait choisi comme saint patron, et la m√™me ann√©e, consacre √† Charlemagne un paragraphe des \"M√©moires pour l'instruction du Dauphin\", montrant qu'il le connaissait assez bien sous certains aspects.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (6):\n",
      "   'Vita Karoli Magni' ‚Üí Vita Karoli Magni (not found)\n",
      "   'Cologne' ‚Üí Cologne (not found)\n",
      "   'Utrecht' ‚Üí Utrecht (not found)\n",
      "   'Ingolstadt' ‚Üí Ingolstadt (not found)\n",
      "   '√©v√™que de Meaux' ‚Üí Liste des √©v√™ques de Meaux (not found)\n",
      "   'Pierre Pithou' ‚Üí Pierre Pithou (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Primat de Saint-Denis (sim: 0.899)\n",
      "   [‚úó] #2 Chronique du Pseudo-Turpin (sim: 0.898)\n",
      "   [‚úó] #3 Codex epistolaris Carolinus (sim: 0.892)\n",
      "   [‚úó] #4 Codex Calixtinus (sim: 0.889)\n",
      "   [‚úó] #5 Orens d'Auch (sim: 0.886)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 4: Article 'Charlemagne'\n",
      "Sentence: 'Le travail de publication de documents est poursuivi au XVIII¬†si√®cle par des √©rudits souvent issus du clerg√© r√©gulier. Les plus notables sont le p√®re Anselme (ordre des Augustins) et dom Martin Bouquet (b√©n√©dictin de Saint-Maur), le premier √©diteur d'Eginhard. Son \"Recueil des historiens des Gaules et de la France\" consacre un volume √† P√©pin le Bref et √† Charlemagne.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (2):\n",
      "   'p√®re Anselme' ‚Üí Pierre de Guibours (rank: 12)\n",
      "   'Martin Bouquet' ‚Üí Martin Bouquet (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Recueil des historiens des Gaules et de la France (sim: 0.870)\n",
      "   [‚úó] #2 Histoire litt√©raire de la France (sim: 0.849)\n",
      "   [‚úó] #3 Enluminure carolingienne (sim: 0.842)\n",
      "   [‚úó] #4 Maur Dantine (sim: 0.839)\n",
      "   [‚úó] #5 Jean Mabillon (sim: 0.838)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 5: Article 'Charlemagne'\n",
      "Sentence: 'L'√©dition syst√©matique des documents historiques recommence au XIX¬†si√®cle ; en ce qui concerne Charlemagne, ce sont les historiens allemands (Percy Ernst Schramm, Karl Ferdinand Werner) qui assurent une grande part du travail dans les \"Monumenta Germani√¶ Historica\". En France, √† partir de 1822, est publi√© le \"Recueil g√©n√©ral des anciennes lois fran√ßaises depuis l'an 420\" (Isembert) et √† partir de 1835, la \"Collection de documents in√©dits sur l'histoire de France\". √Ä partir de 1840, Benjamin Gu√©rard publie un certain nombre de documents d'abbayes. Le premier √† chercher √† d√©m√™ler les mythes de la r√©alit√© du personnage est le m√©di√©viste Gaston Paris dans son \"Histoire po√©tique de Charlemagne\" en 1865.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (3):\n",
      "   'Karl Ferdinand Werner' ‚Üí Karl Ferdinand Werner (not found)\n",
      "   'Monumenta Germani√¶ Historica' ‚Üí Monumenta Germaniae Historica (rank: 13)\n",
      "   'Gaston Paris' ‚Üí Gaston Paris (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Annales regni Francorum (sim: 0.875)\n",
      "   [‚úó] #2 Recueil des historiens des Gaules et de la France (sim: 0.873)\n",
      "   [‚úó] #3 Jules Lair (sim: 0.866)\n",
      "   [‚úó] #4 Mythologie nordique (sim: 0.866)\n",
      "   [‚úó] #5 Ma√Ætre Eckhart (sim: 0.865)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 6: Article 'Charlemagne'\n",
      "Sentence: 'En 1165, dans le cadre des conflits entre la papaut√© et l'empire, Fr√©d√©ric Barberousse et l'antipape proc√®dent √† la canonisation de Charlemagne. La c√©r√©monie religieuse d'√©l√©vation des ossements de Charlemagne par Renaud de Dassel, archev√™que de Cologne et , √©v√™que de Li√®ge a lieu le , en pr√©sence d'une nombreuse assistance. Ils sont plac√©s dans une ch√¢sse provisoire, remplac√©e par une autre plus pr√©cieuse aux alentours de 1200.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.33\n",
      "\n",
      "‚úÖ Ground Truth (3):\n",
      "   'Fr√©d√©ric Barberousse' ‚Üí Fr√©d√©ric Barberousse (rank: 6)\n",
      "   'antipape' ‚Üí Antipape (not found)\n",
      "   'ch√¢sse' ‚Üí Ch√¢sse (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Ch√¢sse de Charlemagne (sim: 0.864)\n",
      "   [‚úó] #2 Histoire de l'√©v√™ch√© de Mayence (sim: 0.863)\n",
      "   [‚úó] #3 Cath√©drale d'Aix-la-Chapelle (sim: 0.861)\n",
      "   [‚úó] #4 Alexandre II (prince-√©v√™que de Li√®ge) (sim: 0.858)\n",
      "   [‚úó] #5 Sainte-Chapelle (sim: 0.853)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 7: Article 'Charlemagne'\n",
      "Sentence: 'L‚Äô√âglise catholique pr√©f√®re ne pas le compter au nombre des saints, en raison de la conversion des Saxons par la violence ; mais son titre de bienheureux est tol√©r√© (et donc son culte) par le pape .'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   'bienheureux' ‚Üí Bienheureux (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Jean le Soldat (sim: 0.854)\n",
      "   [‚úó] #2 Bennon de Meissen (sim: 0.845)\n",
      "   [‚úó] #3 Liste des saints du Xe si√®cle (sim: 0.844)\n",
      "   [‚úó] #4 Liste de saints catholiques (sim: 0.841)\n",
      "   [‚úó] #5 Saint Macaire (sim: 0.838)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 8: Article 'Charlemagne'\n",
      "Sentence: 'Charlemagne est entr√© dans l‚Äôordo (calendrier liturgique) de plusieurs dioc√®ses situ√©s dans la r√©gion d'Aix-la-Chapelle, o√π ses ossements sont encore expos√©s √† la v√©n√©ration des fid√®les. Sa f√™te est fix√©e au , anniversaire de sa mort.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   'ordo' ‚Üí Missel romain (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Wendelin (saint catholique) (sim: 0.858)\n",
      "   [‚úó] #2 P√®lerinage d'Aix-la-Chapelle (sim: 0.857)\n",
      "   [‚úó] #3 Cath√©drale d'Aix-la-Chapelle (sim: 0.851)\n",
      "   [‚úó] #4 Chapelle royale (Ancien R√©gime) (sim: 0.849)\n",
      "   [‚úó] #5 Chapelle palatine d'Aix-la-Chapelle (sim: 0.849)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 9: Article 'Charlemagne'\n",
      "Sentence: 'Au XIII¬†si√®cle, √©poque o√π les rois de France s'affirment comme √©gaux √† l'empereur (Philippe Auguste), l'abbaye de Saint-Denis, lieu de l'inhumation de P√©pin le Bref, joue un r√¥le important dans l'√©laboration d'une figure de Charlemagne ¬´ fran√ßais ¬ª, alors que les empereurs d'Allemagne soutiennent en g√©n√©ral un Charlemagne ¬´ allemand ¬ª (d'o√π l'affirmation de la naissance √† Ingelheim par Guillaume de Viterbe au XII¬†si√®cle).'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   'Philippe Auguste' ‚Üí Philippe II Auguste (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Basilique Saint-Denis (sim: 0.874)\n",
      "   [‚úó] #2 Saint-Denis (Seine-Saint-Denis) (sim: 0.860)\n",
      "   [‚úó] #3 Histoire de la papaut√© (sim: 0.858)\n",
      "   [‚úó] #4 Chroniques de Saint-Denis (sim: 0.858)\n",
      "   [‚úó] #5 P√©pin le Bossu (sim: 0.853)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 10: Article 'Charlemagne'\n",
      "Sentence: 'Charlemagne est particuli√®rement mis en valeur par la dynastie des Valois, en particulier par le roi , qui proc√®de √† des √©changes de reliques avec son oncle, l'empereur . Durant son sacre, le souverain fran√ßais utilise un sceptre termin√© par une statuette de Charlemagne, appel√© \"sceptre de\" ou \"sceptre de Charlemagne\". √Ä la fin du XV¬†si√®cle, dans la perspective des guerres d'Italie, un ¬´ Charlemagne ¬ª fait partie du cort√®ge d'accueil d'Anne de Bretagne lors de son mariage avec le roi ; leur fils a√Æn√© est nomm√© Charles-Orland (1492-1495), Orland √©tant la francisation d'Orlando, le nom italien de Roland (cf. \"Orlando furioso\").'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (4):\n",
      "   'dynastie des Valois' ‚Üí Maison de Valois (not found)\n",
      "   'guerres d'Italie' ‚Üí Guerres d'Italie (not found)\n",
      "   'Anne de Bretagne' ‚Üí Anne de Bretagne (not found)\n",
      "   'Orlando furioso' ‚Üí Orlando furioso (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Cour de France (sim: 0.876)\n",
      "   [‚úó] #2 Buste de Charlemagne (sim: 0.875)\n",
      "   [‚úó] #3 Regalia du Saint-Empire (sim: 0.873)\n",
      "   [‚úó] #4 Napol√©on Ier (sim: 0.871)\n",
      "   [‚úó] #5 Couronne du Saint-Empire (sim: 0.868)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 11: Article 'Charlemagne'\n",
      "Sentence: 'D'apr√®s Jacques Le Goff, ¬´ On a exag√©r√© en faisant de Charlemagne un Jules Ferry avant la lettre allant encourager les √©l√®ves dans les √©coles. Ces √©coles, cr√©√©es ou d√©velopp√©es par Charlemagne, s'adressaient surtout au fils de l'aristocratie. ¬ª'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   'Jules Ferry' ‚Üí Jules Ferry (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Acad√©mie palatine (sim: 0.848)\n",
      "   [‚úó] #2 √âlitisme en France (sim: 0.845)\n",
      "   [‚úó] #3 Renaissance du XIIe si√®cle (sim: 0.842)\n",
      "   [‚úó] #4 Carolingiens (sim: 0.842)\n",
      "   [‚úó] #5 Histoire de l'√©ducation en France (sim: 0.842)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 12: Article 'Charlemagne'\n",
      "Sentence: 'Depuis 1661, Charlemagne est le patron de l'universit√© de Paris, qui le f√™te encore annuellement au XIX¬†si√®cle et dans plusieurs coll√®ges encore dans la premi√®re moiti√© du XX¬†si√®cle. √Ä l'heure actuelle, l'Association des laur√©ats du concours g√©n√©ral tient toujours son repas annuel aux environs de la Saint-Charlemagne.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (2):\n",
      "   'universit√© de Paris' ‚Üí Ancienne universit√© de Paris (rank: 14)\n",
      "   'concours g√©n√©ral' ‚Üí Concours g√©n√©ral (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Recteur de l'Universit√© de Paris (sim: 0.844)\n",
      "   [‚úó] #2 Universit√© Paris-I-Panth√©on-Sorbonne (sim: 0.835)\n",
      "   [‚úó] #3 Universit√© Paris-Dauphine-PSL (sim: 0.834)\n",
      "   [‚úó] #4 Saint-Verhaegen (sim: 0.833)\n",
      "   [‚úó] #5 Coll√®ge du Mans (sim: 0.832)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 13: Article 'Charlemagne'\n",
      "Sentence: 'Dans ce contexte, on peut comprendre la chanson \"Sacr√© Charlemagne\" interpr√©t√©e par France Gall dans les ann√©es 1960, m√™me si Charlemagne n'a pas \"invent√© l'√©cole\". L'enseignement existait bien avant lui.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 1.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   'France Gall' ‚Üí France Gall (rank: 6)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Sacr√© Charlemagne (sim: 0.892)\n",
      "   [‚úó] #2 Gregorius pr√¶sul (sim: 0.839)\n",
      "   [‚úó] #3 Schola gr√©gorienne (sim: 0.837)\n",
      "   [‚úó] #4 Enseignement du chant gr√©gorien (sim: 0.837)\n",
      "   [‚úó] #5 Schola Cantorum de B√¢le (sim: 0.836)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 14: Article 'Charlemagne'\n",
      "Sentence: 'La guerre franco-allemande de 1870 et les deux guerres mondiales au XX¬†si√®cle voient le d√©veloppement en France d'une vague d'antigermanisme qui fait de Charlemagne le symbole de l'envahisseur, d'o√π sa relative disparition dans l'historiographie fran√ßaise.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (2):\n",
      "   'guerre franco-allemande' ‚Üí Guerre franco-allemande de 1870 (not found)\n",
      "   'antigermanisme' ‚Üí Antigermanisme (rank: 18)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Francophobie (sim: 0.841)\n",
      "   [‚úó] #2 Unification allemande (sim: 0.839)\n",
      "   [‚úó] #3 Relations entre l'Allemagne et la France (sim: 0.838)\n",
      "   [‚úó] #4 Campagne du Rhin de 1796 (sim: 0.836)\n",
      "   [‚úó] #5 Histoire de l'Europe (sim: 0.835)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 15: Article 'Charlemagne'\n",
      "Sentence: 'Charlemagne est avant tout repr√©sent√© dans des enluminures, comme l'attestent les \"Grandes Chroniques de France\" dont les th√®mes du couronnement, du roi guerrier et du d√©fenseur de la chr√©tient√© sont les plus f√©conds, ou des manuscrits du XV¬†si√®cle, tel celui du \"Miroir des Saxons\", qui voient une multiplication des th√®mes iconographiques.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (2):\n",
      "   'enluminure' ‚Üí Enluminure (not found)\n",
      "   'Grandes Chroniques de France' ‚Üí Grandes Chroniques de France (rank: 11)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Grandes Chroniques de France de Charles V (sim: 0.853)\n",
      "   [‚úó] #2 Art pr√©roman (sim: 0.851)\n",
      "   [‚úó] #3 Grandes Chroniques de France (Jean Fouquet) (sim: 0.848)\n",
      "   [‚úó] #4 Si√®cle des Lumi√®res (sim: 0.847)\n",
      "   [‚úó] #5 Enluminure gothique (sim: 0.843)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 16: Article 'Charlemagne'\n",
      "Sentence: 'La vie de Charlemagne est ant√©rieure √† l'apparition de l'h√©raldique, mais sa notori√©t√© lui a valu l'attribution d'armes qui, du fait de l'anachronisme, rel√®vent des armoiries imaginaires. √Ä Charlemagne, empereur d'Occident et roi des Francs, on attribue naturellement un parti d'Empire (aigle bic√©phale) et de France (fleurs de lis). La premi√®re description de ces armes se trouve dans les \"Enfances Ogier\", compos√©es vers 1275 par Adenet le Roi, m√©nestrel et po√®te √† la cour des ducs de Brabant. C'est avec la diffusion de la l√©gende des Neuf Preux, apparue dans \"Les V≈ìux du Paon\", po√®me compos√© vers 1312 par Jacques de Longuyon qu'elles ont connu un succ√®s durable. Ces armes furent reproduites dans les chroniques et g√©n√©alogies armori√©es des dynasties se rattachant √† Charlemagne, et sur les monuments √©lev√©s √† la gloire de l'empereur et du saint, tant en Allemagne qu'en France, o√π s'est d√©velopp√© le culte de saint Charlemagne. Lors de la r√©ception solennelle de Charles Quint par , √† Paris, le , ces armes furent pr√©sent√©es comme le symbole du rapprochement entre le royaume de France et le Saint-Empire.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.17\n",
      "\n",
      "‚úÖ Ground Truth (6):\n",
      "   'h√©raldique' ‚Üí H√©raldique (not found)\n",
      "   'Adenet le Roi' ‚Üí Adenet le Roi (not found)\n",
      "   'Neuf Preux' ‚Üí Neuf Preux (rank: 6)\n",
      "   'V≈ìux du Paon' ‚Üí V≈ìux du paon (not found)\n",
      "   'Charles Quint' ‚Üí Charles Quint (not found)\n",
      "   'Paris' ‚Üí Paris (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Statuette √©questre dite de Charlemagne (sim: 0.882)\n",
      "   [‚úó] #2 Royaut√© (sim: 0.881)\n",
      "   [‚úó] #3 Napol√©on Ier (sim: 0.880)\n",
      "   [‚úó] #4 Buste de Charlemagne (sim: 0.879)\n",
      "   [‚úó] #5 Oriflamme de Saint-Denis (sim: 0.878)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 17: Article 'Charlemagne'\n",
      "Sentence: 'Le j√©suite dans \"Le roy d'armes\", donne d'autres armoiries imaginaires √† Charlemagne. Il √©crit que ¬´ Charlemagne Roy de France et Empereur d'Occident portoit d'azur, √† un aigle √©ploy√© d'or, diad√©m√©, langu√©, &amp; arm√© de gueules, l'estomach charg√© de l'escu de France, qui estoit d'azur, aux fleurs de lis sans nombre, d'or : &amp; telles armes furent port√©es par les empereurs Fran√ßois ses descendants, iusques √† ce que ceux de la maison de Saxe usurp√®rent l'Empire sur les Fran√ßois, car alors ils chang√®rent les √©maux anciens de l'Empire, &amp; prirent le m√©tal, &amp; la couleur des armes de leur Othon, surnomm√© le grand, qui portoit selon sa naissance, fasc√© d'or, &amp; de sable de six pi√®ces, blasonnant les armes de l'Empire, d'or &amp; de sable, arm√©, lampass√©, &amp; couronn√© d'un diad√®me de gueules. ¬ª On suppose que c'est en partie de ce blasonnement donn√© pour Charlemagne que Napol√©on s'inspira pour d√©finir les armoiries de l'Empire Fran√ßais dans le d√©cret du 21 messidor (10 juillet 1804).'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (3):\n",
      "   'maison de Saxe' ‚Üí Ottoniens (not found)\n",
      "   'Othon' ‚Üí Otton Ier (empereur du Saint-Empire) (not found)\n",
      "   'Empire Fran√ßais' ‚Üí Premier Empire (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Armoiries de la France (sim: 0.890)\n",
      "   [‚úó] #2 Couronne de Napol√©on Ier (sim: 0.887)\n",
      "   [‚úó] #3 Buste de Charlemagne (sim: 0.887)\n",
      "   [‚úó] #4 Maison Bernadotte (sim: 0.886)\n",
      "   [‚úó] #5 Armoiries du comt√© de Bourgogne (sim: 0.885)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 18: Article 'Charlemagne'\n",
      "Sentence: 'En France, un grand nombre de rues, d'associations culturelles, de b√¢timents communaux, d'entreprises, d'√©tablissements scolaires utilisent le nom de Charlemagne et de ses anc√™tres. Aux Pays-Bas et en Belgique n√©erlandophone, on trouve plusieurs \"Karel de Grotestraat\". En revanche, l'usage toponymique de \"Karl der Gro√üe\" est assez rare dans les pays germanophones : une \"Karl-der-Gro√üe-Stra√üe\" √† Barum-St. Dionys (Basse-Saxe, district de Lunebourg). √Ä Zurich un \"Zentrum Karl der Grosse\" (graphie suisse avec deux \"s\") sert comme plateforme pour le discours politique et soci√©tal.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (4):\n",
      "   'Barum' ‚Üí Barum (arrondissement de Lunebourg) (not found)\n",
      "   'Basse-Saxe' ‚Üí Basse-Saxe (not found)\n",
      "   'Lunebourg' ‚Üí Lunebourg (not found)\n",
      "   'Zurich' ‚Üí Zurich (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Liste des toponymes juifs en Belgique (sim: 0.861)\n",
      "   [‚úó] #2 Bruxelles (sim: 0.850)\n",
      "   [‚úó] #3 Trait d'union (sim: 0.848)\n",
      "   [‚úó] #4 Tour et Taxis (sim: 0.848)\n",
      "   [‚úó] #5 Fer des marais (sim: 0.848)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 19: Article 'Charlemagne'\n",
      "Sentence: 'L'√©mission \"Secrets d'Histoire\" du sur France 2, intitul√©e \"Sacr√© Charlemagne !\", lui √©tait consacr√©e.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (1):\n",
      "   'France 2' ‚Üí France 2 (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Pi√®ce de 100 francs Charlemagne (sim: 0.845)\n",
      "   [‚úó] #2 22 av. J.-C. (sim: 0.844)\n",
      "   [‚úó] #3 2 av. J.-C. (sim: 0.843)\n",
      "   [‚úó] #4 212 av. J.-C. (sim: 0.841)\n",
      "   [‚úó] #5 122 av. J.-C. (sim: 0.840)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Example 20: Article 'Liste de biologistes'\n",
      "Sentence: 'Cette liste, non-exhaustive, rassemble des biologistes, class√©s par ordre chronologique de naissance.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "‚úÖ Ground Truth (2):\n",
      "   'non-exhaustive' ‚Üí Exhaustivit√© (not found)\n",
      "   'biologiste' ‚Üí Biologiste (not found)\n",
      "\n",
      "üîÆ Top 5 Predictions:\n",
      "   [‚úó] #1 Liste de climatologues (sim: 0.890)\n",
      "   [‚úó] #2 Liste de philosophes des sciences (sim: 0.887)\n",
      "   [‚úó] #3 Liste d'auteurs et autrices √©cof√©ministes (sim: 0.886)\n",
      "   [‚úó] #4 Liste d'√©crivains qu√©b√©cois par ann√©e de naissance (sim: 0.879)\n",
      "   [‚úó] #5 Faune end√©mique de Rodrigues (sim: 0.869)\n"
     ]
    }
   ],
   "source": [
    "def display_detailed_analysis(eval_results: Dict, num_samples: int = 5, k_display: int = 5):\n",
    "    \"\"\"Show detailed examples of predictions vs ground truth using Recall@K.\"\"\"\n",
    "    \n",
    "    results = eval_results['results_per_sentence']\n",
    "    \n",
    "    # Sort by Recall@k_display to show best and worst\n",
    "    sorted_results = sorted(results, key=lambda x: x['recall_at_k'].get(k_display, 0), reverse=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üéØ BEST PREDICTIONS (Highest Recall@{k_display})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, result in enumerate(sorted_results[:num_samples]):\n",
    "        recall_k = result['recall_at_k'].get(k_display, 0)\n",
    "        if recall_k == 0:\n",
    "            continue\n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"Example {i+1}: Article '{result['source_article_title']}'\")\n",
    "        print(f\"Sentence: '{result['sentence']}'\")\n",
    "        print(f\"Recall@1: {result['recall_at_k'].get(1, 0):.2f} | Recall@5: {result['recall_at_k'].get(5, 0):.2f} | Recall@10: {result['recall_at_k'].get(10, 0):.2f}\")\n",
    "        \n",
    "        # Get ground truth IDs and predicted IDs\n",
    "        gt_ids = set(gt['target_id'] for gt in result['ground_truth_links'])\n",
    "        pred_ids_ranked = [p['article_id'] for p in result['predicted_articles']]\n",
    "        top_k_pred_ids = set(pred_ids_ranked[:k_display])\n",
    "        \n",
    "        print(f\"\\n‚úÖ Ground Truth ({len(result['ground_truth_links'])}):\")\n",
    "        for gt in result['ground_truth_links']:\n",
    "            target_title = id_to_title.get(gt['target_id'], f\"ID {gt['target_id']}\")\n",
    "            matched = \"‚úì\" if gt['target_id'] in top_k_pred_ids else \" \"\n",
    "            # Show rank if found\n",
    "            if gt['target_id'] in pred_ids_ranked:\n",
    "                rank = pred_ids_ranked.index(gt['target_id']) + 1\n",
    "                print(f\"   [{matched}] '{gt['anchor']}' ‚Üí {target_title} (rank: {rank})\")\n",
    "            else:\n",
    "                print(f\"   [{matched}] '{gt['anchor']}' ‚Üí {target_title} (not in top {len(pred_ids_ranked)})\")\n",
    "        \n",
    "        print(f\"\\nüîÆ Top {min(k_display, result['num_predictions'])} Predictions:\")\n",
    "        for j, pred in enumerate(result['predicted_articles'][:k_display]):\n",
    "            matched = \"‚úì\" if pred['article_id'] in gt_ids else \"‚úó\"\n",
    "            print(f\"   [{matched}] #{j+1} {pred['article_title']} (sim: {pred['similarity_score']:.3f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"‚ùå WORST PREDICTIONS (Lowest Recall@{k_display}, with predictions)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    worst = [r for r in sorted_results if r['num_predictions'] > 0][-num_samples:]\n",
    "    \n",
    "    for i, result in enumerate(worst):\n",
    "        print(f\"\\n{'‚îÄ'*80}\")\n",
    "        print(f\"Example {i+1}: Article '{result['source_article_title']}'\")\n",
    "        print(f\"Sentence: '{result['sentence']}'\")\n",
    "        print(f\"Recall@1: {result['recall_at_k'].get(1, 0):.2f} | Recall@5: {result['recall_at_k'].get(5, 0):.2f} | Recall@10: {result['recall_at_k'].get(10, 0):.2f}\")\n",
    "        \n",
    "        gt_ids = set(gt['target_id'] for gt in result['ground_truth_links'])\n",
    "        pred_ids_ranked = [p['article_id'] for p in result['predicted_articles']]\n",
    "        \n",
    "        print(f\"\\n‚úÖ Ground Truth ({len(result['ground_truth_links'])}):\")\n",
    "        for gt in result['ground_truth_links']:\n",
    "            target_title = id_to_title.get(gt['target_id'], f\"ID {gt['target_id']}\")\n",
    "            if gt['target_id'] in pred_ids_ranked:\n",
    "                rank = pred_ids_ranked.index(gt['target_id']) + 1\n",
    "                print(f\"   '{gt['anchor']}' ‚Üí {target_title} (rank: {rank})\")\n",
    "            else:\n",
    "                print(f\"   '{gt['anchor']}' ‚Üí {target_title} (not found)\")\n",
    "        \n",
    "        print(f\"\\nüîÆ Top {min(k_display, result['num_predictions'])} Predictions:\")\n",
    "        for j, pred in enumerate(result['predicted_articles'][:k_display]):\n",
    "            matched = \"‚úì\" if pred['article_id'] in gt_ids else \"‚úó\"\n",
    "            print(f\"   [{matched}] #{j+1} {pred['article_title']} (sim: {pred['similarity_score']:.3f})\")\n",
    "\n",
    "# Display analysis\n",
    "display_detailed_analysis(eval_results, num_samples=20, k_display=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8c28985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä SUMMARY STATISTICS (Recall@K)\n",
      "============================================================\n",
      "\n",
      "üéØ Hit Rate (sentences with ‚â•1 correct in top-K):\n",
      "   Hit@1:  6372/25815 (24.7%)\n",
      "   Hit@5:  12330/25815 (47.8%)\n",
      "   Hit@10: 14550/25815 (56.4%)\n",
      "\n",
      "üìà Average Per-Sentence Recall@K:\n",
      "   Avg Recall@1:  0.092\n",
      "   Avg Recall@5:  0.216\n",
      "   Avg Recall@10: 0.277\n",
      "   Avg Recall@20: 0.341\n",
      "\n",
      "üìä Overall Metrics:\n",
      "   MRR: 0.109\n",
      "   Recall@1: 0.061\n",
      "   Recall@5: 0.163\n",
      "   Recall@10: 0.219\n",
      "   Recall@20: 0.283\n",
      "\n",
      "üìã Prediction Distribution:\n",
      "   Avg predictions per sentence: 20.0\n",
      "   Avg ground truth per sentence: 4.1\n",
      "\n",
      "üîç Coverage:\n",
      "   Sentences with 0 predictions: 0\n",
      "   Sentences with 10+ predictions: 25815\n"
     ]
    }
   ],
   "source": [
    "def print_summary_statistics(eval_results: Dict):\n",
    "    \"\"\"Print summary statistics about the evaluation using Recall@K.\"\"\"\n",
    "    \n",
    "    results = eval_results['results_per_sentence']\n",
    "    \n",
    "    # Count sentences with at least one correct prediction in top-K\n",
    "    sentences_with_hit_at_1 = sum(1 for r in results if r['recall_at_k'].get(1, 0) > 0)\n",
    "    sentences_with_hit_at_5 = sum(1 for r in results if r['recall_at_k'].get(5, 0) > 0)\n",
    "    sentences_with_hit_at_10 = sum(1 for r in results if r['recall_at_k'].get(10, 0) > 0)\n",
    "    \n",
    "    # Average Recall@K per sentence\n",
    "    avg_recall_at_1 = np.mean([r['recall_at_k'].get(1, 0) for r in results])\n",
    "    avg_recall_at_5 = np.mean([r['recall_at_k'].get(5, 0) for r in results])\n",
    "    avg_recall_at_10 = np.mean([r['recall_at_k'].get(10, 0) for r in results])\n",
    "    avg_recall_at_20 = np.mean([r['recall_at_k'].get(20, 0) for r in results])\n",
    "    \n",
    "    # Distribution of predictions\n",
    "    pred_counts = [r['num_predictions'] for r in results]\n",
    "    gt_counts = [r['num_ground_truth'] for r in results]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä SUMMARY STATISTICS (Recall@K)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüéØ Hit Rate (sentences with ‚â•1 correct in top-K):\")\n",
    "    print(f\"   Hit@1:  {sentences_with_hit_at_1}/{len(results)} ({100*sentences_with_hit_at_1/len(results):.1f}%)\")\n",
    "    print(f\"   Hit@5:  {sentences_with_hit_at_5}/{len(results)} ({100*sentences_with_hit_at_5/len(results):.1f}%)\")\n",
    "    print(f\"   Hit@10: {sentences_with_hit_at_10}/{len(results)} ({100*sentences_with_hit_at_10/len(results):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìà Average Per-Sentence Recall@K:\")\n",
    "    print(f\"   Avg Recall@1:  {avg_recall_at_1:.3f}\")\n",
    "    print(f\"   Avg Recall@5:  {avg_recall_at_5:.3f}\")\n",
    "    print(f\"   Avg Recall@10: {avg_recall_at_10:.3f}\")\n",
    "    print(f\"   Avg Recall@20: {avg_recall_at_20:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìä Overall Metrics:\")\n",
    "    print(f\"   MRR: {eval_results['mrr']:.3f}\")\n",
    "    for k, recall in eval_results['recall_at_k'].items():\n",
    "        print(f\"   Recall@{k}: {recall:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìã Prediction Distribution:\")\n",
    "    print(f\"   Avg predictions per sentence: {np.mean(pred_counts):.1f}\")\n",
    "    print(f\"   Avg ground truth per sentence: {np.mean(gt_counts):.1f}\")\n",
    "    \n",
    "    print(f\"\\nüîç Coverage:\")\n",
    "    print(f\"   Sentences with 0 predictions: {sum(1 for p in pred_counts if p == 0)}\")\n",
    "    print(f\"   Sentences with 10+ predictions: {sum(1 for p in pred_counts if p >= 10)}\")\n",
    "\n",
    "print_summary_statistics(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DCL_WIKI_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
