{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ca5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "#If you already saved the model locally and are using docker\n",
    "client = QdrantClient(host=\"localhost\", port=6333, prefer_grpc=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"models/multilingual-e5-large\", device=device)\n",
    "\n",
    "# Run docker container before\n",
    "# docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v /home/Loris/EPFL/MA3/ML/project2/project2Rag/qdrant_storage:/qdrant/storage qdrant/qdrant:latest\n",
    "\n",
    "# VERY IMPORTANT : to query the remote qdrand docker container over ssh, run the following in a terminale before:\n",
    "# ssh -L 6333:localhost:6333 -L 6334:localhost:6334 <your username>@dclgpusrv.epfl.ch -N\n",
    "#Make sure to have your ssh keys before\n",
    "#This forwards the qdrant ports to your local machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9929b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles stored: 2556402\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http import models as rest\n",
    "#get articles count from Qdrant DB \n",
    "\n",
    "collection_name = \"wikipedia_fr\"\n",
    "total = client.count(collection_name=collection_name, exact=True).count\n",
    "print(f\"Total articles stored: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2cb083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Dataset loaded as LazyFrame (not in memory)\n",
      "   Schema: Schema([('id', Int64), ('title', String), ('text', String), ('links', List(Struct({'full_url': String, 'start_idx': Int64, 'anchor': String, 'href_raw': String, 'href_decoded': String}))), ('link_count', UInt32), ('text_withoutHref', String)])\n",
      "\n",
      "   Link structure example:\n",
      "   shape: (2,)\n",
      "Series: '' [struct[5]]\n",
      "[\n",
      "\t{\"https://fr.wikipedia.org/wiki/Moulins%20%28Allier%29\",25,\"Moulins\",\"Moulins%20%28Allier%29\",\"Moulins (Allier)\"}\n",
      "\t{\"https://fr.wikipedia.org/wiki/Allier%20%28d%C3%A9partement%29\",83,\"Allier\",\"Allier%20%28d%C3%A9partement%29\",\"Allier (dÃ©partement)\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Keep as LazyFrame - don't collect the full dataset!\n",
    "df_lazy = pl.scan_parquet(\"articles_fr_merged.parquet\").filter(\n",
    "    pl.col('link_count') > 0\n",
    ")\n",
    "\n",
    "# Only collect a small sample to verify structure\n",
    "sample = df_lazy.head(3).collect()\n",
    "\n",
    "print(f\"ðŸ“š Dataset loaded as LazyFrame (not in memory)\")\n",
    "print(f\"   Schema: {df_lazy.collect_schema()}\")\n",
    "print(f\"\\n   Link structure example:\")\n",
    "print(f\"   {sample.select('links').to_series()[0][:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ce5997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—ºï¸  Building URL â†’ ID mapping from 'wikipedia_fr'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 12,682,763 URL mappings from 2,556,402 articles\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def create_url_to_id_mapping_from_qdrant(client, collection_name: str = \"wikipedia_fr\") -> Dict[str, int]:\n",
    "    \"\"\"Build URL â†’ ID mapping using articles in Qdrant.\"\"\"\n",
    "    from urllib.parse import quote\n",
    "    \n",
    "    url_to_id = {}\n",
    "    id_to_title = {}\n",
    "    \n",
    "    print(f\"ðŸ—ºï¸  Building URL â†’ ID mapping from '{collection_name}'...\")\n",
    "    \n",
    "    offset = None\n",
    "    batch_size = 1000\n",
    "    total_processed = 0\n",
    "    \n",
    "    while True:\n",
    "        points, offset = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=batch_size,\n",
    "            offset=offset,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        \n",
    "        if not points:\n",
    "            break\n",
    "        \n",
    "        for point in points:\n",
    "            article_id = point.payload.get(\"id\")\n",
    "            title = point.payload.get(\"title\", \"\")\n",
    "            \n",
    "            if not article_id or not title:\n",
    "                continue\n",
    "            \n",
    "            id_to_title[article_id] = title\n",
    "            \n",
    "            # Create URL pattern variations\n",
    "            patterns = [\n",
    "                title,\n",
    "                title.replace(\" \", \"_\"),\n",
    "                quote(title.replace(\" \", \"_\"), safe=\"\"),\n",
    "                title.lower(),\n",
    "                title.lower().replace(\" \", \"_\"),\n",
    "                title.lower().replace(\" \", \"%20\"),\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                url_to_id[pattern] = article_id\n",
    "        \n",
    "        total_processed += len(points)\n",
    "        \n",
    "        if offset is None:\n",
    "            break\n",
    "    \n",
    "    print(f\"âœ… Created {len(url_to_id):,} URL mappings from {total_processed:,} articles\")\n",
    "    return url_to_id, id_to_title\n",
    "\n",
    "# Build mappings\n",
    "url_to_id, id_to_title = create_url_to_id_mapping_from_qdrant(client, \"wikipedia_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d85d9fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sentence extraction function defined\n"
     ]
    }
   ],
   "source": [
    "def extract_sentences_with_links_from_positions(\n",
    "    text: str, \n",
    "    links: list\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Extract sentences containing hyperlinks using position information.\"\"\"\n",
    "    if not text or not links or len(links) == 0:\n",
    "        return []\n",
    "    \n",
    "    sorted_links = sorted(links, key=lambda x: x.get(\"start_idx\", 0))\n",
    "    sentence_pattern = r'[.!?]+\\s+'\n",
    "    sentences = re.split(sentence_pattern, text)\n",
    "    \n",
    "    results = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if not sentence.strip():\n",
    "            continue\n",
    "        \n",
    "        sent_start = text.find(sentence, current_pos)\n",
    "        if sent_start == -1:\n",
    "            continue\n",
    "        sent_end = sent_start + len(sentence)\n",
    "        \n",
    "        links_in_sent = []\n",
    "        for link in sorted_links:\n",
    "            link_pos = link.get(\"start_idx\", -1)\n",
    "            \n",
    "            if sent_start <= link_pos < sent_end:\n",
    "                links_in_sent.append({\n",
    "                    'anchor': link.get('anchor', ''),\n",
    "                    'href_decoded': link.get('href_decoded', ''),\n",
    "                    'position': link_pos,\n",
    "                })\n",
    "        \n",
    "        if links_in_sent:\n",
    "            results.append({\n",
    "                'sentence': sentence.strip(),\n",
    "                'links_in_sentence': links_in_sent,\n",
    "                'start_pos': sent_start,\n",
    "                'num_links': len(links_in_sent)\n",
    "            })\n",
    "        \n",
    "        current_pos = sent_end\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Sentence extraction function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faee200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Creating test dataset (max 200 sentences)...\n",
      "   Sampling 5,000 articles from LazyFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24626/799159996.py:24: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n",
      "  .collect(streaming=True)  # Stream processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 5,000 articles into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 9/5000 [00:00<03:52, 21.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Created test dataset:\n",
      "   Sentences: 200\n",
      "   Articles processed: 9\n",
      "   Total ground truth links: 285\n"
     ]
    }
   ],
   "source": [
    "def create_test_dataset_optimized(\n",
    "    df_lazy: pl.LazyFrame,\n",
    "    url_to_id: Dict[str, int],\n",
    "    max_sentences: int = 100,\n",
    "    min_links_per_sentence: int = 1,\n",
    "    sample_articles: int = 5000  # Sample size to avoid loading all 2.5M\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create test dataset using LazyFrame streaming.\n",
    "    Only loads a sample of articles into memory.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” Creating test dataset (max {max_sentences} sentences)...\")\n",
    "    print(f\"   Sampling {sample_articles:,} articles from LazyFrame...\")\n",
    "    \n",
    "    # Sample articles using LazyFrame operations\n",
    "    # This only materializes the sample, not the full dataset\n",
    "    sampled_df = (\n",
    "        df_lazy\n",
    "        .filter(pl.col('link_count') > 0)\n",
    "        .filter(pl.col('text_withoutHref').is_not_null())\n",
    "        .filter(pl.col('text_withoutHref').str.len_chars() > 100)  # Skip very short articles\n",
    "        .select(['id', 'title', 'text_withoutHref', 'links'])  # Only needed columns\n",
    "        .head(sample_articles * 3)  # Get more than needed to allow for filtering\n",
    "        .collect(streaming=True)  # Stream processing\n",
    "        .sample(n=min(sample_articles, sample_articles * 3), shuffle=True, seed=42)\n",
    "    )\n",
    "    \n",
    "    print(f\"   Loaded {len(sampled_df):,} articles into memory\")\n",
    "    \n",
    "    test_data = []\n",
    "    articles_processed = 0\n",
    "    \n",
    "    for row in tqdm(sampled_df.iter_rows(named=True), total=len(sampled_df), desc=\"Processing\"):\n",
    "        if len(test_data) >= max_sentences:\n",
    "            break\n",
    "        \n",
    "        article_id = row[\"id\"]\n",
    "        article_title = row[\"title\"]\n",
    "        text = row.get(\"text_withoutHref\", \"\")\n",
    "        links_raw = row.get(\"links\", [])\n",
    "        \n",
    "        if links_raw is None:\n",
    "            continue\n",
    "        links = list(links_raw) if hasattr(links_raw, '__iter__') else []\n",
    "        \n",
    "        if not text or len(links) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Extract sentences with links\n",
    "        sentences_with_links = extract_sentences_with_links_from_positions(text, links)\n",
    "        \n",
    "        for sent_data in sentences_with_links:\n",
    "            if len(test_data) >= max_sentences:\n",
    "                break\n",
    "            \n",
    "            sentence = sent_data[\"sentence\"]\n",
    "            links_in_sent = sent_data[\"links_in_sentence\"]\n",
    "            \n",
    "            # Map links to article IDs\n",
    "            ground_truth_links = []\n",
    "            for link in links_in_sent:\n",
    "                href_decoded = link[\"href_decoded\"]\n",
    "                anchor = link[\"anchor\"]\n",
    "                \n",
    "                if not anchor or not anchor.strip():\n",
    "                    continue\n",
    "                \n",
    "                target_id = url_to_id.get(href_decoded)\n",
    "                if not target_id:\n",
    "                    for variation in [\n",
    "                        href_decoded.replace(\"_\", \" \"),\n",
    "                        href_decoded.replace(\"%20\", \" \"),\n",
    "                        href_decoded.lower(),\n",
    "                        href_decoded.lower().replace(\"_\", \" \")\n",
    "                    ]:\n",
    "                        target_id = url_to_id.get(variation)\n",
    "                        if target_id:\n",
    "                            break\n",
    "                \n",
    "                if target_id and target_id != article_id:\n",
    "                    ground_truth_links.append({\n",
    "                        'anchor': anchor,\n",
    "                        'target_id': target_id,\n",
    "                        'href_decoded': href_decoded\n",
    "                    })\n",
    "            \n",
    "            if len(ground_truth_links) >= min_links_per_sentence:\n",
    "                test_data.append({\n",
    "                    'source_article_id': article_id,\n",
    "                    'source_article_title': article_title,\n",
    "                    'sentence': sentence,\n",
    "                    'ground_truth_links': ground_truth_links,\n",
    "                    'num_ground_truth': len(ground_truth_links)\n",
    "                })\n",
    "        \n",
    "        articles_processed += 1\n",
    "    \n",
    "    # Free memory\n",
    "    del sampled_df\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\nâœ… Created test dataset:\")\n",
    "    print(f\"   Sentences: {len(test_data)}\")\n",
    "    print(f\"   Articles processed: {articles_processed}\")\n",
    "    total_gt_links = sum(t['num_ground_truth'] for t in test_data)\n",
    "    print(f\"   Total ground truth links: {total_gt_links}\")\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "# Create test dataset - pass LazyFrame, not collected DataFrame\n",
    "test_data = create_test_dataset_optimized(\n",
    "    df_lazy,  # LazyFrame, not df\n",
    "    url_to_id, \n",
    "    max_sentences=200,\n",
    "    min_links_per_sentence=1,\n",
    "    sample_articles=5000  # Only load 5K articles into memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd64c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”® Predicting links for 200 sentences...\n",
      "   Encoding sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a71de1c624464f89f16494dceba8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Querying Qdrant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_24626/2008121553.py:38: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = client.search(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:04<00:00, 45.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated predictions for 200 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_links_for_sentences(\n",
    "    client,\n",
    "    model,\n",
    "    test_data: List[Dict],\n",
    "    collection_name: str = \"wikipedia_fr\",\n",
    "    top_k: int = 10,\n",
    "    min_similarity: float = 0.5\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Predict links by finding similar articles in Qdrant.\n",
    "    \n",
    "    Approach: Encode sentence â†’ Find similar articles â†’ Suggest those articles as links\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”® Predicting links for {len(test_data)} sentences...\")\n",
    "    \n",
    "    # Extract all sentences\n",
    "    sentences = [t['sentence'] for t in test_data]\n",
    "    source_ids = [t['source_article_id'] for t in test_data]\n",
    "    \n",
    "    # Encode all sentences\n",
    "    print(\"   Encoding sentences...\")\n",
    "    embeddings = model.encode(\n",
    "        sentences,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    print(\"   Querying Qdrant...\")\n",
    "    for i, (sentence, embedding, source_id) in enumerate(tqdm(\n",
    "        zip(sentences, embeddings, source_ids), \n",
    "        total=len(sentences)\n",
    "    )):\n",
    "        # Search for similar articles\n",
    "        search_results = client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=embedding.tolist(),\n",
    "            limit=top_k + 1,  # Extra to account for self-match\n",
    "            score_threshold=min_similarity\n",
    "        )\n",
    "        \n",
    "        # Filter out source article and collect predictions\n",
    "        predicted_articles = []\n",
    "        for result in search_results:\n",
    "            article_id = result.payload.get('id')\n",
    "            \n",
    "            # Skip self-match\n",
    "            if article_id == source_id:\n",
    "                continue\n",
    "            \n",
    "            predicted_articles.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': result.payload.get('title', f'ID {article_id}'),\n",
    "                'similarity_score': result.score,\n",
    "                'text_preview': result.payload.get('text_withoutHref', '')[:150] + '...'\n",
    "            })\n",
    "        \n",
    "        # Limit to top_k after filtering\n",
    "        predicted_articles = predicted_articles[:top_k]\n",
    "        \n",
    "        predictions.append({\n",
    "            **test_data[i],\n",
    "            'predicted_articles': predicted_articles,\n",
    "            'num_predictions': len(predicted_articles)\n",
    "        })\n",
    "    \n",
    "    print(f\"âœ… Generated predictions for {len(predictions)} sentences\")\n",
    "    return predictions\n",
    "\n",
    "# Generate predictions\n",
    "predictions = predict_links_for_sentences(\n",
    "    client,\n",
    "    model,\n",
    "    test_data,\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    top_k=10,\n",
    "    min_similarity=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09b13458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Š EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ Overall Metrics:\n",
      "   Precision: 0.002 (3/2000)\n",
      "   Recall:    0.011 (3/285)\n",
      "   F1 Score:  0.003\n",
      "\n",
      "ðŸ“‹ Dataset Stats:\n",
      "   Test sentences: 200\n",
      "   Total ground truth links: 285\n",
      "   Total predictions: 2000\n",
      "   Correct predictions: 3\n"
     ]
    }
   ],
   "source": [
    "def evaluate_predictions(predictions: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate predictions against ground truth.\n",
    "    \n",
    "    A prediction is correct if the predicted article ID matches\n",
    "    any of the ground truth target article IDs.\n",
    "    \"\"\"\n",
    "    total_ground_truth = 0\n",
    "    total_predictions = 0\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    results_per_sentence = []\n",
    "    \n",
    "    for pred in predictions:\n",
    "        gt_ids = set(link['target_id'] for link in pred['ground_truth_links'])\n",
    "        pred_ids = set(p['article_id'] for p in pred['predicted_articles'])\n",
    "        \n",
    "        # Count matches\n",
    "        matches = gt_ids & pred_ids\n",
    "        \n",
    "        total_ground_truth += len(gt_ids)\n",
    "        total_predictions += len(pred_ids)\n",
    "        correct_predictions += len(matches)\n",
    "        \n",
    "        # Per-sentence metrics\n",
    "        precision = len(matches) / len(pred_ids) if pred_ids else 0\n",
    "        recall = len(matches) / len(gt_ids) if gt_ids else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results_per_sentence.append({\n",
    "            **pred,\n",
    "            'matched_ids': list(matches),\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        })\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_precision = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    overall_recall = correct_predictions / total_ground_truth if total_ground_truth > 0 else 0\n",
    "    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nðŸ“ˆ Overall Metrics:\")\n",
    "    print(f\"   Precision: {overall_precision:.3f} ({correct_predictions}/{total_predictions})\")\n",
    "    print(f\"   Recall:    {overall_recall:.3f} ({correct_predictions}/{total_ground_truth})\")\n",
    "    print(f\"   F1 Score:  {overall_f1:.3f}\")\n",
    "    print(f\"\\nðŸ“‹ Dataset Stats:\")\n",
    "    print(f\"   Test sentences: {len(predictions)}\")\n",
    "    print(f\"   Total ground truth links: {total_ground_truth}\")\n",
    "    print(f\"   Total predictions: {total_predictions}\")\n",
    "    print(f\"   Correct predictions: {correct_predictions}\")\n",
    "    \n",
    "    return {\n",
    "        'precision': overall_precision,\n",
    "        'recall': overall_recall,\n",
    "        'f1': overall_f1,\n",
    "        'total_ground_truth': total_ground_truth,\n",
    "        'total_predictions': total_predictions,\n",
    "        'correct_predictions': correct_predictions,\n",
    "        'results_per_sentence': results_per_sentence\n",
    "    }\n",
    "\n",
    "# Evaluate\n",
    "eval_results = evaluate_predictions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ecaef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ BEST PREDICTIONS (Highest F1)\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 1: Article 'Haute qualitÃ© environnementale'\n",
      "Sentence: 'La dÃ©marche HQE Ã©tant critiquÃ©e pour son manque de lisibilitÃ© et sa dÃ©fense des intÃ©rÃªts commerciaux des industriels : l...'\n",
      "Precision: 0.10 | Recall: 1.00 | F1: 0.18\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'haute performance Ã©nergÃ©tique' â†’ Haute performance Ã©nergÃ©tique\n",
      "\n",
      "ðŸ”® Top Predictions (5):\n",
      "   [âœ—] Association HQE (sim: 0.860)\n",
      "   [âœ—] Association des professionnels de la construction et de l'habitation du QuÃ©bec (sim: 0.838)\n",
      "   [âœ—] QualitÃ© environnementale (sim: 0.831)\n",
      "   [âœ—] Association des constructeurs europÃ©ens d'automobiles (sim: 0.830)\n",
      "   [âœ—] The Society of British Interior Design (sim: 0.829)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 2: Article 'Haute qualitÃ© environnementale'\n",
      "Sentence: 'La HQE serait un Â« impensÃ© politique Â» permettant de gÃ©nÃ©rer du profit sur le dos de l'environnement grÃ¢ce Ã  la mode de ...'\n",
      "Precision: 0.10 | Recall: 1.00 | F1: 0.18\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'biodiversitÃ© dans le bÃ¢ti et le jardin' â†’ BiodiversitÃ© dans le bÃ¢ti et le jardin\n",
      "\n",
      "ðŸ”® Top Predictions (5):\n",
      "   [âœ—] QuinziÃ¨me cible HQE (sim: 0.847)\n",
      "   [âœ—] Route HQE (sim: 0.846)\n",
      "   [âœ—] ClÃ´ture de haute qualitÃ© environnementale (sim: 0.838)\n",
      "   [âœ—] Technique alternative de gestion des eaux de ruissellement urbain (sim: 0.837)\n",
      "   [âœ—] QualitÃ© environnementale (sim: 0.837)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 3: Article 'Haute qualitÃ© environnementale'\n",
      "Sentence: 'La dÃ©marche pour l'obtention de la certification peut Ãªtre effectuÃ©e par l'association HQE, association franÃ§aise reconn...'\n",
      "Precision: 0.10 | Recall: 0.50 | F1: 0.17\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   [ ] 'label' â†’ Label de qualitÃ©\n",
      "   [âœ“] 'haute performance Ã©nergÃ©tique' â†’ Haute performance Ã©nergÃ©tique\n",
      "\n",
      "ðŸ”® Top Predictions (5):\n",
      "   [âœ—] Association HQE (sim: 0.864)\n",
      "   [âœ—] Commission nationale de la certification environnementale (sim: 0.842)\n",
      "   [âœ—] Haut conseil des Maliens de l'extÃ©rieur (sim: 0.838)\n",
      "   [âœ—] Health &amp; Safety Executive (sim: 0.836)\n",
      "   [âœ—] Solicitors Qualifying Examination (sim: 0.834)\n",
      "\n",
      "================================================================================\n",
      "âŒ WORST PREDICTIONS (Lowest F1, with predictions)\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 1: Article 'ChÃ¢teau-Thierry'\n",
      "Sentence: 'C'est de l'Ã©poque d'Herbert II que date la premiÃ¨re attestation d'un site fortifiÃ© et du nom de ChÃ¢teau-Thierry dans les...'\n",
      "Precision: 0.00 | Recall: 0.00 | F1: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'Fablio' â†’ Transports en commun de ChÃ¢teau-Thierry\n",
      "\n",
      "ðŸ”® Top Predictions (3):\n",
      "   âœ— ChÃ¢teau de Thoraise (sim: 0.835)\n",
      "   âœ— ChÃ¢teau de Thurant (sim: 0.835)\n",
      "   âœ— Fort Sainte-ThÃ©rÃ¨se (sim: 0.833)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 2: Article 'ChÃ¢teau-Thierry'\n",
      "Sentence: 'En effet, en 923, le comte y enferme Charles III, dit le Simple, pendant quatre ans...'\n",
      "Precision: 0.00 | Recall: 0.00 | F1: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'DÃ©lÃ©gation de Service Public' â†’ DÃ©lÃ©gation de service public\n",
      "\n",
      "ðŸ”® Top Predictions (3):\n",
      "   âœ— 893 (sim: 0.862)\n",
      "   âœ— 903 (sim: 0.852)\n",
      "   âœ— Epte (homonymie) (sim: 0.839)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 3: Article 'ChÃ¢teau-Thierry'\n",
      "Sentence: 'Entre 933 et 936, lors d'un conflit avec Raoul, roi de Bourgogne, la forteresse est assiÃ©gÃ©e par deux fois...'\n",
      "Precision: 0.00 | Recall: 0.00 | F1: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'Keolis' â†’ Keolis\n",
      "\n",
      "ðŸ”® Top Predictions (3):\n",
      "   âœ— Fort Jullien (sim: 0.830)\n",
      "   âœ— Lunettes de Trois-ChÃ¢tels et de Tousey (sim: 0.829)\n",
      "   âœ— Fortifications de Rouen (sim: 0.829)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 4: Article 'ChÃ¢teau-Thierry'\n",
      "Sentence: 'Elle revient Ã  Herbert II, en 936, Ã  la mort de Raoul...'\n",
      "Precision: 0.00 | Recall: 0.00 | F1: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'RÃ©seau interurbain de l'Aisne' â†’ RÃ©seau interurbain de l'Aisne\n",
      "\n",
      "ðŸ”® Top Predictions (3):\n",
      "   âœ— Ulrich II de Wurtemberg (sim: 0.845)\n",
      "   âœ— 972 (sim: 0.838)\n",
      "   âœ— 2019 en rugby Ã  XIII (sim: 0.836)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 5: Article 'ChÃ¢teau-Thierry'\n",
      "Sentence: 'Ã€ la mort d'Herbert II, son fils Herbert III dit le Vieux, comte de Troyes et de Meaux, hÃ©rite de la forteresse de ChÃ¢te...'\n",
      "Precision: 0.00 | Recall: 0.00 | F1: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'aÃ©rodrome' â†’ AÃ©rodrome de ChÃ¢teau-Thierry - Belleau\n",
      "\n",
      "ðŸ”® Top Predictions (3):\n",
      "   âœ— Herbert le Jeune, comte de Troyes et de Meaux (sim: 0.871)\n",
      "   âœ— Herbert III d'Omois (sim: 0.855)\n",
      "   âœ— Ã‰tienne Ier de Troyes (sim: 0.851)\n"
     ]
    }
   ],
   "source": [
    "def display_detailed_analysis(eval_results: Dict, num_samples: int = 5):\n",
    "    \"\"\"Show detailed examples of predictions vs ground truth.\"\"\"\n",
    "    \n",
    "    results = eval_results['results_per_sentence']\n",
    "    \n",
    "    # Sort by F1 score to show best and worst\n",
    "    sorted_results = sorted(results, key=lambda x: x['f1'], reverse=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ¯ BEST PREDICTIONS (Highest F1)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, result in enumerate(sorted_results[:num_samples]):\n",
    "        if result['f1'] == 0:\n",
    "            continue\n",
    "        print(f\"\\n{'â”€'*80}\")\n",
    "        print(f\"Example {i+1}: Article '{result['source_article_title']}'\")\n",
    "        print(f\"Sentence: '{result['sentence'][:120]}...'\")\n",
    "        print(f\"Precision: {result['precision']:.2f} | Recall: {result['recall']:.2f} | F1: {result['f1']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Ground Truth ({result['num_ground_truth']}):\")\n",
    "        for gt in result['ground_truth_links']:\n",
    "            target_title = id_to_title.get(gt['target_id'], f\"ID {gt['target_id']}\")\n",
    "            matched = \"âœ“\" if gt['target_id'] in result['matched_ids'] else \" \"\n",
    "            print(f\"   [{matched}] '{gt['anchor']}' â†’ {target_title}\")\n",
    "        \n",
    "        print(f\"\\nðŸ”® Top Predictions ({min(5, result['num_predictions'])}):\")\n",
    "        for pred in result['predicted_articles'][:5]:\n",
    "            matched = \"âœ“\" if pred['article_id'] in result['matched_ids'] else \"âœ—\"\n",
    "            print(f\"   [{matched}] {pred['article_title']} (sim: {pred['similarity_score']:.3f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âŒ WORST PREDICTIONS (Lowest F1, with predictions)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get worst predictions that have at least some predictions\n",
    "    worst = [r for r in sorted_results if r['num_predictions'] > 0][-num_samples:]\n",
    "    \n",
    "    for i, result in enumerate(worst):\n",
    "        print(f\"\\n{'â”€'*80}\")\n",
    "        print(f\"Example {i+1}: Article '{result['source_article_title']}'\")\n",
    "        print(f\"Sentence: '{result['sentence'][:120]}...'\")\n",
    "        print(f\"Precision: {result['precision']:.2f} | Recall: {result['recall']:.2f} | F1: {result['f1']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Ground Truth ({result['num_ground_truth']}):\")\n",
    "        for gt in result['ground_truth_links']:\n",
    "            target_title = id_to_title.get(gt['target_id'], f\"ID {gt['target_id']}\")\n",
    "            print(f\"   '{gt['anchor']}' â†’ {target_title}\")\n",
    "        \n",
    "        print(f\"\\nðŸ”® Top Predictions ({min(3, result['num_predictions'])}):\")\n",
    "        for pred in result['predicted_articles'][:3]:\n",
    "            print(f\"   âœ— {pred['article_title']} (sim: {pred['similarity_score']:.3f})\")\n",
    "\n",
    "# Display analysis\n",
    "display_detailed_analysis(eval_results, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c28985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Š SUMMARY STATISTICS\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Hit Rate:\n",
      "   Sentences with â‰¥1 correct prediction: 3/200 (1.5%)\n",
      "\n",
      "ðŸ“ˆ Average Per-Sentence Metrics:\n",
      "   Avg Precision: 0.002\n",
      "   Avg Recall:    0.013\n",
      "   Avg F1:        0.003\n",
      "\n",
      "ðŸ“‹ Prediction Distribution:\n",
      "   Avg predictions per sentence: 10.0\n",
      "   Avg ground truth per sentence: 1.4\n",
      "\n",
      "ðŸ” Coverage:\n",
      "   Sentences with 0 predictions: 0\n",
      "   Sentences with 5+ predictions: 200\n"
     ]
    }
   ],
   "source": [
    "def print_summary_statistics(eval_results: Dict):\n",
    "    \"\"\"Print summary statistics about the evaluation.\"\"\"\n",
    "    \n",
    "    results = eval_results['results_per_sentence']\n",
    "    \n",
    "    # Count sentences with at least one correct prediction\n",
    "    sentences_with_match = sum(1 for r in results if len(r['matched_ids']) > 0)\n",
    "    \n",
    "    # Average metrics\n",
    "    avg_precision = np.mean([r['precision'] for r in results])\n",
    "    avg_recall = np.mean([r['recall'] for r in results])\n",
    "    avg_f1 = np.mean([r['f1'] for r in results])\n",
    "    \n",
    "    # Distribution of predictions\n",
    "    pred_counts = [r['num_predictions'] for r in results]\n",
    "    gt_counts = [r['num_ground_truth'] for r in results]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Hit Rate:\")\n",
    "    print(f\"   Sentences with â‰¥1 correct prediction: {sentences_with_match}/{len(results)} ({100*sentences_with_match/len(results):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Average Per-Sentence Metrics:\")\n",
    "    print(f\"   Avg Precision: {avg_precision:.3f}\")\n",
    "    print(f\"   Avg Recall:    {avg_recall:.3f}\")\n",
    "    print(f\"   Avg F1:        {avg_f1:.3f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Prediction Distribution:\")\n",
    "    print(f\"   Avg predictions per sentence: {np.mean(pred_counts):.1f}\")\n",
    "    print(f\"   Avg ground truth per sentence: {np.mean(gt_counts):.1f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ” Coverage:\")\n",
    "    print(f\"   Sentences with 0 predictions: {sum(1 for p in pred_counts if p == 0)}\")\n",
    "    print(f\"   Sentences with 5+ predictions: {sum(1 for p in pred_counts if p >= 5)}\")\n",
    "\n",
    "print_summary_statistics(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
