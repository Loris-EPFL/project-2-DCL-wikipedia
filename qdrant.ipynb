{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55eaadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads .env into environment variables\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "# ----------------------------\n",
    "# Step 1: Connect to Qdrant\n",
    "# ----------------------------\n",
    "# Option 1: In-memory (no Docker)\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# # Option 2: Local Qdrant server\n",
    "# client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Initialize model\n",
    "# ----------------------------\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "model.save(\"models/multilingual-e5-large\") #save model locally (do only once)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1ce311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "model = SentenceTransformer(\"models/multilingual-e5-large\")\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "# client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "326c9a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 4498441 articles into a single Polars DataFrame\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# from pathlib import Path\n",
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# # ----------------------------\n",
    "# # Step 3: Load first 100 articles from JSON\n",
    "# # ----------------------------\n",
    "# path = Path(\"../wikiextractor/articles_fr_withLinks.json\")\n",
    "# # ----------------------------\n",
    "# # Step 3: Load articles from JSON (all lines into DataFrame)\n",
    "# # ----------------------------\n",
    "# # ... existing code ...\n",
    "# ARTICLES_NUMBER = None  # None = load all; set an int to cap when sampling\n",
    "\n",
    "# def load_jsonl_to_df(path, max_rows=None):\n",
    "#     rows = []\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for i, line in enumerate(f):\n",
    "#             if max_rows is not None and i >= max_rows:\n",
    "#                 break\n",
    "#             try:\n",
    "#                 rows.append(json.loads(line))\n",
    "#             except json.JSONDecodeError:\n",
    "#                 continue\n",
    "#     df = pd.DataFrame(rows)\n",
    "#     # Ensure consistent types and fill missing fields\n",
    "#     if \"id\" in df.columns:\n",
    "#         df[\"id\"] = pd.to_numeric(df[\"id\"], errors=\"coerce\").astype(\"Int64\").astype(int)\n",
    "#     df[\"title\"] = df.get(\"title\", \"\").fillna(\"\")\n",
    "#     df[\"url\"] = df.get(\"url\", \"\").fillna(\"\")\n",
    "#     df[\"text\"] = df.get(\"text\", \"\").fillna(\"\")\n",
    "#     return df\n",
    "\n",
    "# df = load_jsonl_to_df(path, max_rows=ARTICLES_NUMBER)\n",
    "# print(f\"âœ… Loaded {len(df)} articles into a single DataFrame\")\n",
    "# # ... existing code ...\n",
    "\n",
    "# # ----------------------------\n",
    "# # Step 3: Create collection\n",
    "# # ----------------------------\n",
    "# vector_size = model.get_sentence_embedding_dimension()\n",
    "\n",
    "# client.recreate_collection(\n",
    "#     collection_name=\"wikipedia_fr\",\n",
    "#     vectors_config=VectorParams(size=vector_size, distance=\"Cosine\"),\n",
    "# )\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Load articles from JSON (Polars)\n",
    "# ----------------------------\n",
    "from pathlib import Path\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "path = Path(\"../wikiextractor/articles_fr_withLinks.json\")\n",
    "\n",
    "ARTICLES_NUMBER = None  # None = load all; set an int to cap when sampling\n",
    "\n",
    "def load_jsonl_to_df(path: Path, max_rows: int | None = None) -> pl.DataFrame:\n",
    "    # If loading the full dataset, use Polars' fast NDJSON reader\n",
    "    if max_rows is None:\n",
    "        try:\n",
    "            df = pl.read_ndjson(str(path))\n",
    "        except Exception:\n",
    "            # Fallback if the NDJSON reader hits malformed lines: manual parse\n",
    "            rows = []\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        rows.append(json.loads(line))\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "            df = pl.DataFrame(rows)\n",
    "    else:\n",
    "        # Limited load: manual parse up to max_rows\n",
    "        rows = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                try:\n",
    "                    rows.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        df = pl.DataFrame(rows)\n",
    "\n",
    "    # Ensure columns exist and normalize types/nulls\n",
    "    cols = set(df.columns)\n",
    "\n",
    "    if \"id\" in cols:\n",
    "        # Cast id to Int64\n",
    "        df = df.with_columns(pl.col(\"id\").cast(pl.Int64))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(None).cast(pl.Int64).alias(\"id\"))\n",
    "\n",
    "    if \"title\" in cols:\n",
    "        df = df.with_columns(pl.col(\"title\").cast(pl.Utf8).fill_null(\"\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(\"\").alias(\"title\"))\n",
    "\n",
    "    if \"url\" in cols:\n",
    "        df = df.with_columns(pl.col(\"url\").cast(pl.Utf8).fill_null(\"\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(\"\").alias(\"url\"))\n",
    "\n",
    "    if \"text\" in cols:\n",
    "        df = df.with_columns(pl.col(\"text\").cast(pl.Utf8).fill_null(\"\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(\"\").alias(\"text\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_jsonl_to_df(path, max_rows=ARTICLES_NUMBER)\n",
    "print(f\"âœ… Loaded {len(df)} articles into a single Polars DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6d7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_parquet(\"articles_fr_withLinks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9476964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/x7v4d0vs261d025y0qz66bdh0000gn/T/ipykernel_1569/534072922.py:4: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "vector_size = model.get_sentence_embedding_dimension()\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    vectors_config=VectorParams(size=vector_size, distance=\"Cosine\"),\n",
    ")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c93f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting parallel vectorization of 50000 articles...\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 4: Ingest articles into Qdrant with payload (Robust & Parallelized)\n",
    "# ----------------------------\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from qdrant_client.models import PointStruct\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the number of articles to process. Set to None to process all articles.\n",
    "ARTICLES_TO_PROCESS = 50000 # For example, process the first 10,000 articles\n",
    "\n",
    "chunk_size_words = 256\n",
    "# Batch size for upserting points to Qdrant\n",
    "points_batch_size = 512 \n",
    "\n",
    "# --- Get model embedding dimension ---\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "# --- Core vectorization function (to be run in parallel) ---\n",
    "def get_article_vector(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Chunks text, encodes chunks, and returns the mean-pooled vector.\n",
    "    This function is designed to be applied to each row by Polars' map_elements.\n",
    "    \"\"\"\n",
    "    # 1. Chunk the text\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+chunk_size_words]) for i in range(0, len(words), chunk_size_words)]\n",
    "    \n",
    "    # Handle empty texts by encoding a single empty string\n",
    "    if not chunks:\n",
    "        chunks = [\"\"]\n",
    "        \n",
    "    # 2. Add the E5 model's required prefix\n",
    "    prefixed_chunks = [f\"passage: {chunk}\" for chunk in chunks]\n",
    "    \n",
    "    # 3. Encode the chunks for this single article\n",
    "    vectors = model.encode(prefixed_chunks, normalize_embeddings=True) # returns float32\n",
    "    \n",
    "    # 4. Mean-pool the vectors and ensure correct dtype\n",
    "    if vectors.shape[0] > 0:\n",
    "        return np.mean(vectors, axis=0).astype(np.float32)\n",
    "    else:\n",
    "        return np.zeros(embedding_dim, dtype=np.float32)\n",
    "\n",
    "# --- Main processing ---\n",
    "# Slice the DataFrame to process only the specified number of articles\n",
    "if ARTICLES_TO_PROCESS is not None:\n",
    "    df_to_process = df.slice(0, ARTICLES_TO_PROCESS)\n",
    "else:\n",
    "    df_to_process = df\n",
    "\n",
    "print(f\"ðŸš€ Starting parallel vectorization of {df_to_process.height} articles...\")\n",
    "\n",
    "# Use `map_elements` to apply our function to each text in parallel.\n",
    "df_with_vectors = df_to_process.with_columns(\n",
    "    vector=pl.col(\"text\").map_elements(\n",
    "        get_article_vector,\n",
    "        # Ensure the return type matches the function's output exactly\n",
    "        return_dtype=pl.Array(pl.Float32, embedding_dim),\n",
    "        strategy=\"thread_local\" # Use multiple threads for parallel execution\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"âœ… Vectorization complete. Now creating and upserting points...\")\n",
    "\n",
    "# --- Create and upsert points in batches (Optimized) ---\n",
    "points = []\n",
    "total_rows = df_with_vectors.height\n",
    "\n",
    "# Extract columns into Python lists for fast iteration\n",
    "ids = df_with_vectors.get_column(\"id\").to_list()\n",
    "vectors_list = df_with_vectors.get_column(\"vector\").to_list()\n",
    "payloads = df_with_vectors.select([\"id\", \"title\", \"url\"]).to_dicts()\n",
    "\n",
    "for i in tqdm(range(total_rows), desc=\"Upserting to Qdrant\"):\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=int(ids[i]),\n",
    "            vector=vectors_list[i],\n",
    "            payload=payloads[i]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Upsert in batches to Qdrant for network efficiency\n",
    "    if len(points) >= points_batch_size:\n",
    "        client.upsert(collection_name=\"wikipedia_fr\", points=points, wait=False)\n",
    "        points.clear()\n",
    "\n",
    "# --- Final flush for any remaining points ---\n",
    "if points:\n",
    "    client.upsert(collection_name=\"wikipedia_fr\", points=points, wait=True)\n",
    "    points.clear()\n",
    "    \n",
    "\n",
    "# Clean up memory\n",
    "gc.collect()\n",
    "\n",
    "print(f\"âœ… Ingestion complete. Upserted embeddings for {total_rows} articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b36e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top matches for 'Math':\n",
      "â€¢ AlgÃ¨bre de Boole (logique) (score=0.804)\n",
      "â€¢ AlgÃ¨bre gÃ©nÃ©rale (score=0.787)\n",
      "â€¢ Algorithmique (score=0.785)\n",
      "â€¢ AlgÃ¨bre linÃ©aire (score=0.785)\n",
      "â€¢ Algorithme (score=0.781)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18505/433745059.py:14: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Query similar articles\n",
    "# ----------------------------\n",
    "query_title = \"Math\"\n",
    "\n",
    "query_row = df[df[\"title\"].str.lower() == query_title.lower()]\n",
    "if not query_row.empty:\n",
    "    query_text = query_row.iloc[0][\"text\"]\n",
    "else:\n",
    "    query_text = query_title\n",
    "\n",
    "query_vector = model.encode([query_text], normalize_embeddings=True)[0]\n",
    "\n",
    "results = client.search(\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    query_vector=query_vector,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ” Top matches for '{query_title}':\")\n",
    "for r in results:\n",
    "    print(f\"â€¢ {r.payload['title']} (score={r.score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af26d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top matches for 'Math':\n",
      "â€¢ AlgÃ¨bre de Boole (logique) (score=0.818)\n",
      "â€¢ AlgÃ¨bre linÃ©aire (score=0.804)\n",
      "â€¢ AbrÃ©viations en informatique H (score=0.804)\n",
      "â€¢ Aichi (score=0.804)\n",
      "â€¢ AndrÃ© Marie AmpÃ¨re (score=0.804)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18505/1095570288.py:25: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Query similar articles\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "import numpy as np\n",
    "\n",
    "def build_query_vector_from_article_text(text, model, chunk_size_words=256):\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+chunk_size_words]) for i in range(0, len(words), chunk_size_words)]\n",
    "    prefixed = [f\"query: {chunk}\" for chunk in chunks]\n",
    "    vecs = model.encode(prefixed, normalize_embeddings=True)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "query_title = \"Math\"\n",
    "query_row = df[df[\"title\"].str.lower() == query_title.lower()]\n",
    "\n",
    "if not query_row.empty:\n",
    "    # Use whole article text (chunked) for the query vector\n",
    "    query_text = query_row.iloc[0][\"text\"]\n",
    "    query_vector = build_query_vector_from_article_text(query_text, model)\n",
    "else:\n",
    "    # Fallback: use the title as plain query\n",
    "    query_vector = model.encode([f\"query: {query_title}\"], normalize_embeddings=True)[0]\n",
    "\n",
    "results = client.search(\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    query_vector=query_vector,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ” Top matches for '{query_title}':\")\n",
    "for r in results:\n",
    "    print(f\"â€¢ {r.payload['title']} (score={r.score:.3f})\")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb8233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top matches for 'Math':\n",
      "â€¢ AlgÃ¨bre de Boole (logique) (score=0.818)\n",
      "â€¢ AlgÃ¨bre linÃ©aire (score=0.804)\n",
      "â€¢ AbrÃ©viations en informatique H (score=0.804)\n",
      "â€¢ Aichi (score=0.804)\n",
      "â€¢ AndrÃ© Marie AmpÃ¨re (score=0.804)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18505/340894359.py:21: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Query similar articles\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "from qdrant_client.http import models as rest\n",
    "\n",
    "query_title = \"Math\"\n",
    "query_row = df[df[\"title\"].str.lower() == query_title.lower()]\n",
    "\n",
    "if not query_row.empty:\n",
    "    # Assume point id in Qdrant equals the article 'id' stored in df\n",
    "    article_id = int(query_row.iloc[0][\"id\"])\n",
    "    results = client.recommend(\n",
    "        collection_name=\"wikipedia_fr\",\n",
    "        positive=[article_id],\n",
    "        limit=5\n",
    "    )\n",
    "else:\n",
    "    # Fallback to text-based search using query prefix\n",
    "    query_vector = model.encode([f\"query: {query_title}\"], normalize_embeddings=True)[0]\n",
    "    results = client.search(\n",
    "        collection_name=\"wikipedia_fr\",\n",
    "        query_vector=query_vector,\n",
    "        limit=5\n",
    "    )\n",
    "\n",
    "print(f\"\\nðŸ” Top matches for '{query_title}':\")\n",
    "for r in results:\n",
    "    print(f\"â€¢ {r.payload['title']} (score={r.score:.3f})\")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Similar articles for: AlgÃ¨bre linÃ©aire (id=7)\n",
      "  â€¢ AlgÃ¨bre gÃ©nÃ©rale (id=9, score=0.962)\n",
      "  â€¢ Algorithmique (id=10, score=0.958)\n",
      "  â€¢ Astronomie (id=64, score=0.953)\n",
      "  â€¢ AlgÃ¨bre de Boole (logique) (id=24, score=0.950)\n",
      "  â€¢ Algorithme (id=19, score=0.945)\n",
      "  â€¢ AntiquitÃ© (id=123, score=0.936)\n",
      "  â€¢ Assistant personnel (id=124, score=0.936)\n",
      "  â€¢ Anthropologie (id=152, score=0.935)\n",
      "  â€¢ Antoine de Saint-ExupÃ©ry (id=97, score=0.928)\n",
      "  â€¢ Asie (id=159, score=0.928)\n",
      "\n",
      "ðŸ”Ž Similar articles for: AlgÃ¨bre gÃ©nÃ©rale (id=9)\n",
      "  â€¢ AlgÃ¨bre linÃ©aire (id=7, score=0.962)\n",
      "  â€¢ Algorithmique (id=10, score=0.946)\n",
      "  â€¢ Astronomie (id=64, score=0.936)\n",
      "  â€¢ Algorithme (id=19, score=0.932)\n",
      "  â€¢ AlgÃ¨bre de Boole (logique) (id=24, score=0.931)\n",
      "  â€¢ Anthropologie (id=152, score=0.919)\n",
      "  â€¢ AntiquitÃ© (id=123, score=0.919)\n",
      "  â€¢ Assistant personnel (id=124, score=0.917)\n",
      "  â€¢ Asie (id=159, score=0.908)\n",
      "  â€¢ Apple (id=63, score=0.908)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18505/2584421462.py:46: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  return client.recommend(collection_name=collection_name, positive=[point_id], limit=k)\n",
      "/tmp/ipykernel_18505/2584421462.py:46: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  return client.recommend(collection_name=collection_name, positive=[point_id], limit=k)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Article-to-article similarity by full content\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "import numpy as np\n",
    "from qdrant_client.http import models as rest\n",
    "\n",
    "def encode_article_text(text, model, chunk_size_words=256):\n",
    "    \"\"\"Encode full article by chunking, using E5 'passage:' prefix and mean-pooling.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+chunk_size_words]) for i in range(0, len(words), chunk_size_words)]\n",
    "    prefixed = [f\"passage: {chunk}\" for chunk in chunks]\n",
    "    vecs = model.encode(prefixed, normalize_embeddings=True)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def find_point_id_by_payload_id(client, collection_name, article_id):\n",
    "    \"\"\"Locate the stored point by payload 'id'.\"\"\"\n",
    "    points, next_page = client.scroll(\n",
    "        collection_name=collection_name,\n",
    "        scroll_filter=rest.Filter(\n",
    "            must=[rest.FieldCondition(key=\"id\", match=rest.MatchValue(value=str(article_id)))]\n",
    "        ),\n",
    "        limit=1,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    if points:\n",
    "        return points[0].id\n",
    "    # Also try numeric payload ids, if stored as int\n",
    "    points, next_page = client.scroll(\n",
    "        collection_name=collection_name,\n",
    "        scroll_filter=rest.Filter(\n",
    "            must=[rest.FieldCondition(key=\"id\", match=rest.MatchValue(value=int(article_id)))]\n",
    "        ),\n",
    "        limit=1,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    return points[0].id if points else None\n",
    "\n",
    "def get_similar_articles_by_id(client, collection_name, article_id, k=10):\n",
    "    \"\"\"Nearest neighbors via Qdrant recommend, using the stored vector.\"\"\"\n",
    "    point_id = find_point_id_by_payload_id(client, collection_name, article_id)\n",
    "    if point_id is None:\n",
    "        return None\n",
    "    return client.recommend(collection_name=collection_name, positive=[point_id], limit=k)\n",
    "\n",
    "def get_similar_articles_by_text(client, collection_name, text, model, k=10):\n",
    "    \"\"\"Fallback: nearest neighbors by encoding full article text.\"\"\"\n",
    "    vec = encode_article_text(text, model)\n",
    "    return client.search(collection_name=collection_name, query_vector=vec, limit=k)\n",
    "\n",
    "def get_title_by_id(article_id: int) -> str:\n",
    "    # Try DataFrame first\n",
    "    row = df[df[\"id\"].astype(str) == str(article_id)]\n",
    "    if not row.empty and pd.notna(row.iloc[0][\"title\"]):\n",
    "        return str(row.iloc[0][\"title\"])\n",
    "    # Fallback: try Qdrant payload\n",
    "    try:\n",
    "        pt = client.retrieve(\n",
    "            collection_name=collection_name,\n",
    "            ids=[int(article_id)],\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        if pt and pt[0].payload and pt[0].payload.get(\"title\"):\n",
    "            return str(pt[0].payload[\"title\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"(unknown title)\"\n",
    "\n",
    "collection_name = \"wikipedia_fr\"\n",
    "\n",
    "# Example: article-to-article neighbors for two articles\n",
    "for query_article_id in [7, 9]:\n",
    "    query_title = get_title_by_id(int(query_article_id))\n",
    "    print(f\"\\nðŸ”Ž Similar articles for: {query_title} (id={query_article_id})\")\n",
    "    results = get_similar_articles_by_id(client, collection_name, query_article_id, k=10)\n",
    "\n",
    "    if results is None:\n",
    "        # Fallback to local encoding if the article vector is not stored in Qdrant\n",
    "        row = df[df[\"id\"].astype(str) == str(query_article_id)]\n",
    "        if row.empty:\n",
    "            print(f\"  â€¢ Article id={query_article_id} not found in DataFrame.\")\n",
    "            continue\n",
    "        text = row.iloc[0][\"text\"]\n",
    "        results = get_similar_articles_by_text(client, collection_name, text, model, k=10)\n",
    "\n",
    "    for r in results:\n",
    "        print(f\"  â€¢ {r.payload.get('title')} (id={r.payload.get('id')}, score={r.score:.3f})\")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816b818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18505/2584421462.py:46: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  return client.recommend(collection_name=collection_name, positive=[point_id], limit=k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported recomputed links to recomputed_similarity_links.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Export recomputed similarity links (top-k neighbors)\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "import csv\n",
    "\n",
    "def iter_all_point_ids(client, collection_name, batch_size=1024):\n",
    "    \"\"\"Yield all point IDs in the collection (without vectors).\"\"\"\n",
    "    next_page = None\n",
    "    while True:\n",
    "        points, next_page = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=batch_size,\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "            offset=next_page\n",
    "        )\n",
    "        if not points:\n",
    "            break\n",
    "        for p in points:\n",
    "            # Prefer payload 'id' if present, else use point id\n",
    "            payload_id = p.payload.get(\"id\")\n",
    "            yield payload_id if payload_id is not None else p.id\n",
    "        if next_page is None:\n",
    "            break\n",
    "\n",
    "def export_similarity_links(client, collection_name, k, out_csv_path):\n",
    "    \"\"\"For each article, record top-k similar neighbors to CSV.\"\"\"\n",
    "    with open(out_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"source_id\", \"target_id\", \"score\"])\n",
    "        count = 0\n",
    "        for source_id in iter_all_point_ids(client, collection_name):\n",
    "            # Resolve to actual point ID if payload_id is used\n",
    "            neighbors = get_similar_articles_by_id(client, collection_name, source_id, k=k)\n",
    "            if neighbors is None:\n",
    "                # Skip if the source article is missing as a stored vector\n",
    "                continue\n",
    "            for r in neighbors:\n",
    "                target_id = r.payload.get(\"id\", r.id)\n",
    "                if str(target_id) == str(source_id):\n",
    "                    continue\n",
    "                writer.writerow([source_id, target_id, f\"{r.score:.6f}\"])\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(f\"Processed {count} articles...\")\n",
    "    print(f\"âœ… Exported recomputed links to {out_csv_path}\")\n",
    "\n",
    "# Generate a 10-NN hyperlink graph (adjust k as needed)\n",
    "export_similarity_links(client, collection_name=\"wikipedia_fr\", k=10, out_csv_path=\"recomputed_similarity_links.csv\")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5081d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-433",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
