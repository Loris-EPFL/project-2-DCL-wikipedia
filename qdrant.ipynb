{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55eaadc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqdrant_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QdrantClient\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mqdrant_client\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PointStruct, VectorParams\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/sentence_transformers/__init__.py:15\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     export_dynamic_quantized_onnx_model,\n\u001b[32m     12\u001b[39m     export_optimized_onnx_model,\n\u001b[32m     13\u001b[39m     export_static_quantized_openvino_model,\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CrossEncoder,\n\u001b[32m     17\u001b[39m     CrossEncoderModelCardData,\n\u001b[32m     18\u001b[39m     CrossEncoderTrainer,\n\u001b[32m     19\u001b[39m     CrossEncoderTrainingArguments,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mLoggingHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/sentence_transformers/cross_encoder/__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mCrossEncoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_card\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautonotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     AutoConfig,\n\u001b[32m     18\u001b[39m     AutoModelForSequenceClassification,\n\u001b[32m     19\u001b[39m     AutoTokenizer,\n\u001b[32m     20\u001b[39m     PretrainedConfig,\n\u001b[32m     21\u001b[39m     PreTrainedModel,\n\u001b[32m     22\u001b[39m     PreTrainedTokenizer,\n\u001b[32m     23\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Union\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     24\u001b[39m     _BaseAutoBackboneClass,\n\u001b[32m     25\u001b[39m     _BaseAutoModelClass,\n\u001b[32m     26\u001b[39m     _LazyAutoMapping,\n\u001b[32m     27\u001b[39m     auto_class_update,\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONFIG_MAPPING_NAMES\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:43\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_auto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, model_type_to_module_name, replace_list_option_in_docstrings\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerationMixin\n\u001b[32m     46\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     48\u001b[39m _T = TypeVar(\u001b[33m\"\u001b[39m\u001b[33m_T\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/transformers/generation/utils.py:43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_deepspeed_zero3_enabled\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfsdp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_fsdp_managed_module\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmasking_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_masks_for_generate\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m isin_mps_friendly\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenization_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExtensionsTrie\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/transformers/masking_utils.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _is_torch_xpu_available = is_torch_xpu_available()\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_greater_or_equal_than_2_6:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_trace_wrapped_higher_order_op\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TransformGetItemToIndex\n\u001b[32m     43\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mand_masks\u001b[39m(*mask_functions: Callable) -> Callable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/torch/_dynamo/__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     aot_compile,\n\u001b[32m     15\u001b[39m     config,\n\u001b[32m     16\u001b[39m     convert_frame,\n\u001b[32m     17\u001b[39m     eval_frame,\n\u001b[32m     18\u001b[39m     functional_export,\n\u001b[32m     19\u001b[39m     resume_execution,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/torch/_dynamo/aot_compile.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprecompile_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrecompileContext\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_frame\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Hooks\n\u001b[32m     19\u001b[39m log = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:57\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CallbackTrigger\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:53\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ObservedException, TensorifyScalarRestartAnalysis\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstructured\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dump_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/torch/_dynamo/exc.py:45\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_file_path_2\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/torch/_dynamo/utils.py:2414\u001b[39m\n\u001b[32m   2389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[32m   2392\u001b[39m common_constant_types: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mtype\u001b[39m] = {\n\u001b[32m   2393\u001b[39m     \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   2394\u001b[39m     \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2411\u001b[39m     torch.cuda._CudaDeviceProperties,\n\u001b[32m   2412\u001b[39m }\n\u001b[32m-> \u001b[39m\u001b[32m2414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_triton_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2415\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\n\u001b[32m   2417\u001b[39m     common_constant_types.add(triton.language.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/torch/utils/_triton.py:9\u001b[39m, in \u001b[36mhas_triton_package\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;129m@functools\u001b[39m.cache\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhas_triton_package\u001b[39m() -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/triton/__init__.py:8\u001b[39m\n\u001b[32m      2\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m3.5.0\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# ---------------------------------------\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Note: import order is significant here.\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# submodules\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mruntime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     autotune,\n\u001b[32m     10\u001b[39m     Config,\n\u001b[32m     11\u001b[39m     heuristics,\n\u001b[32m     12\u001b[39m     JITFunction,\n\u001b[32m     13\u001b[39m     KernelInterface,\n\u001b[32m     14\u001b[39m     reinterpret,\n\u001b[32m     15\u001b[39m     TensorWrapper,\n\u001b[32m     16\u001b[39m     OutOfResources,\n\u001b[32m     17\u001b[39m     InterpreterError,\n\u001b[32m     18\u001b[39m     MockTensor,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mruntime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constexpr_function, jit\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mruntime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_async_compile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AsyncCompileMode, FutureKernel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/triton/runtime/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautotuner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (Autotuner, Config, Heuristics, autotune, heuristics)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RedisRemoteCacheBackend, RemoteCacheBackend\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdriver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m driver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/triton/runtime/autotuner.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cached_property\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Tuple, List, Optional\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m knobs\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KernelInterface, JITFunction\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OutOfResources, PTXASError\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ml/lib/python3.11/site-packages/triton/knobs.py:14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontextlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cast, Any, Callable, Generator, Generic, Optional, Protocol, Type, TypeVar, TypedDict, TYPE_CHECKING, Union\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlibtriton\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m getenv, getenv_bool  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mruntime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CacheManager, RemoteCacheBackend\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import env\n",
    "\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(env.HF_LOGIN_TOKEN)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Connect to Qdrant\n",
    "# ----------------------------\n",
    "# Option 1: In-memory (no Docker)\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# # Option 2: Local Qdrant server\n",
    "# client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Initialize model\n",
    "# ----------------------------\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "model.save(\"models/multilingual-e5-large\") #save model locally (do only once)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1ce311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "model = SentenceTransformer(\"models/multilingual-e5-large\")\n",
    "\n",
    "# client = QdrantClient(\":memory:\")\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326c9a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 4498441 articles into a single Polars DataFrame\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# from pathlib import Path\n",
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "# # ----------------------------\n",
    "# # Step 3: Load first 100 articles from JSON\n",
    "# # ----------------------------\n",
    "# path = Path(\"../wikiextractor/articles_fr_withLinks.json\")\n",
    "# # ----------------------------\n",
    "# # Step 3: Load articles from JSON (all lines into DataFrame)\n",
    "# # ----------------------------\n",
    "# # ... existing code ...\n",
    "# ARTICLES_NUMBER = None  # None = load all; set an int to cap when sampling\n",
    "\n",
    "# def load_jsonl_to_df(path, max_rows=None):\n",
    "#     rows = []\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for i, line in enumerate(f):\n",
    "#             if max_rows is not None and i >= max_rows:\n",
    "#                 break\n",
    "#             try:\n",
    "#                 rows.append(json.loads(line))\n",
    "#             except json.JSONDecodeError:\n",
    "#                 continue\n",
    "#     df = pd.DataFrame(rows)\n",
    "#     # Ensure consistent types and fill missing fields\n",
    "#     if \"id\" in df.columns:\n",
    "#         df[\"id\"] = pd.to_numeric(df[\"id\"], errors=\"coerce\").astype(\"Int64\").astype(int)\n",
    "#     df[\"title\"] = df.get(\"title\", \"\").fillna(\"\")\n",
    "#     df[\"url\"] = df.get(\"url\", \"\").fillna(\"\")\n",
    "#     df[\"text\"] = df.get(\"text\", \"\").fillna(\"\")\n",
    "#     return df\n",
    "\n",
    "# df = load_jsonl_to_df(path, max_rows=ARTICLES_NUMBER)\n",
    "# print(f\"‚úÖ Loaded {len(df)} articles into a single DataFrame\")\n",
    "# # ... existing code ...\n",
    "\n",
    "# # ----------------------------\n",
    "# # Step 3: Create collection\n",
    "# # ----------------------------\n",
    "# vector_size = model.get_sentence_embedding_dimension()\n",
    "\n",
    "# client.recreate_collection(\n",
    "#     collection_name=\"wikipedia_fr\",\n",
    "#     vectors_config=VectorParams(size=vector_size, distance=\"Cosine\"),\n",
    "# )\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Load articles from JSON (Polars)\n",
    "# ----------------------------\n",
    "from pathlib import Path\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "path = Path(\"../wikiextractor/articles_fr_withLinks.json\")\n",
    "\n",
    "ARTICLES_NUMBER = None  # None = load all; set an int to cap when sampling\n",
    "\n",
    "def load_jsonl_to_df(path: Path, max_rows: int | None = None) -> pl.DataFrame:\n",
    "    # If loading the full dataset, use Polars' fast NDJSON reader\n",
    "    if max_rows is None:\n",
    "        try:\n",
    "            df = pl.read_ndjson(str(path))\n",
    "        except Exception:\n",
    "            # Fallback if the NDJSON reader hits malformed lines: manual parse\n",
    "            rows = []\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        rows.append(json.loads(line))\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "            df = pl.DataFrame(rows)\n",
    "    else:\n",
    "        # Limited load: manual parse up to max_rows\n",
    "        rows = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                try:\n",
    "                    rows.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        df = pl.DataFrame(rows)\n",
    "\n",
    "    # Ensure columns exist and normalize types/nulls\n",
    "    cols = set(df.columns)\n",
    "\n",
    "    if \"id\" in cols:\n",
    "        # Cast id to Int64\n",
    "        df = df.with_columns(pl.col(\"id\").cast(pl.Int64))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(None).cast(pl.Int64).alias(\"id\"))\n",
    "\n",
    "    if \"title\" in cols:\n",
    "        df = df.with_columns(pl.col(\"title\").cast(pl.Utf8).fill_null(\"\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(\"\").alias(\"title\"))\n",
    "\n",
    "    if \"url\" in cols:\n",
    "        df = df.with_columns(pl.col(\"url\").cast(pl.Utf8).fill_null(\"\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(\"\").alias(\"url\"))\n",
    "\n",
    "    if \"text\" in cols:\n",
    "        df = df.with_columns(pl.col(\"text\").cast(pl.Utf8).fill_null(\"\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(\"\").alias(\"text\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_jsonl_to_df(path, max_rows=ARTICLES_NUMBER)\n",
    "print(f\"‚úÖ Loaded {len(df)} articles into a single Polars DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6d7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_parquet(\"articles_fr_withLinks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9476964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37438/534072922.py:4: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "vector_size = model.get_sentence_embedding_dimension()\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    vectors_config=VectorParams(size=vector_size, distance=\"Cosine\"),\n",
    ")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c93f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting parallel vectorization of 50000 articles...\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 4: Ingest articles into Qdrant with payload (Robust & Parallelized)\n",
    "# ----------------------------\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from qdrant_client.models import PointStruct\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the number of articles to process. Set to None to process all articles.\n",
    "ARTICLES_TO_PROCESS = 50000 # For example, process the first 10,000 articles\n",
    "\n",
    "chunk_size_words = 256\n",
    "# Batch size for upserting points to Qdrant\n",
    "points_batch_size = 512 \n",
    "\n",
    "# --- Get model embedding dimension ---\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "# --- Core vectorization function (to be run in parallel) ---\n",
    "def get_article_vector(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Chunks text, encodes chunks, and returns the mean-pooled vector.\n",
    "    This function is designed to be applied to each row by Polars' map_elements.\n",
    "    \"\"\"\n",
    "    # 1. Chunk the text\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+chunk_size_words]) for i in range(0, len(words), chunk_size_words)]\n",
    "    \n",
    "    # Handle empty texts by encoding a single empty string\n",
    "    if not chunks:\n",
    "        chunks = [\"\"]\n",
    "        \n",
    "    # 2. Add the E5 model's required prefix\n",
    "    prefixed_chunks = [f\"passage: {chunk}\" for chunk in chunks]\n",
    "    \n",
    "    # 3. Encode the chunks for this single article\n",
    "    vectors = model.encode(prefixed_chunks, normalize_embeddings=True) # returns float32\n",
    "    \n",
    "    # 4. Mean-pool the vectors and ensure correct dtype\n",
    "    if vectors.shape[0] > 0:\n",
    "        return np.mean(vectors, axis=0).astype(np.float32)\n",
    "    else:\n",
    "        return np.zeros(embedding_dim, dtype=np.float32)\n",
    "\n",
    "# --- Main processing ---\n",
    "# Slice the DataFrame to process only the specified number of articles\n",
    "if ARTICLES_TO_PROCESS is not None:\n",
    "    df_to_process = df.slice(0, ARTICLES_TO_PROCESS)\n",
    "else:\n",
    "    df_to_process = df\n",
    "\n",
    "print(f\"üöÄ Starting parallel vectorization of {df_to_process.height} articles...\")\n",
    "\n",
    "# Use `map_elements` to apply our function to each text in parallel.\n",
    "df_with_vectors = df_to_process.with_columns(\n",
    "    vector=pl.col(\"text\").map_elements(\n",
    "        get_article_vector,\n",
    "        # Ensure the return type matches the function's output exactly\n",
    "        return_dtype=pl.Array(pl.Float32, embedding_dim),\n",
    "        strategy=\"thread_local\" # Use multiple threads for parallel execution\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vectorization complete. Now creating and upserting points...\")\n",
    "\n",
    "# --- Create and upsert points in batches (Optimized) ---\n",
    "points = []\n",
    "total_rows = df_with_vectors.height\n",
    "\n",
    "# Extract columns into Python lists for fast iteration\n",
    "ids = df_with_vectors.get_column(\"id\").to_list()\n",
    "vectors_list = df_with_vectors.get_column(\"vector\").to_list()\n",
    "payloads = df_with_vectors.select([\"id\", \"title\", \"url\"]).to_dicts()\n",
    "\n",
    "for i in tqdm(range(total_rows), desc=\"Upserting to Qdrant\"):\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=int(ids[i]),\n",
    "            vector=vectors_list[i],\n",
    "            payload=payloads[i]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Upsert in batches to Qdrant for network efficiency\n",
    "    if len(points) >= points_batch_size:\n",
    "        client.upsert(collection_name=\"wikipedia_fr\", points=points, wait=False)\n",
    "        points.clear()\n",
    "\n",
    "# --- Final flush for any remaining points ---\n",
    "if points:\n",
    "    client.upsert(collection_name=\"wikipedia_fr\", points=points, wait=True)\n",
    "    points.clear()\n",
    "\n",
    "# Clean up memory\n",
    "gc.collect()\n",
    "\n",
    "print(f\"‚úÖ Ingestion complete. Upserted embeddings for {total_rows} articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b36e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top matches for 'Math':\n",
      "‚Ä¢ Alg√®bre de Boole (logique) (score=0.804)\n",
      "‚Ä¢ Alg√®bre g√©n√©rale (score=0.787)\n",
      "‚Ä¢ Algorithmique (score=0.785)\n",
      "‚Ä¢ Alg√®bre lin√©aire (score=0.785)\n",
      "‚Ä¢ Algorithme (score=0.781)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18505/433745059.py:14: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Query similar articles\n",
    "# ----------------------------\n",
    "query_title = \"Math\"\n",
    "\n",
    "query_row = df[df[\"title\"].str.lower() == query_title.lower()]\n",
    "if not query_row.empty:\n",
    "    query_text = query_row.iloc[0][\"text\"]\n",
    "else:\n",
    "    query_text = query_title\n",
    "\n",
    "query_vector = model.encode([query_text], normalize_embeddings=True)[0]\n",
    "\n",
    "results = client.search(\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    query_vector=query_vector,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(f\"\\nüîç Top matches for '{query_title}':\")\n",
    "for r in results:\n",
    "    print(f\"‚Ä¢ {r.payload['title']} (score={r.score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af26d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top matches for 'Math':\n",
      "‚Ä¢ Alg√®bre de Boole (logique) (score=0.818)\n",
      "‚Ä¢ Alg√®bre lin√©aire (score=0.804)\n",
      "‚Ä¢ Abr√©viations en informatique H (score=0.804)\n",
      "‚Ä¢ Aichi (score=0.804)\n",
      "‚Ä¢ Andr√© Marie Amp√®re (score=0.804)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18505/1095570288.py:25: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Query similar articles\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "import numpy as np\n",
    "\n",
    "def build_query_vector_from_article_text(text, model, chunk_size_words=256):\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+chunk_size_words]) for i in range(0, len(words), chunk_size_words)]\n",
    "    prefixed = [f\"query: {chunk}\" for chunk in chunks]\n",
    "    vecs = model.encode(prefixed, normalize_embeddings=True)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "query_title = \"Math\"\n",
    "query_row = df[df[\"title\"].str.lower() == query_title.lower()]\n",
    "\n",
    "if not query_row.empty:\n",
    "    # Use whole article text (chunked) for the query vector\n",
    "    query_text = query_row.iloc[0][\"text\"]\n",
    "    query_vector = build_query_vector_from_article_text(query_text, model)\n",
    "else:\n",
    "    # Fallback: use the title as plain query\n",
    "    query_vector = model.encode([f\"query: {query_title}\"], normalize_embeddings=True)[0]\n",
    "\n",
    "results = client.search(\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    query_vector=query_vector,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(f\"\\nüîç Top matches for '{query_title}':\")\n",
    "for r in results:\n",
    "    print(f\"‚Ä¢ {r.payload['title']} (score={r.score:.3f})\")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb8233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top matches for 'Math':\n",
      "‚Ä¢ Alg√®bre de Boole (logique) (score=0.818)\n",
      "‚Ä¢ Alg√®bre lin√©aire (score=0.804)\n",
      "‚Ä¢ Abr√©viations en informatique H (score=0.804)\n",
      "‚Ä¢ Aichi (score=0.804)\n",
      "‚Ä¢ Andr√© Marie Amp√®re (score=0.804)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18505/340894359.py:21: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Query similar articles\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "from qdrant_client.http import models as rest\n",
    "\n",
    "query_title = \"Math\"\n",
    "query_row = df[df[\"title\"].str.lower() == query_title.lower()]\n",
    "\n",
    "if not query_row.empty:\n",
    "    # Assume point id in Qdrant equals the article 'id' stored in df\n",
    "    article_id = int(query_row.iloc[0][\"id\"])\n",
    "    results = client.recommend(\n",
    "        collection_name=\"wikipedia_fr\",\n",
    "        positive=[article_id],\n",
    "        limit=5\n",
    "    )\n",
    "else:\n",
    "    # Fallback to text-based search using query prefix\n",
    "    query_vector = model.encode([f\"query: {query_title}\"], normalize_embeddings=True)[0]\n",
    "    results = client.search(\n",
    "        collection_name=\"wikipedia_fr\",\n",
    "        query_vector=query_vector,\n",
    "        limit=5\n",
    "    )\n",
    "\n",
    "print(f\"\\nüîç Top matches for '{query_title}':\")\n",
    "for r in results:\n",
    "    print(f\"‚Ä¢ {r.payload['title']} (score={r.score:.3f})\")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Similar articles for: Alg√®bre lin√©aire (id=7)\n",
      "  ‚Ä¢ Alg√®bre g√©n√©rale (id=9, score=0.962)\n",
      "  ‚Ä¢ Algorithmique (id=10, score=0.958)\n",
      "  ‚Ä¢ Astronomie (id=64, score=0.953)\n",
      "  ‚Ä¢ Alg√®bre de Boole (logique) (id=24, score=0.950)\n",
      "  ‚Ä¢ Algorithme (id=19, score=0.945)\n",
      "  ‚Ä¢ Antiquit√© (id=123, score=0.936)\n",
      "  ‚Ä¢ Assistant personnel (id=124, score=0.936)\n",
      "  ‚Ä¢ Anthropologie (id=152, score=0.935)\n",
      "  ‚Ä¢ Antoine de Saint-Exup√©ry (id=97, score=0.928)\n",
      "  ‚Ä¢ Asie (id=159, score=0.928)\n",
      "\n",
      "üîé Similar articles for: Alg√®bre g√©n√©rale (id=9)\n",
      "  ‚Ä¢ Alg√®bre lin√©aire (id=7, score=0.962)\n",
      "  ‚Ä¢ Algorithmique (id=10, score=0.946)\n",
      "  ‚Ä¢ Astronomie (id=64, score=0.936)\n",
      "  ‚Ä¢ Algorithme (id=19, score=0.932)\n",
      "  ‚Ä¢ Alg√®bre de Boole (logique) (id=24, score=0.931)\n",
      "  ‚Ä¢ Anthropologie (id=152, score=0.919)\n",
      "  ‚Ä¢ Antiquit√© (id=123, score=0.919)\n",
      "  ‚Ä¢ Assistant personnel (id=124, score=0.917)\n",
      "  ‚Ä¢ Asie (id=159, score=0.908)\n",
      "  ‚Ä¢ Apple (id=63, score=0.908)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18505/2584421462.py:46: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  return client.recommend(collection_name=collection_name, positive=[point_id], limit=k)\n",
      "/tmp/ipykernel_18505/2584421462.py:46: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  return client.recommend(collection_name=collection_name, positive=[point_id], limit=k)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Article-to-article similarity by full content\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "import numpy as np\n",
    "from qdrant_client.http import models as rest\n",
    "\n",
    "def encode_article_text(text, model, chunk_size_words=256):\n",
    "    \"\"\"Encode full article by chunking, using E5 'passage:' prefix and mean-pooling.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+chunk_size_words]) for i in range(0, len(words), chunk_size_words)]\n",
    "    prefixed = [f\"passage: {chunk}\" for chunk in chunks]\n",
    "    vecs = model.encode(prefixed, normalize_embeddings=True)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def find_point_id_by_payload_id(client, collection_name, article_id):\n",
    "    \"\"\"Locate the stored point by payload 'id'.\"\"\"\n",
    "    points, next_page = client.scroll(\n",
    "        collection_name=collection_name,\n",
    "        scroll_filter=rest.Filter(\n",
    "            must=[rest.FieldCondition(key=\"id\", match=rest.MatchValue(value=str(article_id)))]\n",
    "        ),\n",
    "        limit=1,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    if points:\n",
    "        return points[0].id\n",
    "    # Also try numeric payload ids, if stored as int\n",
    "    points, next_page = client.scroll(\n",
    "        collection_name=collection_name,\n",
    "        scroll_filter=rest.Filter(\n",
    "            must=[rest.FieldCondition(key=\"id\", match=rest.MatchValue(value=int(article_id)))]\n",
    "        ),\n",
    "        limit=1,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    return points[0].id if points else None\n",
    "\n",
    "def get_similar_articles_by_id(client, collection_name, article_id, k=10):\n",
    "    \"\"\"Nearest neighbors via Qdrant recommend, using the stored vector.\"\"\"\n",
    "    point_id = find_point_id_by_payload_id(client, collection_name, article_id)\n",
    "    if point_id is None:\n",
    "        return None\n",
    "    return client.recommend(collection_name=collection_name, positive=[point_id], limit=k)\n",
    "\n",
    "def get_similar_articles_by_text(client, collection_name, text, model, k=10):\n",
    "    \"\"\"Fallback: nearest neighbors by encoding full article text.\"\"\"\n",
    "    vec = encode_article_text(text, model)\n",
    "    return client.search(collection_name=collection_name, query_vector=vec, limit=k)\n",
    "\n",
    "def get_title_by_id(article_id: int) -> str:\n",
    "    # Try DataFrame first\n",
    "    row = df[df[\"id\"].astype(str) == str(article_id)]\n",
    "    if not row.empty and pd.notna(row.iloc[0][\"title\"]):\n",
    "        return str(row.iloc[0][\"title\"])\n",
    "    # Fallback: try Qdrant payload\n",
    "    try:\n",
    "        pt = client.retrieve(\n",
    "            collection_name=collection_name,\n",
    "            ids=[int(article_id)],\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        if pt and pt[0].payload and pt[0].payload.get(\"title\"):\n",
    "            return str(pt[0].payload[\"title\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"(unknown title)\"\n",
    "\n",
    "collection_name = \"wikipedia_fr\"\n",
    "\n",
    "# Example: article-to-article neighbors for two articles\n",
    "for query_article_id in [7, 9]:\n",
    "    query_title = get_title_by_id(int(query_article_id))\n",
    "    print(f\"\\nüîé Similar articles for: {query_title} (id={query_article_id})\")\n",
    "    results = get_similar_articles_by_id(client, collection_name, query_article_id, k=10)\n",
    "\n",
    "    if results is None:\n",
    "        # Fallback to local encoding if the article vector is not stored in Qdrant\n",
    "        row = df[df[\"id\"].astype(str) == str(query_article_id)]\n",
    "        if row.empty:\n",
    "            print(f\"  ‚Ä¢ Article id={query_article_id} not found in DataFrame.\")\n",
    "            continue\n",
    "        text = row.iloc[0][\"text\"]\n",
    "        results = get_similar_articles_by_text(client, collection_name, text, model, k=10)\n",
    "\n",
    "    for r in results:\n",
    "        print(f\"  ‚Ä¢ {r.payload.get('title')} (id={r.payload.get('id')}, score={r.score:.3f})\")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816b818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18505/2584421462.py:46: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  return client.recommend(collection_name=collection_name, positive=[point_id], limit=k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Exported recomputed links to recomputed_similarity_links.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Export recomputed similarity links (top-k neighbors)\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "import csv\n",
    "\n",
    "def iter_all_point_ids(client, collection_name, batch_size=1024):\n",
    "    \"\"\"Yield all point IDs in the collection (without vectors).\"\"\"\n",
    "    next_page = None\n",
    "    while True:\n",
    "        points, next_page = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=batch_size,\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "            offset=next_page\n",
    "        )\n",
    "        if not points:\n",
    "            break\n",
    "        for p in points:\n",
    "            # Prefer payload 'id' if present, else use point id\n",
    "            payload_id = p.payload.get(\"id\")\n",
    "            yield payload_id if payload_id is not None else p.id\n",
    "        if next_page is None:\n",
    "            break\n",
    "\n",
    "def export_similarity_links(client, collection_name, k, out_csv_path):\n",
    "    \"\"\"For each article, record top-k similar neighbors to CSV.\"\"\"\n",
    "    with open(out_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"source_id\", \"target_id\", \"score\"])\n",
    "        count = 0\n",
    "        for source_id in iter_all_point_ids(client, collection_name):\n",
    "            # Resolve to actual point ID if payload_id is used\n",
    "            neighbors = get_similar_articles_by_id(client, collection_name, source_id, k=k)\n",
    "            if neighbors is None:\n",
    "                # Skip if the source article is missing as a stored vector\n",
    "                continue\n",
    "            for r in neighbors:\n",
    "                target_id = r.payload.get(\"id\", r.id)\n",
    "                if str(target_id) == str(source_id):\n",
    "                    continue\n",
    "                writer.writerow([source_id, target_id, f\"{r.score:.6f}\"])\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(f\"Processed {count} articles...\")\n",
    "    print(f\"‚úÖ Exported recomputed links to {out_csv_path}\")\n",
    "\n",
    "# Generate a 10-NN hyperlink graph (adjust k as needed)\n",
    "export_similarity_links(client, collection_name=\"wikipedia_fr\", k=10, out_csv_path=\"recomputed_similarity_links.csv\")\n",
    "# ... existing code ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
