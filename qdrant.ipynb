{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55eaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import env\n",
    "\n",
    "\n",
    "#RUN this if you are not using the docker image (use it cause takes too much memory otherwise)\n",
    "from huggingface_hub import login\n",
    "login(env.HF_LOGIN_TOKEN)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Connect to Qdrant\n",
    "# ----------------------------\n",
    "# Option 1: In-memory (no Docker)\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# # Option 2: Local Qdrant server\n",
    "# client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 2: Initialize model\n",
    "# ----------------------------\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "model.save(\"models/multilingual-e5-large\") #save model locally (do only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ce311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "#If you already saved the model locally and are using docker\n",
    "client = QdrantClient(host=\"localhost\", port=6333, prefer_grpc=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"models/multilingual-e5-large\", device=device)\n",
    "\n",
    "# Run docker container before\n",
    "# docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v /home/Loris/EPFL/MA3/ML/project2/project2Rag/qdrant_storage:/qdrant/storage qdrant/qdrant:latest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326c9a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 4498441 articles into a single Polars DataFrame\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Step 3: Load articles from JSON (Polars)\n",
    "# ----------------------------\n",
    "from pathlib import Path\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "path = Path(\"../wikiextractor/articles_fr_withLinks.json\")\n",
    "\n",
    "ARTICLES_NUMBER = None  # None = load all; set an int to cap when sampling\n",
    "\n",
    "def load_jsonl_to_df(path: Path, max_rows: int | None = None) -> pl.DataFrame:\n",
    "    # If loading the full dataset, use Polars' fast NDJSON reader\n",
    "    if max_rows is None:\n",
    "        try:\n",
    "            df = pl.read_ndjson(str(path))\n",
    "        except Exception:\n",
    "            # Fallback if the NDJSON reader hits malformed lines: manual parse\n",
    "            rows = []\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        rows.append(json.loads(line))\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "            df = pl.DataFrame(rows)\n",
    "    else:\n",
    "        # Limited load: manual parse up to max_rows\n",
    "        rows = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                try:\n",
    "                    rows.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        df = pl.DataFrame(rows)\n",
    "\n",
    "    # Ensure columns exist and normalize types/nulls\n",
    "    cols = set(df.columns)\n",
    "\n",
    "    if \"id\" in cols:\n",
    "        # Cast id to Int64\n",
    "        df = df.with_columns(pl.col(\"id\").cast(pl.Int64))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(None).cast(pl.Int64).alias(\"id\"))\n",
    "\n",
    "    if \"title\" in cols:\n",
    "        df = df.with_columns(pl.col(\"title\").cast(pl.Utf8).fill_null(\"\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(\"\").alias(\"title\"))\n",
    "\n",
    "    if \"url\" in cols:\n",
    "        df = df.with_columns(pl.col(\"url\").cast(pl.Utf8).fill_null(\"\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(\"\").alias(\"url\"))\n",
    "\n",
    "    if \"text\" in cols:\n",
    "        df = df.with_columns(pl.col(\"text\").cast(pl.Utf8).fill_null(\"\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(\"\").alias(\"text\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_jsonl_to_df(path, max_rows=ARTICLES_NUMBER)\n",
    "print(f\"‚úÖ Loaded {len(df)} articles into a single Polars DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write polars df to disk if needed\n",
    "df.write_parquet(\"articles_fr_withLinks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa96c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'full_url': 'https://fr.wikipedia.org/wiki/Moulins%20%28Allier%29', 'start_idx': 25, 'anchor': 'Moulins', 'href_raw': 'Moulins%20%28Allier%29', 'href_decoded': 'Moulins (Allier)'}, {'full_url': 'https://fr.wikipedia.org/wiki/Allier%20%28d%C3%A9partement%29', 'start_idx': 83, 'anchor': 'Allier', 'href_raw': 'Allier%20%28d%C3%A9partement%29', 'href_decoded': 'Allier (d√©partement)'}, {'full_url': 'https://fr.wikipedia.org/wiki/Ch%C3%A2teaumeillant', 'start_idx': 162, 'anchor': 'Ch√¢teaumeillant', 'href_raw': 'Ch%C3%A2teaumeillant', 'href_decoded': 'Ch√¢teaumeillant'}, {'full_url': 'https://fr.wikipedia.org/wiki/Cher%20%28d%C3%A9partement%29', 'start_idx': 226, 'anchor': 'Cher', 'href_raw': 'Cher%20%28d%C3%A9partement%29', 'href_decoded': 'Cher (d√©partement)'}, {'full_url': 'https://fr.wikipedia.org/wiki/Philologie', 'start_idx': 296, 'anchor': 'philologue', 'href_raw': 'Philologie', 'href_decoded': 'Philologie'}, {'full_url': 'https://fr.wikipedia.org/wiki/liste%20de%20linguistes', 'start_idx': 367, 'anchor': 'linguiste', 'href_raw': 'liste%20de%20linguistes', 'href_decoded': 'liste de linguistes'}, {'full_url': 'https://fr.wikipedia.org/wiki/Allier%20%28d%C3%A9partement%29', 'start_idx': 549, 'anchor': 'bourbonnaise', 'href_raw': 'Allier%20%28d%C3%A9partement%29', 'href_decoded': 'Allier (d√©partement)'}, {'full_url': 'https://fr.wikipedia.org/wiki/notaire', 'start_idx': 631, 'anchor': 'notaire', 'href_raw': 'notaire', 'href_decoded': 'notaire'}, {'full_url': 'https://fr.wikipedia.org/wiki/Ch%C3%A2teaumeillant', 'start_idx': 676, 'anchor': 'Ch√¢teaumeillant', 'href_raw': 'Ch%C3%A2teaumeillant', 'href_decoded': 'Ch√¢teaumeillant'}, {'full_url': 'https://fr.wikipedia.org/wiki/Cher%20%28d%C3%A9partement%29', 'start_idx': 740, 'anchor': 'Cher', 'href_raw': 'Cher%20%28d%C3%A9partement%29', 'href_decoded': 'Cher (d√©partement)'}, {'full_url': 'https://fr.wikipedia.org/wiki/Moulins%20%28Allier%29', 'start_idx': 813, 'anchor': 'Moulins', 'href_raw': 'Moulins%20%28Allier%29', 'href_decoded': 'Moulins (Allier)'}, {'full_url': 'https://fr.wikipedia.org/wiki/lyc%C3%A9e%20Th%C3%A9odore-de-Banville', 'start_idx': 967, 'anchor': 'lyc√©e', 'href_raw': 'lyc%C3%A9e%20Th%C3%A9odore-de-Banville', 'href_decoded': 'lyc√©e Th√©odore-de-Banville'}, {'full_url': 'https://fr.wikipedia.org/wiki/facult%C3%A9%20des%20lettres%20de%20Paris', 'start_idx': 1081, 'anchor': 'facult√© des lettres de Paris', 'href_raw': 'facult%C3%A9%20des%20lettres%20de%20Paris', 'href_decoded': 'facult√© des lettres de Paris'}, {'full_url': 'https://fr.wikipedia.org/wiki/Louis%20Havet', 'start_idx': 1212, 'anchor': 'Louis Havet', 'href_raw': 'Louis%20Havet', 'href_decoded': 'Louis Havet'}, {'full_url': 'https://fr.wikipedia.org/wiki/Michel%20Br%C3%A9al', 'start_idx': 1296, 'anchor': 'Michel Br√©al', 'href_raw': 'Michel%20Br%C3%A9al', 'href_decoded': 'Michel Br√©al'}, {'full_url': 'https://fr.wikipedia.org/wiki/Coll%C3%A8ge%20de%20France', 'start_idx': 1358, 'anchor': 'Coll√®ge de France', 'href_raw': 'Coll%C3%A8ge%20de%20France', 'href_decoded': 'Coll√®ge de France'}, {'full_url': 'https://fr.wikipedia.org/wiki/Ferdinand%20de%20Saussure', 'start_idx': 1435, 'anchor': 'Ferdinand de Saussure', 'href_raw': 'Ferdinand%20de%20Saussure', 'href_decoded': 'Ferdinand de Saussure'}, {'full_url': 'https://fr.wikipedia.org/wiki/%C3%89cole%20pratique%20des%20hautes%20%C3%A9tudes', 'start_idx': 1513, 'anchor': '√âcole pratique des hautes √©tudes', 'href_raw': '%C3%89cole%20pratique%20des%20hautes%20%C3%A9tudes', 'href_decoded': '√âcole pratique des hautes √©tudes'}, {'full_url': 'https://fr.wikipedia.org/wiki/agr%C3%A9gation%20de%20grammaire', 'start_idx': 1651, 'anchor': 'agr√©gation de grammaire', 'href_raw': 'agr%C3%A9gation%20de%20grammaire', 'href_decoded': 'agr√©gation de grammaire'}, {'full_url': 'https://fr.wikipedia.org/wiki/Arm%C3%A9nie', 'start_idx': 1774, 'anchor': 'Arm√©nie', 'href_raw': 'Arm%C3%A9nie', 'href_decoded': 'Arm√©nie'}, {'full_url': 'https://fr.wikipedia.org/wiki/Etchmiadzin', 'start_idx': 1834, 'anchor': 'Etchmiadzin', 'href_raw': 'Etchmiadzin', 'href_decoded': 'Etchmiadzin'}, {'full_url': 'https://fr.wikipedia.org/wiki/grammaire%20compar%C3%A9e', 'start_idx': 2026, 'anchor': 'grammaire compar√©e', 'href_raw': 'grammaire%20compar%C3%A9e', 'href_decoded': 'grammaire compar√©e'}, {'full_url': 'https://fr.wikipedia.org/wiki/langues%20persanes', 'start_idx': 2157, 'anchor': 'langues persanes', 'href_raw': 'langues%20persanes', 'href_decoded': 'langues persanes'}, {'full_url': 'https://fr.wikipedia.org/wiki/Doctorat%20%C3%A8s%20lettres%20%28France%29', 'start_idx': 2258, 'anchor': 'doctorat √®s lettres', 'href_raw': 'Doctorat%20%C3%A8s%20lettres%20%28France%29', 'href_decoded': 'Doctorat √®s lettres (France)'}, {'full_url': 'https://fr.wikipedia.org/wiki/vieux-slave', 'start_idx': 2398, 'anchor': 'vieux-slave', 'href_raw': 'vieux-slave', 'href_decoded': 'vieux-slave'}, {'full_url': 'https://fr.wikipedia.org/wiki/Auguste%20Carri%C3%A8re', 'start_idx': 2494, 'anchor': 'Auguste Carri√®re', 'href_raw': 'Auguste%20Carri%C3%A8re', 'href_decoded': 'Auguste Carri√®re'}, {'full_url': 'https://fr.wikipedia.org/wiki/arm%C3%A9nien', 'start_idx': 2575, 'anchor': 'arm√©nien', 'href_raw': 'arm%C3%A9nien', 'href_decoded': 'arm√©nien'}, {'full_url': 'https://fr.wikipedia.org/wiki/Institut%20national%20des%20langues%20et%20civilisations%20orientales', 'start_idx': 2629, 'anchor': '√âcole des langues orientales', 'href_raw': 'Institut%20national%20des%20langues%20et%20civilisations%20orientales', 'href_decoded': 'Institut national des langues et civilisations orientales'}, {'full_url': 'https://fr.wikipedia.org/wiki/Michel%20Br%C3%A9al', 'start_idx': 2778, 'anchor': 'Michel Br√©al', 'href_raw': 'Michel%20Br%C3%A9al', 'href_decoded': 'Michel Br√©al'}, {'full_url': 'https://fr.wikipedia.org/wiki/Coll%C3%A8ge%20de%20France', 'start_idx': 2882, 'anchor': 'Coll√®ge de France', 'href_raw': 'Coll%C3%A8ge%20de%20France', 'href_decoded': 'Coll√®ge de France'}, {'full_url': 'https://fr.wikipedia.org/wiki/langues%20indo-europ%C3%A9ennes', 'start_idx': 3014, 'anchor': 'langues indo-europ√©ennes', 'href_raw': 'langues%20indo-europ%C3%A9ennes', 'href_decoded': 'langues indo-europ√©ennes'}, {'full_url': 'https://fr.wikipedia.org/wiki/%C3%89cole%20pratique%20des%20hautes%20%C3%A9tudes', 'start_idx': 3255, 'anchor': '√âcole pratique des hautes √©tudes', 'href_raw': '%C3%89cole%20pratique%20des%20hautes%20%C3%A9tudes', 'href_decoded': '√âcole pratique des hautes √©tudes'}, {'full_url': 'https://fr.wikipedia.org/wiki/Soci%C3%A9t%C3%A9%20de%20linguistique%20de%20Paris', 'start_idx': 3383, 'anchor': 'Soci√©t√© de linguistique de Paris', 'href_raw': 'Soci%C3%A9t%C3%A9%20de%20linguistique%20de%20Paris', 'href_decoded': 'Soci√©t√© de linguistique de Paris'}, {'full_url': 'https://fr.wikipedia.org/wiki/Acad%C3%A9mie%20des%20inscriptions%20et%20belles-lettres', 'start_idx': 3509, 'anchor': 'Acad√©mie des inscriptions et belles-lettres', 'href_raw': 'Acad%C3%A9mie%20des%20inscriptions%20et%20belles-lettres', 'href_decoded': 'Acad√©mie des inscriptions et belles-lettres'}, {'full_url': 'https://fr.wikipedia.org/wiki/Institut%20d%27%C3%A9tudes%20slaves', 'start_idx': 3668, 'anchor': \"Institut d'√©tudes slaves\", 'href_raw': 'Institut%20d%27%C3%A9tudes%20slaves', 'href_decoded': \"Institut d'√©tudes slaves\"}, {'full_url': 'https://fr.wikipedia.org/wiki/%C3%89mile%20Benveniste', 'start_idx': 3845, 'anchor': '√âmile Benveniste', 'href_raw': '%C3%89mile%20Benveniste', 'href_decoded': '√âmile Benveniste'}, {'full_url': 'https://fr.wikipedia.org/wiki/Marcel%20Cohen', 'start_idx': 3913, 'anchor': 'Marcel Cohen', 'href_raw': 'Marcel%20Cohen', 'href_decoded': 'Marcel Cohen'}, {'full_url': 'https://fr.wikipedia.org/wiki/Georges%20Dum%C3%A9zil', 'start_idx': 3968, 'anchor': 'Georges Dum√©zil', 'href_raw': 'Georges%20Dum%C3%A9zil', 'href_decoded': 'Georges Dum√©zil'}, {'full_url': 'https://fr.wikipedia.org/wiki/Lilias%20Homburger', 'start_idx': 4034, 'anchor': 'Lilias Homburger', 'href_raw': 'Lilias%20Homburger', 'href_decoded': 'Lilias Homburger'}, {'full_url': 'https://fr.wikipedia.org/wiki/Andr%C3%A9%20Martinet', 'start_idx': 4097, 'anchor': 'Andr√© Martinet', 'href_raw': 'Andr%C3%A9%20Martinet', 'href_decoded': 'Andr√© Martinet'}, {'full_url': 'https://fr.wikipedia.org/wiki/Aur%C3%A9lien%20Sauvageot', 'start_idx': 4161, 'anchor': 'Aur√©lien Sauvageot', 'href_raw': 'Aur%C3%A9lien%20Sauvageot', 'href_decoded': 'Aur√©lien Sauvageot'}, {'full_url': 'https://fr.wikipedia.org/wiki/Lucien%20Tesni%C3%A8re', 'start_idx': 4233, 'anchor': 'Lucien Tesni√®re', 'href_raw': 'Lucien%20Tesni%C3%A8re', 'href_decoded': 'Lucien Tesni√®re'}, {'full_url': 'https://fr.wikipedia.org/wiki/Japonisation', 'start_idx': 4302, 'anchor': 'japonisant', 'href_raw': 'Japonisation', 'href_decoded': 'Japonisation'}, {'full_url': 'https://fr.wikipedia.org/wiki/Charles%20Haguenauer', 'start_idx': 4352, 'anchor': 'Charles Haguenauer', 'href_raw': 'Charles%20Haguenauer', 'href_decoded': 'Charles Haguenauer'}, {'full_url': 'https://fr.wikipedia.org/wiki/Joseph%20Vendryes', 'start_idx': 4421, 'anchor': 'Joseph Vendryes', 'href_raw': 'Joseph%20Vendryes', 'href_decoded': 'Joseph Vendryes'}, {'full_url': 'https://fr.wikipedia.org/wiki/Jean%20Paulhan', 'start_idx': 4525, 'anchor': 'Jean Paulhan', 'href_raw': 'Jean%20Paulhan', 'href_decoded': 'Jean Paulhan'}, {'full_url': 'https://fr.wikipedia.org/wiki/Gustave%20Guillaume', 'start_idx': 4636, 'anchor': 'Gustave Guillaume', 'href_raw': 'Gustave%20Guillaume', 'href_decoded': 'Gustave Guillaume'}, {'full_url': 'https://fr.wikipedia.org/wiki/grammaticalisation', 'start_idx': 4827, 'anchor': 'grammaticalisation', 'href_raw': 'grammaticalisation', 'href_decoded': 'grammaticalisation'}, {'full_url': 'https://fr.wikipedia.org/wiki/Linguistique', 'start_idx': 4901, 'anchor': 'linguiste', 'href_raw': 'Linguistique', 'href_decoded': 'Linguistique'}, {'full_url': 'https://fr.wikipedia.org/wiki/Walter%20Porzig', 'start_idx': 4959, 'anchor': 'Walter Porzig', 'href_raw': 'Walter%20Porzig', 'href_decoded': 'Walter Porzig'}, {'full_url': 'https://fr.wikipedia.org/wiki/Dialecte', 'start_idx': 5091, 'anchor': 'dialectes', 'href_raw': 'Dialecte', 'href_decoded': 'Dialecte'}, {'full_url': 'https://fr.wikipedia.org/wiki/Variation%20linguistique', 'start_idx': 5213, 'anchor': 'variation diatopique', 'href_raw': 'Variation%20linguistique', 'href_decoded': 'Variation linguistique'}, {'full_url': 'https://fr.wikipedia.org/wiki/sociolinguistique', 'start_idx': 5312, 'anchor': 'sociolinguistique', 'href_raw': 'sociolinguistique', 'href_decoded': 'sociolinguistique'}, {'full_url': 'https://fr.wikipedia.org/wiki/Cours%20de%20linguistique%20g%C3%A9n%C3%A9rale', 'start_idx': 5458, 'anchor': 'Cours de linguistique g√©n√©rale', 'href_raw': 'Cours%20de%20linguistique%20g%C3%A9n%C3%A9rale', 'href_decoded': 'Cours de linguistique g√©n√©rale'}, {'full_url': 'https://fr.wikipedia.org/wiki/Ferdinand%20de%20Saussure', 'start_idx': 5565, 'anchor': 'Ferdinand de Saussure', 'href_raw': 'Ferdinand%20de%20Saussure', 'href_decoded': 'Ferdinand de Saussure'}, {'full_url': 'https://fr.wikipedia.org/wiki/Ch%C3%A2teaumeillant', 'start_idx': 5723, 'anchor': 'Ch√¢teaumeillant', 'href_raw': 'Ch%C3%A2teaumeillant', 'href_decoded': 'Ch√¢teaumeillant'}, {'full_url': 'https://fr.wikipedia.org/wiki/Moulins%20%28Allier%29', 'start_idx': 5817, 'anchor': 'Moulins', 'href_raw': 'Moulins%20%28Allier%29', 'href_decoded': 'Moulins (Allier)'}, {'full_url': 'https://fr.wikipedia.org/wiki/Milman%20Parry', 'start_idx': 5965, 'anchor': 'Milman Parry', 'href_raw': 'Milman%20Parry', 'href_decoded': 'Milman Parry'}, {'full_url': 'https://fr.wikipedia.org/wiki/l%27Iliade', 'start_idx': 6118, 'anchor': \"l'Iliade\", 'href_raw': 'l%27Iliade', 'href_decoded': \"l'Iliade\"}, {'full_url': 'https://fr.wikipedia.org/wiki/l%27Iliade', 'start_idx': 6388, 'anchor': \"l'Iliade\", 'href_raw': 'l%27Iliade', 'href_decoded': \"l'Iliade\"}, {'full_url': 'https://fr.wikipedia.org/wiki/Matija%20Murko', 'start_idx': 6523, 'anchor': 'Matija Murko', 'href_raw': 'Matija%20Murko', 'href_decoded': 'Matija Murko'}, {'full_url': 'https://fr.wikipedia.org/wiki/Slov%C3%A9nie', 'start_idx': 6599, 'anchor': 'Slov√©nie', 'href_raw': 'Slov%C3%A9nie', 'href_decoded': 'Slov√©nie'}, {'full_url': 'https://fr.wikipedia.org/wiki/Balkans', 'start_idx': 6717, 'anchor': 'Balkans', 'href_raw': 'Balkans', 'href_decoded': 'Balkans'}, {'full_url': 'https://fr.wikipedia.org/wiki/Bosnie-Herz%C3%A9govine', 'start_idx': 6771, 'anchor': 'Bosnie-Herz√©govine', 'href_raw': 'Bosnie-Herz%C3%A9govine', 'href_decoded': 'Bosnie-Herz√©govine'}, {'full_url': 'https://fr.wikipedia.org/wiki/Albert%20Lord', 'start_idx': 6955, 'anchor': 'Albert Lord', 'href_raw': 'Albert%20Lord', 'href_decoded': 'Albert Lord'}]\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import re\n",
    "import html\n",
    "from urllib.parse import unquote\n",
    "#Extract href links from the articles with links and saves them to a df\n",
    "\n",
    "BASE_URL = \"https://fr.wikipedia.org/wiki/\"\n",
    "ANCHOR_RE = re.compile(r'<a\\s+[^>]*href=\"([^\"]+)\"[^>]*>(.*?)</a>', re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def extract_links_with_pos(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    unescaped = html.unescape(text)\n",
    "    out = []\n",
    "    search_from = 0\n",
    "    for m in ANCHOR_RE.finditer(unescaped):\n",
    "        href_raw = m.group(1)\n",
    "        anchor = html.unescape(m.group(2))\n",
    "        needle = f'&lt;a href=\"{href_raw}\"'\n",
    "        pos = text.find(needle, search_from)\n",
    "        if pos == -1:\n",
    "            pos = text.find(\"&lt;a \", search_from)\n",
    "        if pos == -1:\n",
    "            pos = 0\n",
    "        search_from = pos + 1\n",
    "        out.append({\n",
    "            \"full_url\": BASE_URL + href_raw,\n",
    "            \"start_idx\": pos,\n",
    "            \"anchor\": anchor,\n",
    "            \"href_raw\": href_raw,\n",
    "            \"href_decoded\": unquote(href_raw),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "LINK_ITEM = pl.Struct([\n",
    "    pl.Field(\"full_url\", pl.Utf8),\n",
    "    pl.Field(\"start_idx\", pl.Int64),\n",
    "    pl.Field(\"anchor\", pl.Utf8),\n",
    "    pl.Field(\"href_raw\", pl.Utf8),\n",
    "    pl.Field(\"href_decoded\", pl.Utf8),\n",
    "])\n",
    "LINK_LIST = pl.List(LINK_ITEM)\n",
    "\n",
    "def build_per_article_links(df: pl.DataFrame, max_rows: int | None = None) -> pl.DataFrame:\n",
    "    if max_rows is not None:\n",
    "        df = df.slice(0, min(max_rows, df.height))\n",
    "    idx = (\n",
    "        df.select([\"id\", \"title\", \"text\"])\n",
    "          .with_columns(pl.col(\"text\").cast(pl.Utf8).fill_null(\"\"))\n",
    "          .with_columns(links=pl.col(\"text\").map_elements(extract_links_with_pos, return_dtype=LINK_LIST))\n",
    "          .drop(\"text\")\n",
    "          .with_columns(\n",
    "              links=pl.when(pl.col(\"links\").is_null()).then(pl.lit([]).cast(LINK_LIST)).otherwise(pl.col(\"links\")),\n",
    "              link_count=pl.col(\"links\").list.len()\n",
    "          )\n",
    "    )\n",
    "    return idx\n",
    "\n",
    "# Example usage on your Polars df:\n",
    "per_article = build_per_article_links(df, max_rows=10000)\n",
    "# print(per_article.select([\"id\", \"title\", \"link_count\"]).head(10))\n",
    "\n",
    "# Inspect one article: all links and their positions in a single row\n",
    "article_id = 3\n",
    "row = per_article.filter(pl.col(\"id\") == article_id)\n",
    "links = row.select(\"links\").to_series().to_list()[0] if row.height else []\n",
    "print(links)\n",
    "# for l in links[:20]:\n",
    "#     print(f\"{l['full_url']} - {l['start_idx']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9476964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector size from model\n",
    "vector_size = model.get_sentence_embedding_dimension()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning: only create the collection once ! Otherwise you will loose the content of it\n",
    "\n",
    "# collection_name = \"wikipedia_fr\"\n",
    "# if not client.collection_exists(collection_name):\n",
    "#     client.create_collection(\n",
    "#         collection_name=collection_name,\n",
    "#         vectors_config=VectorParams(size=vector_size, distance=\"Cosine\"),\n",
    "#     )\n",
    "# else:\n",
    "#     print(f\"Collection '{collection_name}' already exists; skipping creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "#load file from parquet (if you saved it once previously)\n",
    "pf = pq.ParquetFile(\"articles_fr_withLinks.parquet\")\n",
    "rowgroup_batch = 512\n",
    "encode_batch = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb711a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles stored: 26112\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http import models as rest\n",
    "#get articles count from Qdrant DB \n",
    "\n",
    "collection_name = \"wikipedia_fr\"\n",
    "total = client.count(collection_name=collection_name, exact=True).count\n",
    "print(f\"Total articles stored: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recover a snapshot from the Qdrant db (should not be needed)\n",
    "client.recover_snapshot(collection_name=\"wikipedia_fr\", location=\"http://localhost:6333/collections/wikipedia_fr/snapshots/<snapshot_name>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Process Articles into the Qdrant DB (warning: takes shit ton of time and ressources)\n",
    "\n",
    "ARTICLES_NUMBER=2000\n",
    "\n",
    "new_processed = 0\n",
    "max_rows = ARTICLES_NUMBER\n",
    "\n",
    "for rg in range(pf.num_row_groups):\n",
    "    if max_rows is not None and new_processed >= max_rows:\n",
    "        break\n",
    "    table = pf.read_row_group(rg, columns=[\"id\", \"title\", \"url\", \"text\"])\n",
    "    df = pl.from_arrow(table).with_columns([\n",
    "        pl.col(\"id\").cast(pl.Int64),\n",
    "        pl.col(\"title\").fill_null(\"\"),\n",
    "        pl.col(\"url\").fill_null(\"\"),\n",
    "        pl.col(\"text\").fill_null(\"\")\n",
    "    ])\n",
    "    n = df.height\n",
    "    remaining = None if max_rows is None else max_rows - new_processed\n",
    "    for start in range(0, n, rowgroup_batch):\n",
    "        length = min(rowgroup_batch, n - start)\n",
    "        if remaining is not None:\n",
    "            length = min(length, remaining)\n",
    "        sub = df.slice(start, length)\n",
    "        ids = sub[\"id\"].to_list()\n",
    "        texts = sub[\"text\"].to_list()\n",
    "        titles = sub[\"title\"].to_list()\n",
    "        urls = sub[\"url\"].to_list()\n",
    "\n",
    "        existing = client.retrieve(\n",
    "            collection_name=\"wikipedia_fr\",\n",
    "            ids=[int(i) for i in ids],\n",
    "            with_payload=False,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        existing_ids = {p.id for p in existing}\n",
    "        missing_idx = [i for i, pid in enumerate(ids) if int(pid) not in existing_ids]\n",
    "        if not missing_idx:\n",
    "            continue\n",
    "\n",
    "        ids_m = [ids[i] for i in missing_idx]\n",
    "        texts_m = [texts[i] for i in missing_idx]\n",
    "        titles_m = [titles[i] for i in missing_idx]\n",
    "        urls_m = [urls[i] for i in missing_idx]\n",
    "\n",
    "        vectors = model.encode(texts_m, batch_size=encode_batch, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=int(ids_m[i]),\n",
    "                vector=vectors[i].tolist(),\n",
    "                payload={\"id\": int(ids_m[i]), \"title\": titles_m[i], \"url\": urls_m[i], \"text\": texts_m[i]},\n",
    "            )\n",
    "            for i in range(len(ids_m))\n",
    "        ]\n",
    "        client.upsert(collection_name=\"wikipedia_fr\", points=points, wait=True)\n",
    "        new_processed += len(ids_m)\n",
    "        if max_rows is not None and new_processed >= max_rows:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aafc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "import polars as pl\n",
    "from typing import Optional\n",
    "\n",
    "#Query Qdrant DB to rebuild the polars df (warning: untested)\n",
    "\n",
    "def rebuild_df_from_qdrant(\n",
    "    client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    max_rows: Optional[int] = 1000,\n",
    "    batch_limit: int = 1000,\n",
    ") -> pl.DataFrame:\n",
    "    rows = []\n",
    "    next_page = None\n",
    "\n",
    "    selector = rest.PayloadSelectorInclude(include=[\"id\", \"title\", \"url\", \"text\"])\n",
    "\n",
    "    while True:\n",
    "        points, next_page = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=batch_limit,\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "            payload_selector=selector,\n",
    "            offset=next_page,\n",
    "        )\n",
    "        if not points:\n",
    "            break\n",
    "\n",
    "        for p in points:\n",
    "            pid = p.payload.get(\"id\", p.id)\n",
    "            rows.append({\n",
    "                \"id\": pid,\n",
    "                \"title\": p.payload.get(\"title\", \"\"),\n",
    "                \"url\": p.payload.get(\"url\", \"\"),\n",
    "                \"text\": p.payload.get(\"text\", \"\"),\n",
    "            })\n",
    "            if max_rows is not None and len(rows) >= max_rows:\n",
    "                break\n",
    "\n",
    "        if max_rows is not None and len(rows) >= max_rows:\n",
    "            break\n",
    "        if next_page is None:\n",
    "            break\n",
    "\n",
    "    df = pl.DataFrame(rows).with_columns([\n",
    "        pl.col(\"id\").cast(pl.Int64),\n",
    "        pl.col(\"title\").cast(pl.Utf8).fill_null(\"\"),\n",
    "        pl.col(\"url\").cast(pl.Utf8).fill_null(\"\"),\n",
    "        pl.col(\"text\").cast(pl.Utf8).fill_null(\"\"),\n",
    "    ])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b36e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top matches for 'Math':\n",
      "‚Ä¢ Nombre entier (score=0.824)\n",
      "‚Ä¢ S√©rie (math√©matiques) (score=0.818)\n",
      "‚Ä¢ Distance (math√©matiques) (score=0.813)\n",
      "‚Ä¢ Nombre (score=0.812)\n",
      "‚Ä¢ G√©om√©trie arithm√©tique (score=0.811)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1272/1232709101.py:11: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "#Query DB with one keyword for cosine similarity (warning : NANs issues gives 1.0 score)\n",
    "\n",
    "query_title = \"Math\"\n",
    "\n",
    "qdf = df.filter(pl.col(\"title\").str.to_lowercase() == query_title.lower())\n",
    "query_text = qdf.select(\"text\").to_series()[0] if qdf.height > 0 else query_title\n",
    "\n",
    "# E5 requires the 'query:' prefix for queries; documents were encoded as 'passage:' during ingestion\n",
    "query_vector = model.encode([f\"query: {query_text}\"], normalize_embeddings=True)[0]\n",
    "\n",
    "results = client.search(\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    query_vector=query_vector,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(f\"\\nüîç Top matches for '{query_title}':\")\n",
    "for r in results:\n",
    "    print(f\"‚Ä¢ {r.payload.get('title')} (score={r.score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f510b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Similar articles for: Atome (id=189)\n",
      "  ‚Ä¢ Proton (id=2414, score=0.939)\n",
      "  ‚Ä¢ Liaison chimique (id=22131, score=0.939)\n",
      "  ‚Ä¢ √âl√©ment chimique (id=15349, score=0.936)\n",
      "  ‚Ä¢ √âlectron (id=6716, score=0.935)\n",
      "  ‚Ä¢ Particule √©l√©mentaire (id=23547, score=0.934)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33189/278556868.py:25: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.recommend(collection_name=\"wikipedia_fr\", positive=[int(article_id)], limit=k)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Function to encode whole article text into vector \n",
    "# Then query the db with the article embedding to get the k most similar articles\n",
    "\n",
    "def encode_article_text(text):\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return np.zeros(model.get_sentence_embedding_dimension(), dtype=np.float32)\n",
    "    chunks = [\" \".join(words[i:i+256]) for i in range(0, len(words), 256)]\n",
    "    vecs = model.encode([f\"passage: {c}\" for c in chunks], normalize_embeddings=True)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "mode = \"ids\"  # \"all\" or \"ids\"\n",
    "ids_to_process = [189, 205]\n",
    "k = 5\n",
    "\n",
    "ids_all = df.select(pl.col(\"id\").cast(pl.Int64)).to_series().to_list()\n",
    "ids = ids_all if mode == \"all\" else [i for i in ids_to_process if i in set(ids_all)]\n",
    "\n",
    "for article_id in ids:\n",
    "    title_row = df.filter(pl.col(\"id\") == int(article_id)).select(\"title\")\n",
    "    title = title_row.to_series()[0] if title_row.height > 0 else \"(unknown)\"\n",
    "    try:\n",
    "        points = client.retrieve(collection_name=\"wikipedia_fr\", ids=[int(article_id)], with_payload=True, with_vectors=False)\n",
    "        if points:\n",
    "            results = client.recommend(collection_name=\"wikipedia_fr\", positive=[int(article_id)], limit=k)\n",
    "        else:\n",
    "            raise ValueError(\"missing\")\n",
    "    except Exception:\n",
    "        row = df.filter(pl.col(\"id\") == int(article_id))\n",
    "        if row.height == 0:\n",
    "            continue\n",
    "        text = row.select(pl.col(\"text\").fill_null(\"\")).to_series()[0]\n",
    "        vec = encode_article_text(text)\n",
    "        results = client.search(collection_name=\"wikipedia_fr\", query_vector=vec, limit=k)\n",
    "    print(f\"\\nüîé Similar articles for: {title} (id={int(article_id)})\")\n",
    "    for r in results:\n",
    "        print(f\"  ‚Ä¢ {r.payload.get('title')} (id={r.payload.get('id')}, score={r.score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a718c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "#Export articles to csv for further use\n",
    "\n",
    "cols = [\"id\", \"title\", \"url\", \"text\"]\n",
    "df.select(cols).write_csv(\"articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30563b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as pacsv\n",
    "\n",
    "pf = pq.ParquetFile(\"articles_fr_withLinks.parquet\")\n",
    "cols = [\"id\", \"title\", \"url\", \"text\"]\n",
    "ARTICLES_NUMBER = 100000\n",
    "processed = 0\n",
    "first = True\n",
    "output_path = \"articles_stream.csv\"\n",
    "\n",
    "for rg in range(pf.num_row_groups):\n",
    "    if ARTICLES_NUMBER is not None and processed >= ARTICLES_NUMBER:\n",
    "        break\n",
    "    table = pf.read_row_group(rg, columns=cols)\n",
    "    df_rg = pl.from_arrow(table).with_columns([\n",
    "        pl.col(\"id\").cast(pl.Int64),\n",
    "        pl.col(\"title\").fill_null(\"\"),\n",
    "        pl.col(\"url\").fill_null(\"\"),\n",
    "        pl.col(\"text\").fill_null(\"\")\n",
    "    ])\n",
    "    remaining = None if ARTICLES_NUMBER is None else ARTICLES_NUMBER - processed\n",
    "    if remaining is not None and df_rg.height > remaining:\n",
    "        df_rg = df_rg.slice(0, remaining)\n",
    "    mode = \"wb\" if first else \"ab\"\n",
    "    with open(output_path, mode) as f:\n",
    "        pacsv.write_csv(df_rg.to_arrow(), f, write_options=pacsv.WriteOptions(include_header=first))\n",
    "    processed += df_rg.height\n",
    "    first = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816b818",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_similar_articles_by_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Exported recomputed links to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_csv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Generate a 10-NN hyperlink graph (adjust k as needed)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mexport_similarity_links\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwikipedia_fr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_csv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecomputed_similarity_links.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# ... existing code ...\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mexport_similarity_links\u001b[39m\u001b[34m(client, collection_name, k, out_csv_path)\u001b[39m\n\u001b[32m     32\u001b[39m count = \u001b[32m0\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m source_id \u001b[38;5;129;01min\u001b[39;00m iter_all_point_ids(client, collection_name):\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Resolve to actual point ID if payload_id is used\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     neighbors = \u001b[43mget_similar_articles_by_id\u001b[49m(client, collection_name, source_id, k=k)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m neighbors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m         \u001b[38;5;66;03m# Skip if the source article is missing as a stored vector\u001b[39;00m\n\u001b[32m     38\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'get_similar_articles_by_id' is not defined"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def export_points_df(df: pl.DataFrame, client, collection_name: str, model, encode_batch: int = 64, upsert_batch: int = 512, max_rows: int | None = None):\n",
    "    n = df.height if max_rows is None else min(df.height, max_rows)\n",
    "    processed = 0\n",
    "    while processed < n:\n",
    "        length = min(upsert_batch, n - processed)\n",
    "        sub = df.slice(processed, length)\n",
    "        ids = sub[\"id\"].cast(pl.Int64).to_list()\n",
    "        titles = sub[\"title\"].fill_null(\"\").to_list()\n",
    "        urls = sub[\"url\"].fill_null(\"\").to_list()\n",
    "        texts = sub[\"text\"].fill_null(\"\").to_list()\n",
    "        vectors = model.encode(texts, batch_size=encode_batch, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=int(ids[i]),\n",
    "                vector=vectors[i].tolist(),\n",
    "                payload={\"id\": int(ids[i]), \"title\": titles[i], \"url\": urls[i], \"text\": texts[i]},\n",
    "            )\n",
    "            for i in range(len(ids))\n",
    "        ]\n",
    "        client.upsert(collection_name=collection_name, points=points, wait=True)\n",
    "        processed += length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93730d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def export_points_parquet(parquet_path: str, client, collection_name: str, model, encode_batch: int = 64, upsert_batch: int = 512, max_rows: int | None = None):\n",
    "    pf = pq.ParquetFile(parquet_path)\n",
    "    processed = 0\n",
    "    for rg in range(pf.num_row_groups):\n",
    "        if max_rows is not None and processed >= max_rows:\n",
    "            break\n",
    "        table = pf.read_row_group(rg, columns=[\"id\", \"title\", \"url\", \"text\"])\n",
    "        df = pl.from_arrow(table).with_columns([\n",
    "            pl.col(\"id\").cast(pl.Int64),\n",
    "            pl.col(\"title\").fill_null(\"\"),\n",
    "            pl.col(\"url\").fill_null(\"\"),\n",
    "            pl.col(\"text\").fill_null(\"\")\n",
    "        ])\n",
    "        remaining = None if max_rows is None else max_rows - processed\n",
    "        n = df.height if remaining is None else min(df.height, remaining)\n",
    "        inner = 0\n",
    "        while inner < n:\n",
    "            length = min(upsert_batch, n - inner)\n",
    "            sub = df.slice(inner, length)\n",
    "            ids = sub[\"id\"].to_list()\n",
    "            titles = sub[\"title\"].to_list()\n",
    "            urls = sub[\"url\"].to_list()\n",
    "            texts = sub[\"text\"].to_list()\n",
    "            vectors = model.encode(texts, batch_size=encode_batch, convert_to_numpy=True, normalize_embeddings=True)\n",
    "            points = [\n",
    "                PointStruct(\n",
    "                    id=int(ids[i]),\n",
    "                    vector=vectors[i].tolist(),\n",
    "                    payload={\"id\": int(ids[i]), \"title\": titles[i], \"url\": urls[i], \"text\": texts[i]},\n",
    "                )\n",
    "                for i in range(len(ids))\n",
    "            ]\n",
    "            client.upsert(collection_name=collection_name, points=points, wait=True)\n",
    "            processed += length\n",
    "            inner += length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
