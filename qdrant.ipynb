{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55eaadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import env\n",
    "\n",
    "\n",
    "#RUN this if you are not using the docker image (use it cause takes too much memory otherwise)\n",
    "from huggingface_hub import login\n",
    "login(env.HF_LOGIN_TOKEN)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Connect to Qdrant\n",
    "# ----------------------------\n",
    "# Option 1: In-memory (no Docker)\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# # Option 2: Local Qdrant server\n",
    "# client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Step 2: Initialize model\n",
    "# ----------------------------\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "model.save(\"models/multilingual-e5-large\") #save model locally (do only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1ce311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "#If you already saved the model locally and are using docker\n",
    "client = QdrantClient(host=\"localhost\", port=6333, prefer_grpc=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"models/multilingual-e5-large\", device=device)\n",
    "\n",
    "# Run docker container before\n",
    "# docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v /home/Loris/EPFL/MA3/ML/project2/project2Rag/qdrant_storage:/qdrant/storage qdrant/qdrant:latest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326c9a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4498441 articles into a single Polars DataFrame\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Step 3: Load articles from JSON (Polars)\n",
    "# ----------------------------\n",
    "from pathlib import Path\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "path = Path(\"../wikiextractor/articles_fr_withoutLinks.json\")\n",
    "\n",
    "ARTICLES_NUMBER = None  # None = load all; set an int to cap when sampling\n",
    "\n",
    "def load_jsonl_to_df(path: Path, max_rows: int | None = None) -> pl.DataFrame:\n",
    "    # If loading the full dataset, use Polars' fast NDJSON reader\n",
    "    if max_rows is None:\n",
    "        try:\n",
    "            df = pl.read_ndjson(str(path))\n",
    "        except Exception:\n",
    "            # Fallback if the NDJSON reader hits malformed lines: manual parse\n",
    "            rows = []\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        rows.append(json.loads(line))\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "            df = pl.DataFrame(rows)\n",
    "    else:\n",
    "        # Limited load: manual parse up to max_rows\n",
    "        rows = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_rows:\n",
    "                    break\n",
    "                try:\n",
    "                    rows.append(json.loads(line))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        df = pl.DataFrame(rows)\n",
    "\n",
    "    # Ensure columns exist and normalize types/nulls\n",
    "    cols = set(df.columns)\n",
    "\n",
    "    if \"id\" in cols:\n",
    "        # Cast id to Int64\n",
    "        df = df.with_columns(pl.col(\"id\").cast(pl.Int64))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(None).cast(pl.Int64).alias(\"id\"))\n",
    "\n",
    "    if \"title\" in cols:\n",
    "        df = df.with_columns(pl.col(\"title\").cast(pl.Utf8).fill_null(\"\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(\"\").alias(\"title\"))\n",
    "\n",
    "    if \"url\" in cols:\n",
    "        df = df.with_columns(pl.col(\"url\").cast(pl.Utf8).fill_null(\"\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(\"\").alias(\"url\"))\n",
    "\n",
    "    if \"text\" in cols:\n",
    "        df = df.with_columns(pl.col(\"text\").cast(pl.Utf8).fill_null(\"\"))\n",
    "    else:\n",
    "        df = df.with_columns(pl.lit(\"\").alias(\"text\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_jsonl_to_df(path, max_rows=ARTICLES_NUMBER)\n",
    "print(f\"✅ Loaded {len(df)} articles into a single Polars DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6d7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write polars df to disk if needed\n",
    "df.write_parquet(\"articles_fr_withoutLinks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2baa96c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'full_url': 'https://fr.wikipedia.org/wiki/Moulins%20%28Allier%29', 'start_idx': 25, 'anchor': 'Moulins', 'href_raw': 'Moulins%20%28Allier%29', 'href_decoded': 'Moulins (Allier)'}, {'full_url': 'https://fr.wikipedia.org/wiki/Allier%20%28d%C3%A9partement%29', 'start_idx': 83, 'anchor': 'Allier', 'href_raw': 'Allier%20%28d%C3%A9partement%29', 'href_decoded': 'Allier (département)'}, {'full_url': 'https://fr.wikipedia.org/wiki/Ch%C3%A2teaumeillant', 'start_idx': 162, 'anchor': 'Châteaumeillant', 'href_raw': 'Ch%C3%A2teaumeillant', 'href_decoded': 'Châteaumeillant'}, {'full_url': 'https://fr.wikipedia.org/wiki/Cher%20%28d%C3%A9partement%29', 'start_idx': 226, 'anchor': 'Cher', 'href_raw': 'Cher%20%28d%C3%A9partement%29', 'href_decoded': 'Cher (département)'}, {'full_url': 'https://fr.wikipedia.org/wiki/Philologie', 'start_idx': 296, 'anchor': 'philologue', 'href_raw': 'Philologie', 'href_decoded': 'Philologie'}, {'full_url': 'https://fr.wikipedia.org/wiki/liste%20de%20linguistes', 'start_idx': 367, 'anchor': 'linguiste', 'href_raw': 'liste%20de%20linguistes', 'href_decoded': 'liste de linguistes'}, {'full_url': 'https://fr.wikipedia.org/wiki/Allier%20%28d%C3%A9partement%29', 'start_idx': 549, 'anchor': 'bourbonnaise', 'href_raw': 'Allier%20%28d%C3%A9partement%29', 'href_decoded': 'Allier (département)'}, {'full_url': 'https://fr.wikipedia.org/wiki/notaire', 'start_idx': 631, 'anchor': 'notaire', 'href_raw': 'notaire', 'href_decoded': 'notaire'}, {'full_url': 'https://fr.wikipedia.org/wiki/Ch%C3%A2teaumeillant', 'start_idx': 676, 'anchor': 'Châteaumeillant', 'href_raw': 'Ch%C3%A2teaumeillant', 'href_decoded': 'Châteaumeillant'}, {'full_url': 'https://fr.wikipedia.org/wiki/Cher%20%28d%C3%A9partement%29', 'start_idx': 740, 'anchor': 'Cher', 'href_raw': 'Cher%20%28d%C3%A9partement%29', 'href_decoded': 'Cher (département)'}, {'full_url': 'https://fr.wikipedia.org/wiki/Moulins%20%28Allier%29', 'start_idx': 813, 'anchor': 'Moulins', 'href_raw': 'Moulins%20%28Allier%29', 'href_decoded': 'Moulins (Allier)'}, {'full_url': 'https://fr.wikipedia.org/wiki/lyc%C3%A9e%20Th%C3%A9odore-de-Banville', 'start_idx': 967, 'anchor': 'lycée', 'href_raw': 'lyc%C3%A9e%20Th%C3%A9odore-de-Banville', 'href_decoded': 'lycée Théodore-de-Banville'}, {'full_url': 'https://fr.wikipedia.org/wiki/facult%C3%A9%20des%20lettres%20de%20Paris', 'start_idx': 1081, 'anchor': 'faculté des lettres de Paris', 'href_raw': 'facult%C3%A9%20des%20lettres%20de%20Paris', 'href_decoded': 'faculté des lettres de Paris'}, {'full_url': 'https://fr.wikipedia.org/wiki/Louis%20Havet', 'start_idx': 1212, 'anchor': 'Louis Havet', 'href_raw': 'Louis%20Havet', 'href_decoded': 'Louis Havet'}, {'full_url': 'https://fr.wikipedia.org/wiki/Michel%20Br%C3%A9al', 'start_idx': 1296, 'anchor': 'Michel Bréal', 'href_raw': 'Michel%20Br%C3%A9al', 'href_decoded': 'Michel Bréal'}, {'full_url': 'https://fr.wikipedia.org/wiki/Coll%C3%A8ge%20de%20France', 'start_idx': 1358, 'anchor': 'Collège de France', 'href_raw': 'Coll%C3%A8ge%20de%20France', 'href_decoded': 'Collège de France'}, {'full_url': 'https://fr.wikipedia.org/wiki/Ferdinand%20de%20Saussure', 'start_idx': 1435, 'anchor': 'Ferdinand de Saussure', 'href_raw': 'Ferdinand%20de%20Saussure', 'href_decoded': 'Ferdinand de Saussure'}, {'full_url': 'https://fr.wikipedia.org/wiki/%C3%89cole%20pratique%20des%20hautes%20%C3%A9tudes', 'start_idx': 1513, 'anchor': 'École pratique des hautes études', 'href_raw': '%C3%89cole%20pratique%20des%20hautes%20%C3%A9tudes', 'href_decoded': 'École pratique des hautes études'}, {'full_url': 'https://fr.wikipedia.org/wiki/agr%C3%A9gation%20de%20grammaire', 'start_idx': 1651, 'anchor': 'agrégation de grammaire', 'href_raw': 'agr%C3%A9gation%20de%20grammaire', 'href_decoded': 'agrégation de grammaire'}, {'full_url': 'https://fr.wikipedia.org/wiki/Arm%C3%A9nie', 'start_idx': 1774, 'anchor': 'Arménie', 'href_raw': 'Arm%C3%A9nie', 'href_decoded': 'Arménie'}, {'full_url': 'https://fr.wikipedia.org/wiki/Etchmiadzin', 'start_idx': 1834, 'anchor': 'Etchmiadzin', 'href_raw': 'Etchmiadzin', 'href_decoded': 'Etchmiadzin'}, {'full_url': 'https://fr.wikipedia.org/wiki/grammaire%20compar%C3%A9e', 'start_idx': 2026, 'anchor': 'grammaire comparée', 'href_raw': 'grammaire%20compar%C3%A9e', 'href_decoded': 'grammaire comparée'}, {'full_url': 'https://fr.wikipedia.org/wiki/langues%20persanes', 'start_idx': 2157, 'anchor': 'langues persanes', 'href_raw': 'langues%20persanes', 'href_decoded': 'langues persanes'}, {'full_url': 'https://fr.wikipedia.org/wiki/Doctorat%20%C3%A8s%20lettres%20%28France%29', 'start_idx': 2258, 'anchor': 'doctorat ès lettres', 'href_raw': 'Doctorat%20%C3%A8s%20lettres%20%28France%29', 'href_decoded': 'Doctorat ès lettres (France)'}, {'full_url': 'https://fr.wikipedia.org/wiki/vieux-slave', 'start_idx': 2398, 'anchor': 'vieux-slave', 'href_raw': 'vieux-slave', 'href_decoded': 'vieux-slave'}, {'full_url': 'https://fr.wikipedia.org/wiki/Auguste%20Carri%C3%A8re', 'start_idx': 2494, 'anchor': 'Auguste Carrière', 'href_raw': 'Auguste%20Carri%C3%A8re', 'href_decoded': 'Auguste Carrière'}, {'full_url': 'https://fr.wikipedia.org/wiki/arm%C3%A9nien', 'start_idx': 2575, 'anchor': 'arménien', 'href_raw': 'arm%C3%A9nien', 'href_decoded': 'arménien'}, {'full_url': 'https://fr.wikipedia.org/wiki/Institut%20national%20des%20langues%20et%20civilisations%20orientales', 'start_idx': 2629, 'anchor': 'École des langues orientales', 'href_raw': 'Institut%20national%20des%20langues%20et%20civilisations%20orientales', 'href_decoded': 'Institut national des langues et civilisations orientales'}, {'full_url': 'https://fr.wikipedia.org/wiki/Michel%20Br%C3%A9al', 'start_idx': 2778, 'anchor': 'Michel Bréal', 'href_raw': 'Michel%20Br%C3%A9al', 'href_decoded': 'Michel Bréal'}, {'full_url': 'https://fr.wikipedia.org/wiki/Coll%C3%A8ge%20de%20France', 'start_idx': 2882, 'anchor': 'Collège de France', 'href_raw': 'Coll%C3%A8ge%20de%20France', 'href_decoded': 'Collège de France'}, {'full_url': 'https://fr.wikipedia.org/wiki/langues%20indo-europ%C3%A9ennes', 'start_idx': 3014, 'anchor': 'langues indo-européennes', 'href_raw': 'langues%20indo-europ%C3%A9ennes', 'href_decoded': 'langues indo-européennes'}, {'full_url': 'https://fr.wikipedia.org/wiki/%C3%89cole%20pratique%20des%20hautes%20%C3%A9tudes', 'start_idx': 3255, 'anchor': 'École pratique des hautes études', 'href_raw': '%C3%89cole%20pratique%20des%20hautes%20%C3%A9tudes', 'href_decoded': 'École pratique des hautes études'}, {'full_url': 'https://fr.wikipedia.org/wiki/Soci%C3%A9t%C3%A9%20de%20linguistique%20de%20Paris', 'start_idx': 3383, 'anchor': 'Société de linguistique de Paris', 'href_raw': 'Soci%C3%A9t%C3%A9%20de%20linguistique%20de%20Paris', 'href_decoded': 'Société de linguistique de Paris'}, {'full_url': 'https://fr.wikipedia.org/wiki/Acad%C3%A9mie%20des%20inscriptions%20et%20belles-lettres', 'start_idx': 3509, 'anchor': 'Académie des inscriptions et belles-lettres', 'href_raw': 'Acad%C3%A9mie%20des%20inscriptions%20et%20belles-lettres', 'href_decoded': 'Académie des inscriptions et belles-lettres'}, {'full_url': 'https://fr.wikipedia.org/wiki/Institut%20d%27%C3%A9tudes%20slaves', 'start_idx': 3668, 'anchor': \"Institut d'études slaves\", 'href_raw': 'Institut%20d%27%C3%A9tudes%20slaves', 'href_decoded': \"Institut d'études slaves\"}, {'full_url': 'https://fr.wikipedia.org/wiki/%C3%89mile%20Benveniste', 'start_idx': 3845, 'anchor': 'Émile Benveniste', 'href_raw': '%C3%89mile%20Benveniste', 'href_decoded': 'Émile Benveniste'}, {'full_url': 'https://fr.wikipedia.org/wiki/Marcel%20Cohen', 'start_idx': 3913, 'anchor': 'Marcel Cohen', 'href_raw': 'Marcel%20Cohen', 'href_decoded': 'Marcel Cohen'}, {'full_url': 'https://fr.wikipedia.org/wiki/Georges%20Dum%C3%A9zil', 'start_idx': 3968, 'anchor': 'Georges Dumézil', 'href_raw': 'Georges%20Dum%C3%A9zil', 'href_decoded': 'Georges Dumézil'}, {'full_url': 'https://fr.wikipedia.org/wiki/Lilias%20Homburger', 'start_idx': 4034, 'anchor': 'Lilias Homburger', 'href_raw': 'Lilias%20Homburger', 'href_decoded': 'Lilias Homburger'}, {'full_url': 'https://fr.wikipedia.org/wiki/Andr%C3%A9%20Martinet', 'start_idx': 4097, 'anchor': 'André Martinet', 'href_raw': 'Andr%C3%A9%20Martinet', 'href_decoded': 'André Martinet'}, {'full_url': 'https://fr.wikipedia.org/wiki/Aur%C3%A9lien%20Sauvageot', 'start_idx': 4161, 'anchor': 'Aurélien Sauvageot', 'href_raw': 'Aur%C3%A9lien%20Sauvageot', 'href_decoded': 'Aurélien Sauvageot'}, {'full_url': 'https://fr.wikipedia.org/wiki/Lucien%20Tesni%C3%A8re', 'start_idx': 4233, 'anchor': 'Lucien Tesnière', 'href_raw': 'Lucien%20Tesni%C3%A8re', 'href_decoded': 'Lucien Tesnière'}, {'full_url': 'https://fr.wikipedia.org/wiki/Japonisation', 'start_idx': 4302, 'anchor': 'japonisant', 'href_raw': 'Japonisation', 'href_decoded': 'Japonisation'}, {'full_url': 'https://fr.wikipedia.org/wiki/Charles%20Haguenauer', 'start_idx': 4352, 'anchor': 'Charles Haguenauer', 'href_raw': 'Charles%20Haguenauer', 'href_decoded': 'Charles Haguenauer'}, {'full_url': 'https://fr.wikipedia.org/wiki/Joseph%20Vendryes', 'start_idx': 4421, 'anchor': 'Joseph Vendryes', 'href_raw': 'Joseph%20Vendryes', 'href_decoded': 'Joseph Vendryes'}, {'full_url': 'https://fr.wikipedia.org/wiki/Jean%20Paulhan', 'start_idx': 4525, 'anchor': 'Jean Paulhan', 'href_raw': 'Jean%20Paulhan', 'href_decoded': 'Jean Paulhan'}, {'full_url': 'https://fr.wikipedia.org/wiki/Gustave%20Guillaume', 'start_idx': 4636, 'anchor': 'Gustave Guillaume', 'href_raw': 'Gustave%20Guillaume', 'href_decoded': 'Gustave Guillaume'}, {'full_url': 'https://fr.wikipedia.org/wiki/grammaticalisation', 'start_idx': 4827, 'anchor': 'grammaticalisation', 'href_raw': 'grammaticalisation', 'href_decoded': 'grammaticalisation'}, {'full_url': 'https://fr.wikipedia.org/wiki/Linguistique', 'start_idx': 4901, 'anchor': 'linguiste', 'href_raw': 'Linguistique', 'href_decoded': 'Linguistique'}, {'full_url': 'https://fr.wikipedia.org/wiki/Walter%20Porzig', 'start_idx': 4959, 'anchor': 'Walter Porzig', 'href_raw': 'Walter%20Porzig', 'href_decoded': 'Walter Porzig'}, {'full_url': 'https://fr.wikipedia.org/wiki/Dialecte', 'start_idx': 5091, 'anchor': 'dialectes', 'href_raw': 'Dialecte', 'href_decoded': 'Dialecte'}, {'full_url': 'https://fr.wikipedia.org/wiki/Variation%20linguistique', 'start_idx': 5213, 'anchor': 'variation diatopique', 'href_raw': 'Variation%20linguistique', 'href_decoded': 'Variation linguistique'}, {'full_url': 'https://fr.wikipedia.org/wiki/sociolinguistique', 'start_idx': 5312, 'anchor': 'sociolinguistique', 'href_raw': 'sociolinguistique', 'href_decoded': 'sociolinguistique'}, {'full_url': 'https://fr.wikipedia.org/wiki/Cours%20de%20linguistique%20g%C3%A9n%C3%A9rale', 'start_idx': 5458, 'anchor': 'Cours de linguistique générale', 'href_raw': 'Cours%20de%20linguistique%20g%C3%A9n%C3%A9rale', 'href_decoded': 'Cours de linguistique générale'}, {'full_url': 'https://fr.wikipedia.org/wiki/Ferdinand%20de%20Saussure', 'start_idx': 5565, 'anchor': 'Ferdinand de Saussure', 'href_raw': 'Ferdinand%20de%20Saussure', 'href_decoded': 'Ferdinand de Saussure'}, {'full_url': 'https://fr.wikipedia.org/wiki/Ch%C3%A2teaumeillant', 'start_idx': 5723, 'anchor': 'Châteaumeillant', 'href_raw': 'Ch%C3%A2teaumeillant', 'href_decoded': 'Châteaumeillant'}, {'full_url': 'https://fr.wikipedia.org/wiki/Moulins%20%28Allier%29', 'start_idx': 5817, 'anchor': 'Moulins', 'href_raw': 'Moulins%20%28Allier%29', 'href_decoded': 'Moulins (Allier)'}, {'full_url': 'https://fr.wikipedia.org/wiki/Milman%20Parry', 'start_idx': 5965, 'anchor': 'Milman Parry', 'href_raw': 'Milman%20Parry', 'href_decoded': 'Milman Parry'}, {'full_url': 'https://fr.wikipedia.org/wiki/l%27Iliade', 'start_idx': 6118, 'anchor': \"l'Iliade\", 'href_raw': 'l%27Iliade', 'href_decoded': \"l'Iliade\"}, {'full_url': 'https://fr.wikipedia.org/wiki/l%27Iliade', 'start_idx': 6388, 'anchor': \"l'Iliade\", 'href_raw': 'l%27Iliade', 'href_decoded': \"l'Iliade\"}, {'full_url': 'https://fr.wikipedia.org/wiki/Matija%20Murko', 'start_idx': 6523, 'anchor': 'Matija Murko', 'href_raw': 'Matija%20Murko', 'href_decoded': 'Matija Murko'}, {'full_url': 'https://fr.wikipedia.org/wiki/Slov%C3%A9nie', 'start_idx': 6599, 'anchor': 'Slovénie', 'href_raw': 'Slov%C3%A9nie', 'href_decoded': 'Slovénie'}, {'full_url': 'https://fr.wikipedia.org/wiki/Balkans', 'start_idx': 6717, 'anchor': 'Balkans', 'href_raw': 'Balkans', 'href_decoded': 'Balkans'}, {'full_url': 'https://fr.wikipedia.org/wiki/Bosnie-Herz%C3%A9govine', 'start_idx': 6771, 'anchor': 'Bosnie-Herzégovine', 'href_raw': 'Bosnie-Herz%C3%A9govine', 'href_decoded': 'Bosnie-Herzégovine'}, {'full_url': 'https://fr.wikipedia.org/wiki/Albert%20Lord', 'start_idx': 6955, 'anchor': 'Albert Lord', 'href_raw': 'Albert%20Lord', 'href_decoded': 'Albert Lord'}]\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import re\n",
    "import html\n",
    "from urllib.parse import unquote\n",
    "#Extract href links from the articles with links and saves them to a df\n",
    "\n",
    "BASE_URL = \"https://fr.wikipedia.org/wiki/\"\n",
    "ANCHOR_RE = re.compile(r'<a\\s+[^>]*href=\"([^\"]+)\"[^>]*>(.*?)</a>', re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def extract_links_with_pos(text: str):\n",
    "    if not text:\n",
    "        return []\n",
    "    unescaped = html.unescape(text)\n",
    "    out = []\n",
    "    search_from = 0\n",
    "    for m in ANCHOR_RE.finditer(unescaped):\n",
    "        href_raw = m.group(1)\n",
    "        anchor = html.unescape(m.group(2))\n",
    "        needle = f'&lt;a href=\"{href_raw}\"'\n",
    "        pos = text.find(needle, search_from)\n",
    "        if pos == -1:\n",
    "            pos = text.find(\"&lt;a \", search_from)\n",
    "        if pos == -1:\n",
    "            pos = 0\n",
    "        search_from = pos + 1\n",
    "        out.append({\n",
    "            \"full_url\": BASE_URL + href_raw,\n",
    "            \"start_idx\": pos,\n",
    "            \"anchor\": anchor,\n",
    "            \"href_raw\": href_raw,\n",
    "            \"href_decoded\": unquote(href_raw),\n",
    "        })\n",
    "    return out\n",
    "\n",
    "LINK_ITEM = pl.Struct([\n",
    "    pl.Field(\"full_url\", pl.Utf8),\n",
    "    pl.Field(\"start_idx\", pl.Int64),\n",
    "    pl.Field(\"anchor\", pl.Utf8),\n",
    "    pl.Field(\"href_raw\", pl.Utf8),\n",
    "    pl.Field(\"href_decoded\", pl.Utf8),\n",
    "])\n",
    "LINK_LIST = pl.List(LINK_ITEM)\n",
    "\n",
    "def build_per_article_links(df: pl.DataFrame, max_rows: int | None = None) -> pl.DataFrame:\n",
    "    if max_rows is not None:\n",
    "        df = df.slice(0, min(max_rows, df.height))\n",
    "    idx = (\n",
    "        df.select([\"id\", \"title\", \"text\"])\n",
    "          .with_columns(pl.col(\"text\").cast(pl.Utf8).fill_null(\"\"))\n",
    "          .with_columns(links=pl.col(\"text\").map_elements(extract_links_with_pos, return_dtype=LINK_LIST))\n",
    "          .drop(\"text\")\n",
    "          .with_columns(\n",
    "              links=pl.when(pl.col(\"links\").is_null()).then(pl.lit([]).cast(LINK_LIST)).otherwise(pl.col(\"links\")),\n",
    "              link_count=pl.col(\"links\").list.len()\n",
    "          )\n",
    "    )\n",
    "    return idx\n",
    "\n",
    "# Example usage on your Polars df:\n",
    "per_article = build_per_article_links(df, max_rows=10000)\n",
    "# print(per_article.select([\"id\", \"title\", \"link_count\"]).head(10))\n",
    "\n",
    "# Inspect one article: all links and their positions in a single row\n",
    "article_id = 3\n",
    "row = per_article.filter(pl.col(\"id\") == article_id)\n",
    "links = row.select(\"links\").to_series().to_list()[0] if row.height else []\n",
    "print(links)\n",
    "# for l in links[:20]:\n",
    "#     print(f\"{l['full_url']} - {l['start_idx']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156172b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes df, LINK_ITEM, LINK_LIST, and build_per_article_links are already defined in previous cells\n",
    "\n",
    "per_article = build_per_article_links(df, max_rows=None)\n",
    "df_out = df.join(per_article.select([\"id\", \"links\", \"link_count\"]), on=\"id\", how=\"left\")\n",
    "df_out.write_parquet(\"articles_fr_withLinks_withHyperlinks.parquet\")\n",
    "\n",
    "# Optional preview\n",
    "print(df_out.select([\"id\", \"title\", \"link_count\"]).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc89a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Assumes extract_links_with_pos, LINK_ITEM, LINK_LIST, and build_per_article_links(df, max_rows)\n",
    "# are already defined in previous cells\n",
    "\n",
    "def export_with_links_streaming(parquet_in: str, parquet_out: str, max_rows: int | None = None):\n",
    "    pf = pq.ParquetFile(parquet_in)\n",
    "    writer = None\n",
    "    processed = 0\n",
    "\n",
    "    for rg in range(pf.num_row_groups):\n",
    "        if max_rows is not None and processed >= max_rows:\n",
    "            break\n",
    "\n",
    "        table = pf.read_row_group(rg, columns=[\"id\", \"title\", \"text\"])\n",
    "        sub_df = pl.from_arrow(table).with_columns([\n",
    "            pl.col(\"id\").cast(pl.Int64),\n",
    "            pl.col(\"title\").cast(pl.Utf8).fill_null(\"\"),\n",
    "            pl.col(\"text\").cast(pl.Utf8).fill_null(\"\"),\n",
    "        ])\n",
    "\n",
    "        remaining = None if max_rows is None else max_rows - processed\n",
    "        if remaining is not None and sub_df.height > remaining:\n",
    "            sub_df = sub_df.slice(0, remaining)\n",
    "\n",
    "        per_article = build_per_article_links(sub_df, max_rows=None)\n",
    "        sub_out = sub_df.join(per_article.select([\"id\", \"links\", \"link_count\"]), on=\"id\", how=\"left\")\n",
    "\n",
    "        arrow_batch = sub_out.to_arrow()\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(parquet_out, arrow_batch.schema, compression=\"zstd\")\n",
    "        writer.write_table(arrow_batch)\n",
    "\n",
    "        processed += sub_df.height\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "# Example usage (streams input and produces output shards in-place):\n",
    "export_with_links_streaming(\n",
    "    parquet_in=\"articles_fr_withLinks.parquet\",\n",
    "    parquet_out=\"articles_fr_withLinks_withHyperlinks.parquet\",\n",
    "    max_rows=None  # or set an int cap to limit total processed rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6caff977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "df = pl.read_parquet(\"articles_fr_withoutLinks.parquet\")\n",
    "df2 = pl.read_parquet(\"articles_fr_withLinks_withHyperlinks.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0274e8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 5)\n",
      "┌─────┬─────────┬────────────────────────────┬────────────────────────┬────────────────────────────┐\n",
      "│ id  ┆ revid   ┆ url                        ┆ title                  ┆ text                       │\n",
      "│ --- ┆ ---     ┆ ---                        ┆ ---                    ┆ ---                        │\n",
      "│ i64 ┆ str     ┆ str                        ┆ str                    ┆ str                        │\n",
      "╞═════╪═════════╪════════════════════════════╪════════════════════════╪════════════════════════════╡\n",
      "│ 3   ┆ 316223  ┆ https://fr.wikipedia.org/w ┆ Antoine Meillet        ┆ Antoine Meillet, né le à   │\n",
      "│     ┆         ┆ iki?…                      ┆                        ┆ Mouli…                     │\n",
      "│ 7   ┆ 5256901 ┆ https://fr.wikipedia.org/w ┆ Algèbre linéaire       ┆ L’algèbre linéaire est la  │\n",
      "│     ┆         ┆ iki?…                      ┆                        ┆ bran…                      │\n",
      "│ 9   ┆ 1430140 ┆ https://fr.wikipedia.org/w ┆ Algèbre générale       ┆ L'algèbre générale, ou     │\n",
      "│     ┆         ┆ iki?…                      ┆                        ┆ algèbre…                   │\n",
      "│ 10  ┆ 1012    ┆ https://fr.wikipedia.org/w ┆ Algorithmique          ┆ L'algorithmique est        │\n",
      "│     ┆         ┆ iki?…                      ┆                        ┆ l'étude et…                │\n",
      "│ 11  ┆ 202320  ┆ https://fr.wikipedia.org/w ┆ Politique en Argentine ┆ L'Argentine est une        │\n",
      "│     ┆         ┆ iki?…                      ┆                        ┆ république…                │\n",
      "└─────┴─────────┴────────────────────────────┴────────────────────────┴────────────────────────────┘\n",
      "shape: (5, 5)\n",
      "┌─────┬────────────────────────┬───────────────────────────┬──────────────────────────┬────────────┐\n",
      "│ id  ┆ title                  ┆ text                      ┆ links                    ┆ link_count │\n",
      "│ --- ┆ ---                    ┆ ---                       ┆ ---                      ┆ ---        │\n",
      "│ i64 ┆ str                    ┆ str                       ┆ list[struct[5]]          ┆ u32        │\n",
      "╞═════╪════════════════════════╪═══════════════════════════╪══════════════════════════╪════════════╡\n",
      "│ 3   ┆ Antoine Meillet        ┆ Antoine Meillet, né le à  ┆ [{\"https://fr.wikipedia. ┆ 65         │\n",
      "│     ┆                        ┆ &lt;a…                    ┆ org/wi…                  ┆            │\n",
      "│ 7   ┆ Algèbre linéaire       ┆ L’algèbre linéaire est la ┆ [{\"https://fr.wikipedia. ┆ 111        │\n",
      "│     ┆                        ┆ bran…                     ┆ org/wi…                  ┆            │\n",
      "│ 9   ┆ Algèbre générale       ┆ L'&lt;a                   ┆ [{\"https://fr.wikipedia. ┆ 15         │\n",
      "│     ┆                        ┆ href=\"alg%C3%A8bre\"&gt…   ┆ org/wi…                  ┆            │\n",
      "│ 10  ┆ Algorithmique          ┆ L'algorithmique est       ┆ [{\"https://fr.wikipedia. ┆ 101        │\n",
      "│     ┆                        ┆ l'étude et…               ┆ org/wi…                  ┆            │\n",
      "│ 11  ┆ Politique en Argentine ┆ L'&lt;a                   ┆ [{\"https://fr.wikipedia. ┆ 78         │\n",
      "│     ┆                        ┆ href=\"Argentine\"&gt;Ar…   ┆ org/wi…                  ┆            │\n",
      "└─────┴────────────────────────┴───────────────────────────┴──────────────────────────┴────────────┘\n",
      "Schema([('id', Int64), ('revid', String), ('url', String), ('title', String), ('text', String)])\n",
      "Schema([('id', Int64), ('title', String), ('text', String), ('links', List(Struct({'full_url': String, 'start_idx': Int64, 'anchor': String, 'href_raw': String, 'href_decoded': String}))), ('link_count', UInt32)])\n",
      "(4498441, 5)\n",
      "(4498441, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df2.head())\n",
    "print(df.schema)\n",
    "print(df2.schema)\n",
    "print(df.shape)\n",
    "print(df2.shape)\n",
    "\n",
    "# Schema([('id', Int64), ('revid', String), ('url', String), ('title', String), ('text', String)])\n",
    "# Schema([('id', Int64), ('title', String), ('text', String), ('links', List(Struct({'full_url': String, 'start_idx': Int64, 'anchor': String, 'href_raw': String, 'href_decoded': String}))), ('link_count', UInt32)])\n",
    "\n",
    "# print(df.head()[\"links\"])\n",
    "# print(df.head()[\"text\"])\n",
    "\n",
    "\n",
    "# ┌─────┬─────────┬────────────────────────────┬────────────────────────┬────────────────────────────┐\n",
    "# │ id  ┆ revid   ┆ url                        ┆ title                  ┆ text                       │\n",
    "# │ --- ┆ ---     ┆ ---                        ┆ ---                    ┆ ---                        │\n",
    "# │ i64 ┆ str     ┆ str                        ┆ str                    ┆ str                        │\n",
    "# ╞═════╪═════════╪════════════════════════════╪════════════════════════╪════════════════════════════╡\n",
    "# │ 3   ┆ 316223  ┆ https://fr.wikipedia.org/w ┆ Antoine Meillet        ┆ Antoine Meillet, né le à   │\n",
    "# │     ┆         ┆ iki?…                      ┆                        ┆ Mouli…                     │\n",
    "# │ 7   ┆ 5256901 ┆ https://fr.wikipedia.org/w ┆ Algèbre linéaire       ┆ L’algèbre linéaire est la  │\n",
    "# │     ┆         ┆ iki?…                      ┆                        ┆ bran…                      │\n",
    "# │ 9   ┆ 1430140 ┆ https://fr.wikipedia.org/w ┆ Algèbre générale       ┆ L'algèbre générale, ou     │\n",
    "# │     ┆         ┆ iki?…                      ┆                        ┆ algèbre…                   │\n",
    "# │ 10  ┆ 1012    ┆ https://fr.wikipedia.org/w ┆ Algorithmique          ┆ L'algorithmique est        │\n",
    "# │     ┆         ┆ iki?…                      ┆                        ┆ l'étude et…                │\n",
    "# │ 11  ┆ 202320  ┆ https://fr.wikipedia.org/w ┆ Politique en Argentine ┆ L'Argentine est une        │\n",
    "# │     ┆         ┆ iki?…                      ┆                        ┆ république…                │\n",
    "# └─────┴─────────┴────────────────────────────┴────────────────────────┴────────────────────────────┘\n",
    "# shape: (5, 5)\n",
    "# ┌─────┬────────────────────────┬───────────────────────────┬──────────────────────────┬────────────┐\n",
    "# │ id  ┆ title                  ┆ text                      ┆ links                    ┆ link_count │\n",
    "# │ --- ┆ ---                    ┆ ---                       ┆ ---                      ┆ ---        │\n",
    "# │ i64 ┆ str                    ┆ str                       ┆ list[struct[5]]          ┆ u32        │\n",
    "# ╞═════╪════════════════════════╪═══════════════════════════╪══════════════════════════╪════════════╡\n",
    "# │ 3   ┆ Antoine Meillet        ┆ Antoine Meillet, né le à  ┆ [{\"https://fr.wikipedia. ┆ 65         │\n",
    "# │     ┆                        ┆ &lt;a…                    ┆ org/wi…                  ┆            │\n",
    "# ...\n",
    "# │     ┆                        ┆ l'étude et…               ┆ org/wi…                  ┆            │\n",
    "# │ 11  ┆ Politique en Argentine ┆ L'&lt;a                   ┆ [{\"https://fr.wikipedia. ┆ 78         │\n",
    "# │     ┆                        ┆ href=\"Argentine\"&gt;Ar…   ┆ org/wi…                  ┆            │\n",
    "# └─────┴────────────────────────┴───────────────────────────┴──────────────────────────┴────────────┘\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a548d363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner matches: 4498441 left-only: 0 right-only: 0\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "with_links = (\n",
    "    pl.scan_parquet(\"articles_fr_withLinks_withHyperlinks.parquet\")\n",
    "      .select([pl.col(\"id\").cast(pl.Int64), pl.col(\"title\"), pl.col(\"text\"), pl.col(\"links\"), pl.col(\"link_count\")])\n",
    ")\n",
    "without_links = (\n",
    "    pl.scan_parquet(\"articles_fr_withoutLinks.parquet\")\n",
    "      .select([pl.col(\"id\").cast(pl.Int64), pl.col(\"text\").alias(\"text_plain\")])\n",
    ")\n",
    "\n",
    "inner = (\n",
    "    with_links.join(without_links, on=\"id\", how=\"inner\")\n",
    "    .select(pl.len().alias(\"n\"))\n",
    "    .collect(engine=\"streaming\")[\"n\"][0]\n",
    ")\n",
    "left_unmatched = (\n",
    "    with_links.join(without_links, on=\"id\", how=\"anti\")\n",
    "    .select(pl.len().alias(\"n\"))\n",
    "    .collect(engine=\"streaming\")[\"n\"][0]\n",
    ")\n",
    "right_unmatched = (\n",
    "    without_links.join(with_links, on=\"id\", how=\"anti\")\n",
    "    .select(pl.len().alias(\"n\"))\n",
    "    .collect(engine=\"streaming\")[\"n\"][0]\n",
    ")\n",
    "\n",
    "print(\"inner matches:\", inner, \"left-only:\", left_unmatched, \"right-only:\", right_unmatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed4a9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10, 3)\n",
      "┌─────┬───────────────────────────────┬────────────┐\n",
      "│ id  ┆ title                         ┆ link_count │\n",
      "│ --- ┆ ---                           ┆ ---        │\n",
      "│ i64 ┆ str                           ┆ u32        │\n",
      "╞═════╪═══════════════════════════════╪════════════╡\n",
      "│ 3   ┆ Antoine Meillet               ┆ 65         │\n",
      "│ 7   ┆ Algèbre linéaire              ┆ 111        │\n",
      "│ 9   ┆ Algèbre générale              ┆ 15         │\n",
      "│ 10  ┆ Algorithmique                 ┆ 101        │\n",
      "│ 11  ┆ Politique en Argentine        ┆ 78         │\n",
      "│ 12  ┆ Armée républicaine irlandaise ┆ 2          │\n",
      "│ 15  ┆ Autriche                      ┆ 267        │\n",
      "│ 16  ┆ Arc de triomphe de l'Étoile   ┆ 191        │\n",
      "│ 18  ┆ Arsène Lupin                  ┆ 317        │\n",
      "│ 19  ┆ Algorithme                    ┆ 71         │\n",
      "└─────┴───────────────────────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "with_links_path = \"articles_fr_withLinks_withHyperlinks.parquet\"\n",
    "without_links_path = \"articles_fr_withoutLinks.parquet\"\n",
    "out_path = \"articles_fr_merged.parquet\"\n",
    "\n",
    "pf_with = pq.ParquetFile(with_links_path)\n",
    "pf_plain = pq.ParquetFile(without_links_path)\n",
    "\n",
    "n_with = pf_with.metadata.num_rows\n",
    "n_plain = pf_plain.metadata.num_rows\n",
    "if n_with != n_plain:\n",
    "    raise ValueError(f\"Row count mismatch: {n_with} vs {n_plain}\")\n",
    "\n",
    "batch_size = 200_000\n",
    "writer = None\n",
    "\n",
    "for b_with, b_plain in zip(\n",
    "    pf_with.iter_batches(batch_size=batch_size, columns=[\"id\", \"title\", \"text\", \"links\", \"link_count\"]),\n",
    "    pf_plain.iter_batches(batch_size=batch_size, columns=[\"text\"]),\n",
    "):\n",
    "    df_with = pl.from_arrow(b_with)\n",
    "    df_plain = pl.from_arrow(b_plain).rename({\"text\": \"text_withoutHref\"})\n",
    "    out = pl.concat([df_with, df_plain], how=\"horizontal\")\n",
    "    arrow_tbl = out.to_arrow()\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(out_path, arrow_tbl.schema, compression=\"zstd\")\n",
    "    writer.write_table(arrow_tbl)\n",
    "\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "\n",
    "print(pl.scan_parquet(out_path).select([\"id\", \"title\", \"link_count\"]).limit(10).collect(engine=\"streaming\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed096cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 6)\n",
      "┌─────┬───────────────────┬───────────────────┬───────────────────┬────────────┬───────────────────┐\n",
      "│ id  ┆ title             ┆ text              ┆ links             ┆ link_count ┆ text_withoutHref  │\n",
      "│ --- ┆ ---               ┆ ---               ┆ ---               ┆ ---        ┆ ---               │\n",
      "│ i64 ┆ str               ┆ str               ┆ list[struct[5]]   ┆ u32        ┆ str               │\n",
      "╞═════╪═══════════════════╪═══════════════════╪═══════════════════╪════════════╪═══════════════════╡\n",
      "│ 3   ┆ Antoine Meillet   ┆ Antoine Meillet,  ┆ [{\"https://fr.wik ┆ 65         ┆ Antoine Meillet,  │\n",
      "│     ┆                   ┆ né le à &lt;a…    ┆ ipedia.org/wi…    ┆            ┆ né le à Mouli…    │\n",
      "│ 7   ┆ Algèbre linéaire  ┆ L’algèbre         ┆ [{\"https://fr.wik ┆ 111        ┆ L’algèbre         │\n",
      "│     ┆                   ┆ linéaire est la   ┆ ipedia.org/wi…    ┆            ┆ linéaire est la   │\n",
      "│     ┆                   ┆ bran…             ┆                   ┆            ┆ bran…             │\n",
      "│ 9   ┆ Algèbre générale  ┆ L'&lt;a href=\"alg ┆ [{\"https://fr.wik ┆ 15         ┆ L'algèbre         │\n",
      "│     ┆                   ┆ %C3%A8bre\"&gt…    ┆ ipedia.org/wi…    ┆            ┆ générale, ou      │\n",
      "│     ┆                   ┆                   ┆                   ┆            ┆ algèbre…          │\n",
      "│ 10  ┆ Algorithmique     ┆ L'algorithmique   ┆ [{\"https://fr.wik ┆ 101        ┆ L'algorithmique   │\n",
      "│     ┆                   ┆ est l'étude et…   ┆ ipedia.org/wi…    ┆            ┆ est l'étude et…   │\n",
      "│ 11  ┆ Politique en      ┆ L'&lt;a href=\"Arg ┆ [{\"https://fr.wik ┆ 78         ┆ L'Argentine est   │\n",
      "│     ┆ Argentine         ┆ entine\"&gt;Ar…    ┆ ipedia.org/wi…    ┆            ┆ une république…   │\n",
      "└─────┴───────────────────┴───────────────────┴───────────────────┴────────────┴───────────────────┘\n",
      "(4498441, 6)\n",
      "Algèbre générale\n",
      "Angola\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import polars as pl\n",
    "df = pl.read_parquet(\"articles_fr_merged.parquet\")\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df[\"title\"][2])\n",
    "print(df[\"title\"][32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae800903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "with_links = (\n",
    "    pl.scan_parquet(\"articles_fr_withLinks_withHyperlinks.parquet\")\n",
    "      .select([\n",
    "          pl.col(\"id\").cast(pl.Int64),\n",
    "          pl.col(\"title\"),\n",
    "          pl.col(\"text\"),\n",
    "          pl.col(\"links\"),\n",
    "          pl.col(\"link_count\"),\n",
    "      ])\n",
    ")\n",
    "\n",
    "without_links = (\n",
    "    pl.scan_parquet(\"articles_fr_withoutLinks.parquet\")\n",
    "      .select([\n",
    "          pl.col(\"id\").cast(pl.Int64),\n",
    "          pl.col(\"text\").alias(\"text_plain\"),\n",
    "      ])\n",
    ")\n",
    "\n",
    "merged = with_links.join(without_links, on=\"id\", how=\"left\")\n",
    "try:\n",
    "    merged.sink_parquet(\"articles_fr_merged.parquet\")\n",
    "except AttributeError:\n",
    "    merged.collect(streaming=True).write_parquet(\"articles_fr_merged.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9476964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vector size from model\n",
    "vector_size = model.get_sentence_embedding_dimension()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning: only create the collection once ! Otherwise you will loose the content of it\n",
    "\n",
    "# collection_name = \"wikipedia_fr\"\n",
    "# if not client.collection_exists(collection_name):\n",
    "#     client.create_collection(\n",
    "#         collection_name=collection_name,\n",
    "#         vectors_config=VectorParams(size=vector_size, distance=\"Cosine\"),\n",
    "#     )\n",
    "# else:\n",
    "#     print(f\"Collection '{collection_name}' already exists; skipping creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3c93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GO from here to load new articles into Qdrant Onwards\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "#load file from parquet (if you saved it once previously)\n",
    "pf = pq.ParquetFile(\"articles_fr_withLinks.parquet\")\n",
    "rowgroup_batch = 512\n",
    "encode_batch = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77fb711a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles stored: 30208\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http import models as rest\n",
    "#get articles count from Qdrant DB \n",
    "\n",
    "collection_name = \"wikipedia_fr\"\n",
    "total = client.count(collection_name=collection_name, exact=True).count\n",
    "print(f\"Total articles stored: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde4aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recover a snapshot from the Qdrant db (should not be needed)\n",
    "client.recover_snapshot(collection_name=\"wikipedia_fr\", location=\"http://localhost:6333/collections/wikipedia_fr/snapshots/<snapshot_name>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ee6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Process Articles into the Qdrant DB (warning: takes shit ton of time and ressources)\n",
    "\n",
    "ARTICLES_NUMBER=4000\n",
    "\n",
    "new_processed = 0\n",
    "max_rows = ARTICLES_NUMBER\n",
    "\n",
    "for rg in range(pf.num_row_groups):\n",
    "    if max_rows is not None and new_processed >= max_rows:\n",
    "        break\n",
    "    table = pf.read_row_group(rg, columns=[\"id\", \"title\", \"url\", \"text\"])\n",
    "    df = pl.from_arrow(table).with_columns([\n",
    "        pl.col(\"id\").cast(pl.Int64),\n",
    "        pl.col(\"title\").fill_null(\"\"),\n",
    "        pl.col(\"url\").fill_null(\"\"),\n",
    "        pl.col(\"text\").fill_null(\"\")\n",
    "    ])\n",
    "    n = df.height\n",
    "    remaining = None if max_rows is None else max_rows - new_processed\n",
    "    for start in range(0, n, rowgroup_batch):\n",
    "        length = min(rowgroup_batch, n - start)\n",
    "        if remaining is not None:\n",
    "            length = min(length, remaining)\n",
    "        sub = df.slice(start, length)\n",
    "        ids = sub[\"id\"].to_list()\n",
    "        texts = sub[\"text\"].to_list()\n",
    "        titles = sub[\"title\"].to_list()\n",
    "        urls = sub[\"url\"].to_list()\n",
    "\n",
    "        existing = client.retrieve(\n",
    "            collection_name=\"wikipedia_fr\",\n",
    "            ids=[int(i) for i in ids],\n",
    "            with_payload=False,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        existing_ids = {p.id for p in existing}\n",
    "        missing_idx = [i for i, pid in enumerate(ids) if int(pid) not in existing_ids]\n",
    "        if not missing_idx:\n",
    "            continue\n",
    "\n",
    "        ids_m = [ids[i] for i in missing_idx]\n",
    "        texts_m = [texts[i] for i in missing_idx]\n",
    "        titles_m = [titles[i] for i in missing_idx]\n",
    "        urls_m = [urls[i] for i in missing_idx]\n",
    "\n",
    "        vectors = model.encode(texts_m, batch_size=encode_batch, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=int(ids_m[i]),\n",
    "                vector=vectors[i].tolist(),\n",
    "                payload={\"id\": int(ids_m[i]), \"title\": titles_m[i], \"url\": urls_m[i], \"text\": texts_m[i]},\n",
    "            )\n",
    "            for i in range(len(ids_m))\n",
    "        ]\n",
    "        client.upsert(collection_name=\"wikipedia_fr\", points=points, wait=True)\n",
    "        new_processed += len(ids_m)\n",
    "        if max_rows is not None and new_processed >= max_rows:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aafc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "import polars as pl\n",
    "from typing import Optional\n",
    "\n",
    "#Query Qdrant DB to rebuild the polars df (warning: untested)\n",
    "\n",
    "def rebuild_df_from_qdrant(\n",
    "    client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    max_rows: Optional[int] = 1000,\n",
    "    batch_limit: int = 1000,\n",
    ") -> pl.DataFrame:\n",
    "    rows = []\n",
    "    next_page = None\n",
    "\n",
    "    selector = rest.PayloadSelectorInclude(include=[\"id\", \"title\", \"url\", \"text\"])\n",
    "\n",
    "    while True:\n",
    "        points, next_page = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=batch_limit,\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "            payload_selector=selector,\n",
    "            offset=next_page,\n",
    "        )\n",
    "        if not points:\n",
    "            break\n",
    "\n",
    "        for p in points:\n",
    "            pid = p.payload.get(\"id\", p.id)\n",
    "            rows.append({\n",
    "                \"id\": pid,\n",
    "                \"title\": p.payload.get(\"title\", \"\"),\n",
    "                \"url\": p.payload.get(\"url\", \"\"),\n",
    "                \"text\": p.payload.get(\"text\", \"\"),\n",
    "            })\n",
    "            if max_rows is not None and len(rows) >= max_rows:\n",
    "                break\n",
    "\n",
    "        if max_rows is not None and len(rows) >= max_rows:\n",
    "            break\n",
    "        if next_page is None:\n",
    "            break\n",
    "\n",
    "    df = pl.DataFrame(rows).with_columns([\n",
    "        pl.col(\"id\").cast(pl.Int64),\n",
    "        pl.col(\"title\").cast(pl.Utf8).fill_null(\"\"),\n",
    "        pl.col(\"url\").cast(pl.Utf8).fill_null(\"\"),\n",
    "        pl.col(\"text\").cast(pl.Utf8).fill_null(\"\"),\n",
    "    ])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b36e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top matches for 'Math':\n",
      "• Nombre entier (score=0.824)\n",
      "• Série (mathématiques) (score=0.818)\n",
      "• Distance (mathématiques) (score=0.813)\n",
      "• Nombre (score=0.812)\n",
      "• Géométrie arithmétique (score=0.811)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1272/1232709101.py:11: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "#Query DB with one keyword for cosine similarity (warning : NANs issues gives 1.0 score)\n",
    "\n",
    "query_title = \"Math\"\n",
    "\n",
    "qdf = df.filter(pl.col(\"title\").str.to_lowercase() == query_title.lower())\n",
    "query_text = qdf.select(\"text\").to_series()[0] if qdf.height > 0 else query_title\n",
    "\n",
    "# E5 requires the 'query:' prefix for queries; documents were encoded as 'passage:' during ingestion\n",
    "query_vector = model.encode([f\"query: {query_text}\"], normalize_embeddings=True)[0]\n",
    "\n",
    "results = client.search(\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    query_vector=query_vector,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(f\"\\n🔍 Top matches for '{query_title}':\")\n",
    "for r in results:\n",
    "    print(f\"• {r.payload.get('title')} (score={r.score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f510b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Similar articles for: Atome (id=189)\n",
      "  • Proton (id=2414, score=0.939)\n",
      "  • Liaison chimique (id=22131, score=0.939)\n",
      "  • Élément chimique (id=15349, score=0.936)\n",
      "  • Électron (id=6716, score=0.935)\n",
      "  • Particule élémentaire (id=23547, score=0.934)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33189/278556868.py:25: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.recommend(collection_name=\"wikipedia_fr\", positive=[int(article_id)], limit=k)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Function to encode whole article text into vector \n",
    "# Then query the db with the article embedding to get the k most similar articles\n",
    "\n",
    "def encode_article_text(text):\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return np.zeros(model.get_sentence_embedding_dimension(), dtype=np.float32)\n",
    "    chunks = [\" \".join(words[i:i+256]) for i in range(0, len(words), 256)]\n",
    "    vecs = model.encode([f\"passage: {c}\" for c in chunks], normalize_embeddings=True)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "mode = \"ids\"  # \"all\" or \"ids\"\n",
    "ids_to_process = [189, 205]\n",
    "k = 5\n",
    "\n",
    "ids_all = df.select(pl.col(\"id\").cast(pl.Int64)).to_series().to_list()\n",
    "ids = ids_all if mode == \"all\" else [i for i in ids_to_process if i in set(ids_all)]\n",
    "\n",
    "for article_id in ids:\n",
    "    title_row = df.filter(pl.col(\"id\") == int(article_id)).select(\"title\")\n",
    "    title = title_row.to_series()[0] if title_row.height > 0 else \"(unknown)\"\n",
    "    try:\n",
    "        points = client.retrieve(collection_name=\"wikipedia_fr\", ids=[int(article_id)], with_payload=True, with_vectors=False)\n",
    "        if points:\n",
    "            results = client.recommend(collection_name=\"wikipedia_fr\", positive=[int(article_id)], limit=k)\n",
    "        else:\n",
    "            raise ValueError(\"missing\")\n",
    "    except Exception:\n",
    "        row = df.filter(pl.col(\"id\") == int(article_id))\n",
    "        if row.height == 0:\n",
    "            continue\n",
    "        text = row.select(pl.col(\"text\").fill_null(\"\")).to_series()[0]\n",
    "        vec = encode_article_text(text)\n",
    "        results = client.search(collection_name=\"wikipedia_fr\", query_vector=vec, limit=k)\n",
    "    print(f\"\\n🔎 Similar articles for: {title} (id={int(article_id)})\")\n",
    "    for r in results:\n",
    "        print(f\"  • {r.payload.get('title')} (id={r.payload.get('id')}, score={r.score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a718c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "#Export articles to csv for further use\n",
    "\n",
    "cols = [\"id\", \"title\", \"url\", \"text\"]\n",
    "df.select(cols).write_csv(\"articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30563b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as pacsv\n",
    "\n",
    "pf = pq.ParquetFile(\"articles_fr_withLinks.parquet\")\n",
    "cols = [\"id\", \"title\", \"url\", \"text\"]\n",
    "ARTICLES_NUMBER = 100000\n",
    "processed = 0\n",
    "first = True\n",
    "output_path = \"articles_stream.csv\"\n",
    "\n",
    "for rg in range(pf.num_row_groups):\n",
    "    if ARTICLES_NUMBER is not None and processed >= ARTICLES_NUMBER:\n",
    "        break\n",
    "    table = pf.read_row_group(rg, columns=cols)\n",
    "    df_rg = pl.from_arrow(table).with_columns([\n",
    "        pl.col(\"id\").cast(pl.Int64),\n",
    "        pl.col(\"title\").fill_null(\"\"),\n",
    "        pl.col(\"url\").fill_null(\"\"),\n",
    "        pl.col(\"text\").fill_null(\"\")\n",
    "    ])\n",
    "    remaining = None if ARTICLES_NUMBER is None else ARTICLES_NUMBER - processed\n",
    "    if remaining is not None and df_rg.height > remaining:\n",
    "        df_rg = df_rg.slice(0, remaining)\n",
    "    mode = \"wb\" if first else \"ab\"\n",
    "    with open(output_path, mode) as f:\n",
    "        pacsv.write_csv(df_rg.to_arrow(), f, write_options=pacsv.WriteOptions(include_header=first))\n",
    "    processed += df_rg.height\n",
    "    first = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8816b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def export_points_df(df: pl.DataFrame, client, collection_name: str, model, encode_batch: int = 64, upsert_batch: int = 512, max_rows: int | None = None):\n",
    "    n = df.height if max_rows is None else min(df.height, max_rows)\n",
    "    processed = 0\n",
    "    while processed < n:\n",
    "        length = min(upsert_batch, n - processed)\n",
    "        sub = df.slice(processed, length)\n",
    "        ids = sub[\"id\"].cast(pl.Int64).to_list()\n",
    "        titles = sub[\"title\"].fill_null(\"\").to_list()\n",
    "        urls = sub[\"url\"].fill_null(\"\").to_list()\n",
    "        texts = sub[\"text\"].fill_null(\"\").to_list()\n",
    "        vectors = model.encode(texts, batch_size=encode_batch, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=int(ids[i]),\n",
    "                vector=vectors[i].tolist(),\n",
    "                payload={\"id\": int(ids[i]), \"title\": titles[i], \"url\": urls[i], \"text\": texts[i]},\n",
    "            )\n",
    "            for i in range(len(ids))\n",
    "        ]\n",
    "        client.upsert(collection_name=collection_name, points=points, wait=True)\n",
    "        processed += length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93730d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def export_points_parquet(parquet_path: str, client, collection_name: str, model, encode_batch: int = 64, upsert_batch: int = 512, max_rows: int | None = None):\n",
    "    pf = pq.ParquetFile(parquet_path)\n",
    "    processed = 0\n",
    "    for rg in range(pf.num_row_groups):\n",
    "        if max_rows is not None and processed >= max_rows:\n",
    "            break\n",
    "        table = pf.read_row_group(rg, columns=[\"id\", \"title\", \"url\", \"text\"])\n",
    "        df = pl.from_arrow(table).with_columns([\n",
    "            pl.col(\"id\").cast(pl.Int64),\n",
    "            pl.col(\"title\").fill_null(\"\"),\n",
    "            pl.col(\"url\").fill_null(\"\"),\n",
    "            pl.col(\"text\").fill_null(\"\")\n",
    "        ])\n",
    "        remaining = None if max_rows is None else max_rows - processed\n",
    "        n = df.height if remaining is None else min(df.height, remaining)\n",
    "        inner = 0\n",
    "        while inner < n:\n",
    "            length = min(upsert_batch, n - inner)\n",
    "            sub = df.slice(inner, length)\n",
    "            ids = sub[\"id\"].to_list()\n",
    "            titles = sub[\"title\"].to_list()\n",
    "            urls = sub[\"url\"].to_list()\n",
    "            texts = sub[\"text\"].to_list()\n",
    "            vectors = model.encode(texts, batch_size=encode_batch, convert_to_numpy=True, normalize_embeddings=True)\n",
    "            points = [\n",
    "                PointStruct(\n",
    "                    id=int(ids[i]),\n",
    "                    vector=vectors[i].tolist(),\n",
    "                    payload={\"id\": int(ids[i]), \"title\": titles[i], \"url\": urls[i], \"text\": texts[i]},\n",
    "                )\n",
    "                for i in range(len(ids))\n",
    "            ]\n",
    "            client.upsert(collection_name=collection_name, points=points, wait=True)\n",
    "            processed += length\n",
    "            inner += length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7302a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b598be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from qdrant_client.http import models as rest\n",
    "from typing import Optional\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def export_similarity_graph(\n",
    "    client,\n",
    "    collection_name: str = \"wikipedia_fr\",\n",
    "    output_path: str = \"similarity_links.csv\",\n",
    "    k: int = 10,\n",
    "    max_articles: Optional[int] = None,\n",
    "    batch_size: int = 1000,\n",
    "    resume_from_id: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Export k-NN similarity graph from Qdrant database efficiently.\n",
    "    \n",
    "    Args:\n",
    "        client: Qdrant client\n",
    "        collection_name: Name of the collection\n",
    "        output_path: CSV output file path\n",
    "        k: Number of similar articles to find for each article\n",
    "        max_articles: Maximum number of articles to process (None for all)\n",
    "        batch_size: Batch size for scrolling through collection\n",
    "        resume_from_id: Resume from specific article ID (for interrupted runs)\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Starting similarity graph export...\")\n",
    "    print(f\"   Collection: {collection_name}\")\n",
    "    print(f\"   Output: {output_path}\")\n",
    "    print(f\"   k-NN: {k}\")\n",
    "    print(f\"   Max articles: {max_articles or 'All'}\")\n",
    "    \n",
    "    # Open CSV file for writing\n",
    "    mode = 'a' if resume_from_id else 'w'\n",
    "    with open(output_path, mode, newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write header only if starting fresh\n",
    "        if not resume_from_id:\n",
    "            writer.writerow(['source_id', 'target_id', 'score'])\n",
    "        \n",
    "        # Iterator to get all article IDs from Qdrant\n",
    "        processed_count = 0\n",
    "        next_page = None\n",
    "        \n",
    "        # Skip to resume point if specified\n",
    "        if resume_from_id:\n",
    "            print(f\"   Resuming from article ID: {resume_from_id}\")\n",
    "        \n",
    "        while True:\n",
    "            # Scroll through collection to get article IDs\n",
    "            points, next_page = client.scroll(\n",
    "                collection_name=collection_name,\n",
    "                limit=batch_size,\n",
    "                with_payload=True,\n",
    "                with_vectors=False,\n",
    "                offset=next_page,\n",
    "            )\n",
    "            \n",
    "            if not points:\n",
    "                break\n",
    "            \n",
    "            # Process each article in the batch\n",
    "            for point in tqdm(points, desc=f\"Processing batch (total: {processed_count})\"):\n",
    "                source_id = point.payload.get(\"id\", point.id)\n",
    "                \n",
    "                # Skip if resuming and haven't reached resume point\n",
    "                if resume_from_id and int(source_id) < resume_from_id:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Use Qdrant's recommend API for efficient similarity search\n",
    "                    # This leverages the stored vectors without re-encoding\n",
    "                    similar_results = client.recommend(\n",
    "                        collection_name=collection_name,\n",
    "                        positive=[int(source_id)],\n",
    "                        limit=k + 1,  # +1 because it includes the source article\n",
    "                        with_payload=True\n",
    "                    )\n",
    "                    \n",
    "                    # Write similarity edges to CSV\n",
    "                    for result in similar_results:\n",
    "                        target_id = result.payload.get(\"id\", result.id)\n",
    "                        \n",
    "                        # Skip self-similarity\n",
    "                        if int(target_id) == int(source_id):\n",
    "                            continue\n",
    "                        \n",
    "                        # Write edge: source_id -> target_id with similarity score\n",
    "                        writer.writerow([source_id, target_id, f\"{result.score:.6f}\"])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ⚠️  Error processing article {source_id}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "                # Check if we've reached the maximum\n",
    "                if max_articles and processed_count >= max_articles:\n",
    "                    print(f\"\\n✅ Reached maximum articles limit: {max_articles}\")\n",
    "                    return processed_count\n",
    "                \n",
    "                # Flush every 100 articles for safety\n",
    "                if processed_count % 100 == 0:\n",
    "                    csvfile.flush()\n",
    "            \n",
    "            # Break if no more pages\n",
    "            if next_page is None:\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n✅ Similarity graph export completed!\")\n",
    "    print(f\"   Processed articles: {processed_count:,}\")\n",
    "    print(f\"   Output file: {output_path}\")\n",
    "    \n",
    "    return processed_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8fd8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_similarity_graph_hybrid(\n",
    "    client,\n",
    "    df: pl.DataFrame,\n",
    "    collection_name: str = \"wikipedia_fr\",\n",
    "    output_path: str = \"similarity_links_hybrid.csv\",\n",
    "    k: int = 10,\n",
    "    max_articles: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Export similarity graph using hybrid approach:\n",
    "    - Use Qdrant recommend API for articles in the database\n",
    "    - Use encode_article_text + search for articles not in database\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Starting hybrid similarity export...\")\n",
    "    \n",
    "    # Get list of article IDs to process\n",
    "    article_ids = df.select(\"id\").to_series().to_list()\n",
    "    if max_articles:\n",
    "        article_ids = article_ids[:max_articles]\n",
    "    \n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['source_id', 'target_id', 'score'])\n",
    "        \n",
    "        for i, article_id in enumerate(tqdm(article_ids, desc=\"Processing articles\")):\n",
    "            try:\n",
    "                # Try Qdrant recommend API first (faster)\n",
    "                points = client.retrieve(\n",
    "                    collection_name=collection_name, \n",
    "                    ids=[int(article_id)], \n",
    "                    with_payload=True, \n",
    "                    with_vectors=False\n",
    "                )\n",
    "                \n",
    "                if points:\n",
    "                    # Article exists in Qdrant - use recommend API\n",
    "                    results = client.recommend(\n",
    "                        collection_name=collection_name,\n",
    "                        positive=[int(article_id)],\n",
    "                        limit=k + 1\n",
    "                    )\n",
    "                else:\n",
    "                    # Article not in Qdrant - encode and search\n",
    "                    row = df.filter(pl.col(\"id\") == int(article_id))\n",
    "                    if row.height == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    text = row.select(pl.col(\"text_withoutHref\").fill_null(\"\")).to_series()[0]\n",
    "                    vec = encode_article_text(text)\n",
    "                    results = client.search(\n",
    "                        collection_name=collection_name, \n",
    "                        query_vector=vec, \n",
    "                        limit=k\n",
    "                    )\n",
    "                \n",
    "                # Write results\n",
    "                for result in results:\n",
    "                    target_id = result.payload.get(\"id\", result.id)\n",
    "                    if int(target_id) != int(article_id):  # Skip self\n",
    "                        writer.writerow([article_id, target_id, f\"{result.score:.6f}\"])\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {article_id}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Flush periodically\n",
    "            if i % 100 == 0:\n",
    "                csvfile.flush()\n",
    "    \n",
    "    print(f\"✅ Hybrid export completed: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f998e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hyperlink_graph(df: pl.DataFrame, output_path: str = \"hyperlink_graph.csv\"):\n",
    "    \"\"\"\n",
    "    Extract the actual hyperlink graph from Wikipedia articles.\n",
    "    This creates the ground truth graph of existing links.\n",
    "    \"\"\"\n",
    "    print(\"🔗 Extracting hyperlink graph from articles...\")\n",
    "    \n",
    "    # Create a mapping from Wikipedia URLs to article IDs\n",
    "    url_to_id = {}\n",
    "    for row in df.select([\"id\", \"title\"]).iter_rows(named=True):\n",
    "        # Create potential Wikipedia URL patterns for this article\n",
    "        title = row[\"title\"].replace(\" \", \"_\")\n",
    "        url_patterns = [\n",
    "            title,\n",
    "            title.replace(\"_\", \"%20\"),\n",
    "            title.replace(\"_\", \" \")\n",
    "        ]\n",
    "        for pattern in url_patterns:\n",
    "            url_to_id[pattern] = row[\"id\"]\n",
    "    \n",
    "    hyperlinks = []\n",
    "    \n",
    "    for row in tqdm(df.iter_rows(named=True), total=len(df), desc=\"Processing articles\"):\n",
    "        source_id = row[\"id\"]\n",
    "        links = row.get(\"links\", []) or []\n",
    "        \n",
    "        for link in links:\n",
    "            href_decoded = link.get(\"href_decoded\", \"\")\n",
    "            \n",
    "            # Try to map the link to an article ID\n",
    "            target_id = None\n",
    "            \n",
    "            # Direct lookup\n",
    "            if href_decoded in url_to_id:\n",
    "                target_id = url_to_id[href_decoded]\n",
    "            else:\n",
    "                # Try variations\n",
    "                for pattern in [href_decoded.replace(\"_\", \" \"), href_decoded.replace(\"%20\", \" \")]:\n",
    "                    if pattern in url_to_id:\n",
    "                        target_id = url_to_id[pattern]\n",
    "                        break\n",
    "            \n",
    "            if target_id and target_id != source_id:\n",
    "                hyperlinks.append({\n",
    "                    \"source_id\": source_id,\n",
    "                    \"target_id\": target_id,\n",
    "                    \"anchor_text\": link.get(\"anchor\", \"\"),\n",
    "                    \"href_decoded\": href_decoded\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    hyperlink_df = pl.DataFrame(hyperlinks)\n",
    "    hyperlink_df.write_csv(output_path)\n",
    "    \n",
    "    print(f\"✅ Extracted {len(hyperlink_df):,} hyperlinks\")\n",
    "    print(f\"   Unique source articles: {hyperlink_df.select('source_id').n_unique():,}\")\n",
    "    print(f\"   Unique target articles: {hyperlink_df.select('target_id').n_unique():,}\")\n",
    "    print(f\"   Output: {output_path}\")\n",
    "    \n",
    "    return hyperlink_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad7de3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting similarity graph export...\n",
      "   Collection: wikipedia_fr\n",
      "   Output: similarity_links_qdrant.csv\n",
      "   k-NN: 10\n",
      "   Max articles: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch (total: 0):   0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_829/3566637825.py:76: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  similar_results = client.recommend(\n",
      "Processing batch (total: 0): 100%|██████████| 100/100 [00:04<00:00, 20.79it/s]\n",
      "Processing batch (total: 100): 100%|██████████| 100/100 [00:02<00:00, 33.94it/s]\n",
      "Processing batch (total: 200): 100%|██████████| 100/100 [00:03<00:00, 27.63it/s]\n",
      "Processing batch (total: 300): 100%|██████████| 100/100 [00:03<00:00, 28.49it/s]\n",
      "Processing batch (total: 400): 100%|██████████| 100/100 [00:02<00:00, 43.09it/s]\n",
      "Processing batch (total: 500): 100%|██████████| 100/100 [00:02<00:00, 43.78it/s]\n",
      "Processing batch (total: 600): 100%|██████████| 100/100 [00:02<00:00, 43.66it/s]\n",
      "Processing batch (total: 700): 100%|██████████| 100/100 [00:02<00:00, 45.18it/s]\n",
      "Processing batch (total: 800): 100%|██████████| 100/100 [00:02<00:00, 39.30it/s]\n",
      "Processing batch (total: 900):  99%|█████████▉| 99/100 [00:02<00:00, 45.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Reached maximum articles limit: 1000\n",
      "\n",
      "📊 Exported 1,000 articles with their similarity links\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Export similarity graph from Qdrant (fastest)\n",
    "processed = export_similarity_graph(\n",
    "    client=client,\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    output_path=\"similarity_links_qdrant.csv\",\n",
    "    k=10,  # Find 10 most similar articles for each\n",
    "    max_articles=1000,  # Start with 1000 articles for testing, set to None for all\n",
    "    batch_size=100,\n",
    "    resume_from_id=None  # Set to an ID to resume interrupted runs\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Exported {processed:,} articles with their similarity links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb22a054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4,498,441 articles\n",
      "🔄 Starting hybrid similarity export...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:   0%|          | 0/500 [00:00<?, ?it/s]/tmp/ipykernel_829/4227211400.py:37: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.recommend(\n",
      "Processing articles: 100%|██████████| 500/500 [00:23<00:00, 21.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hybrid export completed: similarity_links_hybrid_test.csv\n",
      "shape: (10, 3)\n",
      "┌───────────┬───────────┬──────────┐\n",
      "│ source_id ┆ target_id ┆ score    │\n",
      "│ ---       ┆ ---       ┆ ---      │\n",
      "│ i64       ┆ i64       ┆ f64      │\n",
      "╞═══════════╪═══════════╪══════════╡\n",
      "│ 3         ┆ 37999     ┆ 0.904169 │\n",
      "│ 3         ┆ 251       ┆ 0.901769 │\n",
      "│ 3         ┆ 13418     ┆ 0.901407 │\n",
      "│ 3         ┆ 13779     ┆ 0.901042 │\n",
      "│ 3         ┆ 12434     ┆ 0.900641 │\n",
      "│ 3         ┆ 13419     ┆ 0.900595 │\n",
      "│ 3         ┆ 34222     ┆ 0.89858  │\n",
      "│ 3         ┆ 1647      ┆ 0.896724 │\n",
      "│ 3         ┆ 23162     ┆ 0.896583 │\n",
      "│ 3         ┆ 41558     ┆ 0.895497 │\n",
      "└───────────┴───────────┴──────────┘\n",
      "\n",
      "Total similarity edges: 5,500\n",
      "Unique source articles: 500\n",
      "Average edges per article: 11.0\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Hybrid approach for all articles\n",
    "# Load your merged DataFrame if not already loaded\n",
    "df = pl.read_parquet(\"articles_fr_merged.parquet\")\n",
    "print(f\"Loaded {len(df):,} articles\")\n",
    "\n",
    "# Run hybrid export for a subset first (testing)\n",
    "export_similarity_graph_hybrid(\n",
    "    client=client,\n",
    "    df=df.head(500),  # Start with 500 articles for testing\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    output_path=\"similarity_links_hybrid_test.csv\",\n",
    "    k=10,\n",
    "    max_articles=500\n",
    ")\n",
    "\n",
    "# Preview results\n",
    "sim_df = pl.read_csv(\"similarity_links_hybrid_test.csv\")\n",
    "print(sim_df.head(10))\n",
    "print(f\"\\nTotal similarity edges: {len(sim_df):,}\")\n",
    "print(f\"Unique source articles: {sim_df.select('source_id').n_unique()}\")\n",
    "print(f\"Average edges per article: {len(sim_df) / sim_df.select('source_id').n_unique():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e2f5c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Extracting hyperlink graph from articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 1000/1000 [00:00<00:00, 3455.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 5,383 hyperlinks\n",
      "   Unique source articles: 653\n",
      "   Unique target articles: 510\n",
      "   Output: hyperlink_graph.csv\n",
      "\n",
      "📊 Hyperlink graph preview:\n",
      "shape: (10, 4)\n",
      "┌───────────┬───────────┬───────────────────────┬───────────────────────┐\n",
      "│ source_id ┆ target_id ┆ anchor_text           ┆ href_decoded          │\n",
      "│ ---       ┆ ---       ┆ ---                   ┆ ---                   │\n",
      "│ i64       ┆ i64       ┆ str                   ┆ str                   │\n",
      "╞═══════════╪═══════════╪═══════════════════════╪═══════════════════════╡\n",
      "│ 3         ┆ 33        ┆ Allier                ┆ Allier (département)  │\n",
      "│ 3         ┆ 548       ┆ Cher                  ┆ Cher (département)    │\n",
      "│ 3         ┆ 33        ┆ bourbonnaise          ┆ Allier (département)  │\n",
      "│ 3         ┆ 548       ┆ Cher                  ┆ Cher (département)    │\n",
      "│ 3         ┆ 1106      ┆ Ferdinand de Saussure ┆ Ferdinand de Saussure │\n",
      "│ 3         ┆ 53        ┆ Arménie               ┆ Arménie               │\n",
      "│ 3         ┆ 1054      ┆ Émile Benveniste      ┆ Émile Benveniste      │\n",
      "│ 3         ┆ 251       ┆ Aurélien Sauvageot    ┆ Aurélien Sauvageot    │\n",
      "│ 3         ┆ 1106      ┆ Ferdinand de Saussure ┆ Ferdinand de Saussure │\n",
      "│ 3         ┆ 487       ┆ Bosnie-Herzégovine    ┆ Bosnie-Herzégovine    │\n",
      "└───────────┴───────────┴───────────────────────┴───────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Extract hyperlink graph\n",
    "hyperlink_graph = extract_hyperlink_graph(df.head(1000))  # Start with subset\n",
    "print(\"\\n📊 Hyperlink graph preview:\")\n",
    "print(hyperlink_graph.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b54b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Graph Analysis:\n",
      "Similarity edges: 5,500\n",
      "Hyperlink edges: 5,383\n",
      "Overlapping edges: 402\n",
      "Overlap percentage: 7.5%\n",
      "Potential missing links: 5,319\n"
     ]
    }
   ],
   "source": [
    "# Compare similarity vs hyperlink graphs\n",
    "def analyze_graphs(similarity_path: str, hyperlink_path: str):\n",
    "    \"\"\"Compare semantic similarity graph with actual hyperlink graph\"\"\"\n",
    "    \n",
    "    sim_df = pl.read_csv(similarity_path)\n",
    "    hyper_df = pl.read_csv(hyperlink_path)\n",
    "    \n",
    "    print(\"📊 Graph Analysis:\")\n",
    "    print(f\"Similarity edges: {len(sim_df):,}\")\n",
    "    print(f\"Hyperlink edges: {len(hyper_df):,}\")\n",
    "    \n",
    "    # Find overlap - edges that exist in both graphs\n",
    "    sim_edges = sim_df.select([\"source_id\", \"target_id\"]).with_columns(\n",
    "        pl.concat_str([pl.col(\"source_id\"), pl.col(\"target_id\")], separator=\"_\").alias(\"edge\")\n",
    "    )\n",
    "    hyper_edges = hyper_df.select([\"source_id\", \"target_id\"]).with_columns(\n",
    "        pl.concat_str([pl.col(\"source_id\"), pl.col(\"target_id\")], separator=\"_\").alias(\"edge\")\n",
    "    )\n",
    "    \n",
    "    overlap = sim_edges.join(hyper_edges, on=\"edge\", how=\"inner\")\n",
    "    print(f\"Overlapping edges: {len(overlap):,}\")\n",
    "    print(f\"Overlap percentage: {len(overlap) / len(hyper_edges) * 100:.1f}%\")\n",
    "    \n",
    "    # Find potential missing links (high similarity but no hyperlink)\n",
    "    missing_links = sim_edges.join(hyper_edges, on=\"edge\", how=\"anti\")\n",
    "    print(f\"Potential missing links: {len(missing_links):,}\")\n",
    "    \n",
    "    return {\n",
    "        \"similarity_df\": sim_df,\n",
    "        \"hyperlink_df\": hyper_df,\n",
    "        \"overlap_df\": overlap,\n",
    "        \"missing_links_df\": missing_links\n",
    "    }\n",
    "\n",
    "# Run analysis\n",
    "results = analyze_graphs(\"similarity_links_hybrid_test.csv\", \"hyperlink_graph.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f978377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Hyperlink Graph with Article Titles:\n",
      "================================================================================\n",
      " 1. 'Antoine Meillet' (ID: 3)\n",
      "    └─→ 'Allier (département)' (ID: 33)\n",
      "        Anchor: 'Allier'\n",
      "\n",
      " 2. 'Antoine Meillet' (ID: 3)\n",
      "    └─→ 'Cher (département)' (ID: 548)\n",
      "        Anchor: 'Cher'\n",
      "\n",
      " 3. 'Antoine Meillet' (ID: 3)\n",
      "    └─→ 'Allier (département)' (ID: 33)\n",
      "        Anchor: 'bourbonnaise'\n",
      "\n",
      " 4. 'Antoine Meillet' (ID: 3)\n",
      "    └─→ 'Cher (département)' (ID: 548)\n",
      "        Anchor: 'Cher'\n",
      "\n",
      " 5. 'Antoine Meillet' (ID: 3)\n",
      "    └─→ 'Ferdinand de Saussure' (ID: 1106)\n",
      "        Anchor: 'Ferdinand de Saussure'\n",
      "\n",
      " 6. 'Antoine Meillet' (ID: 3)\n",
      "    └─→ 'Arménie' (ID: 53)\n",
      "        Anchor: 'Arménie'\n",
      "\n",
      " 7. 'Antoine Meillet' (ID: 3)\n",
      "    └─→ 'Émile Benveniste' (ID: 1054)\n",
      "        Anchor: 'Émile Benveniste'\n",
      "\n",
      " 8. 'Antoine Meillet' (ID: 3)\n",
      "    └─→ 'Aurélien Sauvageot' (ID: 251)\n",
      "        Anchor: 'Aurélien Sauvageot'\n",
      "\n",
      " 9. 'Antoine Meillet' (ID: 3)\n",
      "    └─→ 'Ferdinand de Saussure' (ID: 1106)\n",
      "        Anchor: 'Ferdinand de Saussure'\n",
      "\n",
      "10. 'Antoine Meillet' (ID: 3)\n",
      "    └─→ 'Bosnie-Herzégovine' (ID: 487)\n",
      "        Anchor: 'Bosnie-Herzégovine'\n",
      "\n",
      "11. 'Algèbre linéaire' (ID: 7)\n",
      "    └─→ 'Espace vectoriel' (ID: 1051)\n",
      "        Anchor: 'espaces vectoriels'\n",
      "\n",
      "12. 'Algèbre linéaire' (ID: 7)\n",
      "    └─→ 'Espace vectoriel' (ID: 1051)\n",
      "        Anchor: 'espaces vectoriels'\n",
      "\n",
      "13. 'Algèbre générale' (ID: 9)\n",
      "    └─→ 'Andrew Wiles' (ID: 284)\n",
      "        Anchor: 'Andrew Wiles'\n",
      "\n",
      "14. 'Algorithmique' (ID: 10)\n",
      "    └─→ 'Recherche dichotomique' (ID: 842)\n",
      "        Anchor: 'dichotomie'\n",
      "\n",
      "15. 'Algorithmique' (ID: 10)\n",
      "    └─→ 'Informatique' (ID: 1493)\n",
      "        Anchor: 'sciences informatique'\n",
      "\n",
      "\n",
      "📊 Summary:\n",
      "Total hyperlinks: 5,383\n",
      "Unique source articles: 653\n",
      "Unique target articles: 510\n",
      "\n",
      "🎯 Most Referenced Articles:\n",
      "  • 'France' (ID: 1095) - 375 incoming links\n",
      "  • 'Europe' (ID: 1009) - 194 incoming links\n",
      "  • 'Japon' (ID: 1593) - 155 incoming links\n",
      "  • 'François Mitterrand' (ID: 1126) - 141 incoming links\n",
      "  • 'Italie' (ID: 1492) - 138 incoming links\n",
      "  • 'Inde' (ID: 1508) - 105 incoming links\n",
      "  • 'Belgique' (ID: 385) - 78 incoming links\n",
      "  • 'Afrique' (ID: 181) - 75 incoming links\n",
      "  • 'Brésil' (ID: 398) - 65 incoming links\n",
      "  • 'Argentine' (ID: 42) - 64 incoming links\n",
      "\n",
      "📤 Articles with Most Outgoing Links:\n",
      "  • 'Divinités égyptiennes' (ID: 865) - 129 outgoing links\n",
      "  • 'Europe' (ID: 1009) - 81 outgoing links\n",
      "  • 'Coupe du monde de football' (ID: 1200) - 80 outgoing links\n",
      "  • 'France' (ID: 1095) - 79 outgoing links\n",
      "  • 'Horus' (ID: 1383) - 77 outgoing links\n",
      "  • 'Afrique' (ID: 181) - 73 outgoing links\n",
      "  • 'Élection présidentielle française de 1988' (ID: 1209) - 70 outgoing links\n",
      "  • 'Brésil' (ID: 398) - 66 outgoing links\n",
      "  • 'Estonie' (ID: 1047) - 65 outgoing links\n",
      "  • 'Anubis' (ID: 132) - 49 outgoing links\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def print_hyperlink_graph_with_titles(hyperlink_path: str, df: pl.DataFrame, num_rows: int = 20):\n",
    "    \"\"\"\n",
    "    Print hyperlink graph with actual article titles instead of just IDs\n",
    "    \"\"\"\n",
    "    # Load hyperlink graph\n",
    "    hyper_df = pl.read_csv(hyperlink_path)\n",
    "    \n",
    "    # Create ID to title mapping for fast lookup\n",
    "    id_to_title = dict(zip(df.select(\"id\").to_series(), df.select(\"title\").to_series()))\n",
    "    \n",
    "    print(\"🔗 Hyperlink Graph with Article Titles:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, row in enumerate(hyper_df.head(num_rows).iter_rows(named=True)):\n",
    "        source_id = row[\"source_id\"]\n",
    "        target_id = row[\"target_id\"]\n",
    "        anchor_text = row[\"anchor_text\"]\n",
    "        \n",
    "        source_title = id_to_title.get(source_id, f\"Unknown ID {source_id}\")\n",
    "        target_title = id_to_title.get(target_id, f\"Unknown ID {target_id}\")\n",
    "        \n",
    "        print(f\"{i+1:2d}. '{source_title}' (ID: {source_id})\")\n",
    "        print(f\"    └─→ '{target_title}' (ID: {target_id})\")\n",
    "        print(f\"        Anchor: '{anchor_text}'\")\n",
    "        print()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"Total hyperlinks: {len(hyper_df):,}\")\n",
    "    print(f\"Unique source articles: {hyper_df.select('source_id').n_unique():,}\")\n",
    "    print(f\"Unique target articles: {hyper_df.select('target_id').n_unique():,}\")\n",
    "    \n",
    "    # Most linked-to articles\n",
    "    print(f\"\\n🎯 Most Referenced Articles:\")\n",
    "    top_targets = (hyper_df\n",
    "                   .group_by(\"target_id\")\n",
    "                   .len()\n",
    "                   .sort(\"len\", descending=True)\n",
    "                   .head(10))\n",
    "    \n",
    "    for row in top_targets.iter_rows(named=True):\n",
    "        target_id = row[\"target_id\"]\n",
    "        count = row[\"len\"]\n",
    "        title = id_to_title.get(target_id, f\"Unknown ID {target_id}\")\n",
    "        print(f\"  • '{title}' (ID: {target_id}) - {count} incoming links\")\n",
    "    \n",
    "    # Most linking articles\n",
    "    print(f\"\\n📤 Articles with Most Outgoing Links:\")\n",
    "    top_sources = (hyper_df\n",
    "                   .group_by(\"source_id\")\n",
    "                   .len()\n",
    "                   .sort(\"len\", descending=True)\n",
    "                   .head(10))\n",
    "    \n",
    "    for row in top_sources.iter_rows(named=True):\n",
    "        source_id = row[\"source_id\"]\n",
    "        count = row[\"len\"]\n",
    "        title = id_to_title.get(source_id, f\"Unknown ID {source_id}\")\n",
    "        print(f\"  • '{title}' (ID: {source_id}) - {count} outgoing links\")\n",
    "\n",
    "# Usage\n",
    "print_hyperlink_graph_with_titles(\"hyperlink_graph.csv\", df, num_rows=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef87ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_article_links(df: pl.DataFrame, article_id: int, hyperlink_path: str):\n",
    "    \"\"\"\n",
    "    Explore all outgoing and incoming links for a specific article\n",
    "    \"\"\"\n",
    "    hyper_df = pl.read_csv(hyperlink_path)\n",
    "    id_to_title = dict(zip(df.select(\"id\").to_series(), df.select(\"title\").to_series()))\n",
    "    \n",
    "    article_title = id_to_title.get(article_id, f\"Unknown ID {article_id}\")\n",
    "    print(f\"🔍 Exploring links for: '{article_title}' (ID: {article_id})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Outgoing links (what this article links to)\n",
    "    outgoing = hyper_df.filter(pl.col(\"source_id\") == article_id)\n",
    "    print(f\"\\n📤 Outgoing Links ({len(outgoing)}):\")\n",
    "    for row in outgoing.iter_rows(named=True):\n",
    "        target_title = id_to_title.get(row[\"target_id\"], f\"Unknown ID {row['target_id']}\")\n",
    "        print(f\"  → '{target_title}' (anchor: '{row['anchor_text']}')\")\n",
    "    \n",
    "    # Incoming links (what articles link to this one)\n",
    "    incoming = hyper_df.filter(pl.col(\"target_id\") == article_id)\n",
    "    print(f\"\\n📥 Incoming Links ({len(incoming)}):\")\n",
    "    for row in incoming.iter_rows(named=True):\n",
    "        source_title = id_to_title.get(row[\"source_id\"], f\"Unknown ID {row['source_id']}\")\n",
    "        print(f\"  ← '{source_title}' (anchor: '{row['anchor_text']}')\")\n",
    "\n",
    "# Example: Explore links for article ID 3 (Antoine Meillet from your data)\n",
    "explore_article_links(df, 3, \"hyperlink_graph.csv\")\n",
    "\n",
    "# Example: Explore links for article ID 15 (Autriche from your data)\n",
    "explore_article_links(df, 15, \"hyperlink_graph.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
