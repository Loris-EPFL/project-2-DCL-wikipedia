{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55eaadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()  # loads .env into environment variables\n",
    "login(os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Connect to Qdrant\n",
    "# ----------------------------\n",
    "# Option 1: In-memory (no Docker)\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# # Option 2: Local Qdrant server\n",
    "# client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Initialize model\n",
    "# ----------------------------\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "model.save(\"models/multilingual-e5-large\") #save model locally (do only once)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c9a80",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'articles_fr_withLinks.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m data_path = Path(\u001b[33m\"\u001b[39m\u001b[33m./articles_fr_withLinks.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m rows = []\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m i >= \u001b[32m100\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/cs-433/lib/python3.13/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'articles_fr_withLinks.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Load first 100 articles from JSON\n",
    "# ----------------------------\n",
    "data_path = Path(\"../wikiextractor/articles_fr_withLinks.json\")\n",
    "\n",
    "rows = []\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 100:\n",
    "            break\n",
    "        try:\n",
    "            rows.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Ensure id/title/url/text columns exist and are well-typed\n",
    "df[\"id\"] = df[\"id\"].astype(int)            # use numeric point IDs\n",
    "df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "df[\"url\"] = df[\"url\"].fillna(\"\")\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")\n",
    "\n",
    "print(f\"âœ… Loaded {len(df)} articles into a DataFrame\")\n",
    "print(df[[\"id\", \"title\"]].head(10))\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Create collection\n",
    "# ----------------------------\n",
    "vector_size = model.get_sentence_embedding_dimension()\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    vectors_config=VectorParams(size=vector_size, distance=\"Cosine\"),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b706b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¤ Upserted 100 points with payload to Qdrant\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Step 4: Ingest articles into Qdrant with payload\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "import numpy as np\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def encode_article_text(text, chunk_size_words=256):\n",
    "    # Chunk long articles and mean-pool embeddings (E5 doc prefix: 'passage:')\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+chunk_size_words]) for i in range(0, len(words), chunk_size_words)]\n",
    "    if not chunks:\n",
    "        chunks = [\"\"]\n",
    "    prefixed = [f\"passage: {chunk}\" for chunk in chunks]\n",
    "    vecs = model.encode(prefixed, normalize_embeddings=True)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "points = []\n",
    "for row in df.itertuples(index=False):\n",
    "    vec = encode_article_text(row.text)\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=int(row.id),          # point id == article id\n",
    "            vector=vec,\n",
    "            payload={\n",
    "                \"id\": int(row.id),\n",
    "                \"title\": row.title,\n",
    "                \"url\": row.url\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "client.upsert(collection_name=\"wikipedia_fr\", points=points)\n",
    "print(f\"ðŸ“¤ Upserted {len(points)} points with payload to Qdrant\")\n",
    "# ... existing code ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b36e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top matches for 'Math':\n",
      "â€¢ AlgÃ¨bre de Boole (logique) (score=0.804)\n",
      "â€¢ AlgÃ¨bre gÃ©nÃ©rale (score=0.787)\n",
      "â€¢ Algorithmique (score=0.785)\n",
      "â€¢ AlgÃ¨bre linÃ©aire (score=0.785)\n",
      "â€¢ Algorithme (score=0.781)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/x7v4d0vs261d025y0qz66bdh0000gn/T/ipykernel_64326/433745059.py:14: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Query similar articles\n",
    "# ----------------------------\n",
    "query_title = \"Math\"\n",
    "\n",
    "query_row = df[df[\"title\"].str.lower() == query_title.lower()]\n",
    "if not query_row.empty:\n",
    "    query_text = query_row.iloc[0][\"text\"]\n",
    "else:\n",
    "    query_text = query_title\n",
    "\n",
    "query_vector = model.encode([query_text], normalize_embeddings=True)[0]\n",
    "\n",
    "results = client.search(\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    query_vector=query_vector,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ” Top matches for '{query_title}':\")\n",
    "for r in results:\n",
    "    print(f\"â€¢ {r.payload['title']} (score={r.score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af26d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top matches for 'Math':\n",
      "â€¢ AlgÃ¨bre de Boole (logique) (score=0.818)\n",
      "â€¢ AlgÃ¨bre linÃ©aire (score=0.804)\n",
      "â€¢ AbrÃ©viations en informatique H (score=0.804)\n",
      "â€¢ Aichi (score=0.804)\n",
      "â€¢ AndrÃ© Marie AmpÃ¨re (score=0.804)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/x7v4d0vs261d025y0qz66bdh0000gn/T/ipykernel_64326/1095570288.py:25: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Query similar articles\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "import numpy as np\n",
    "\n",
    "def build_query_vector_from_article_text(text, model, chunk_size_words=256):\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+chunk_size_words]) for i in range(0, len(words), chunk_size_words)]\n",
    "    prefixed = [f\"query: {chunk}\" for chunk in chunks]\n",
    "    vecs = model.encode(prefixed, normalize_embeddings=True)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "query_title = \"Math\"\n",
    "query_row = df[df[\"title\"].str.lower() == query_title.lower()]\n",
    "\n",
    "if not query_row.empty:\n",
    "    # Use whole article text (chunked) for the query vector\n",
    "    query_text = query_row.iloc[0][\"text\"]\n",
    "    query_vector = build_query_vector_from_article_text(query_text, model)\n",
    "else:\n",
    "    # Fallback: use the title as plain query\n",
    "    query_vector = model.encode([f\"query: {query_title}\"], normalize_embeddings=True)[0]\n",
    "\n",
    "results = client.search(\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    query_vector=query_vector,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ” Top matches for '{query_title}':\")\n",
    "for r in results:\n",
    "    print(f\"â€¢ {r.payload['title']} (score={r.score:.3f})\")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb8233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top matches for 'Math':\n",
      "â€¢ AlgÃ¨bre de Boole (logique) (score=0.818)\n",
      "â€¢ AlgÃ¨bre linÃ©aire (score=0.804)\n",
      "â€¢ AbrÃ©viations en informatique H (score=0.804)\n",
      "â€¢ Aichi (score=0.804)\n",
      "â€¢ AndrÃ© Marie AmpÃ¨re (score=0.804)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/x7v4d0vs261d025y0qz66bdh0000gn/T/ipykernel_64326/340894359.py:21: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.search(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Query similar articles\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "from qdrant_client.http import models as rest\n",
    "\n",
    "query_title = \"Math\"\n",
    "query_row = df[df[\"title\"].str.lower() == query_title.lower()]\n",
    "\n",
    "if not query_row.empty:\n",
    "    # Assume point id in Qdrant equals the article 'id' stored in df\n",
    "    article_id = int(query_row.iloc[0][\"id\"])\n",
    "    results = client.recommend(\n",
    "        collection_name=\"wikipedia_fr\",\n",
    "        positive=[article_id],\n",
    "        limit=5\n",
    "    )\n",
    "else:\n",
    "    # Fallback to text-based search using query prefix\n",
    "    query_vector = model.encode([f\"query: {query_title}\"], normalize_embeddings=True)[0]\n",
    "    results = client.search(\n",
    "        collection_name=\"wikipedia_fr\",\n",
    "        query_vector=query_vector,\n",
    "        limit=5\n",
    "    )\n",
    "\n",
    "print(f\"\\nðŸ” Top matches for '{query_title}':\")\n",
    "for r in results:\n",
    "    print(f\"â€¢ {r.payload['title']} (score={r.score:.3f})\")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5866b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top matches for 'Aichi':\n",
      "â€¢ AbrÃ©viations en informatique H (score=1.000)\n",
      "â€¢ AbrÃ©viations en informatique A (score=1.000)\n",
      "â€¢ Amenophis IV (score=1.000)\n",
      "â€¢ AbrÃ©viations en informatique N (score=1.000)\n",
      "â€¢ AbrÃ©viations en informatique C (score=1.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/x7v4d0vs261d025y0qz66bdh0000gn/T/ipykernel_64326/495915137.py:39: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = client.recommend(\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Query similar articles (by full article content)\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "import numpy as np\n",
    "from qdrant_client.http import models as rest\n",
    "\n",
    "def build_query_vector_from_article_text(text, model, chunk_size_words=256):\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+chunk_size_words]) for i in range(0, len(words), chunk_size_words)]\n",
    "    # E5 best practice: prefix queries with \"query: \"\n",
    "    prefixed = [f\"query: {chunk}\" for chunk in chunks]\n",
    "    vecs = model.encode(prefixed, normalize_embeddings=True)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def get_point_id_by_title(client, collection_name, title):\n",
    "    # Find the stored article vector by title in payload\n",
    "    points, _ = client.scroll(\n",
    "        collection_name=collection_name,\n",
    "        scroll_filter=rest.Filter(\n",
    "            must=[rest.FieldCondition(key=\"title\", match=rest.MatchValue(value=title))]\n",
    "        ),\n",
    "        limit=1,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    if points:\n",
    "        return points[0].id\n",
    "    return None\n",
    "\n",
    "collection_name = \"wikipedia_fr\"\n",
    "query_title = \"Aichi\"  # set this to the article title you want as the query\n",
    "\n",
    "# Try to recommend by the stored article point in Qdrant\n",
    "point_id = get_point_id_by_title(client, collection_name, query_title)\n",
    "\n",
    "if point_id is not None:\n",
    "    # Article found in Qdrant â†’ use recommend (nearest neighbors by the article's own vector)\n",
    "    results = client.recommend(\n",
    "        collection_name=collection_name,\n",
    "        positive=[point_id],\n",
    "        limit=5\n",
    "    )\n",
    "else:\n",
    "    # Fallback: encode full article text locally and search by that vector\n",
    "    query_row = df[df[\"title\"].str.lower() == query_title.lower()]\n",
    "    if query_row.empty:\n",
    "        raise ValueError(f\"Article titled '{query_title}' not found in DataFrame; please choose an existing title.\")\n",
    "    query_text = query_row.iloc[0][\"text\"]\n",
    "    query_vector = build_query_vector_from_article_text(query_text, model)\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=5\n",
    "    )\n",
    "\n",
    "print(f\"\\nðŸ” Top matches for '{query_title}':\")\n",
    "for r in results:\n",
    "    print(f\"â€¢ {r.payload['title']} (score={r.score:.3f})\")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Similar articles for: AlgÃ¨bre linÃ©aire (id=7)\n",
      "  â€¢ AlgÃ¨bre gÃ©nÃ©rale (id=9, score=0.962)\n",
      "  â€¢ Algorithmique (id=10, score=0.958)\n",
      "  â€¢ Astronomie (id=64, score=0.953)\n",
      "  â€¢ AlgÃ¨bre de Boole (logique) (id=24, score=0.950)\n",
      "  â€¢ Algorithme (id=19, score=0.945)\n",
      "  â€¢ AntiquitÃ© (id=123, score=0.936)\n",
      "  â€¢ Assistant personnel (id=124, score=0.936)\n",
      "  â€¢ Anthropologie (id=152, score=0.935)\n",
      "  â€¢ Antoine de Saint-ExupÃ©ry (id=97, score=0.928)\n",
      "  â€¢ Asie (id=159, score=0.928)\n",
      "\n",
      "ðŸ”Ž Similar articles for: AlgÃ¨bre gÃ©nÃ©rale (id=9)\n",
      "  â€¢ AlgÃ¨bre linÃ©aire (id=7, score=0.962)\n",
      "  â€¢ Algorithmique (id=10, score=0.946)\n",
      "  â€¢ Astronomie (id=64, score=0.936)\n",
      "  â€¢ Algorithme (id=19, score=0.932)\n",
      "  â€¢ AlgÃ¨bre de Boole (logique) (id=24, score=0.931)\n",
      "  â€¢ Anthropologie (id=152, score=0.919)\n",
      "  â€¢ AntiquitÃ© (id=123, score=0.919)\n",
      "  â€¢ Assistant personnel (id=124, score=0.917)\n",
      "  â€¢ Asie (id=159, score=0.908)\n",
      "  â€¢ Apple (id=63, score=0.908)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/x7v4d0vs261d025y0qz66bdh0000gn/T/ipykernel_64326/2584421462.py:46: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  return client.recommend(collection_name=collection_name, positive=[point_id], limit=k)\n",
      "/var/folders/6x/x7v4d0vs261d025y0qz66bdh0000gn/T/ipykernel_64326/2584421462.py:46: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  return client.recommend(collection_name=collection_name, positive=[point_id], limit=k)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Article-to-article similarity by full content\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "import numpy as np\n",
    "from qdrant_client.http import models as rest\n",
    "\n",
    "def encode_article_text(text, model, chunk_size_words=256):\n",
    "    \"\"\"Encode full article by chunking, using E5 'passage:' prefix and mean-pooling.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+chunk_size_words]) for i in range(0, len(words), chunk_size_words)]\n",
    "    prefixed = [f\"passage: {chunk}\" for chunk in chunks]\n",
    "    vecs = model.encode(prefixed, normalize_embeddings=True)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "def find_point_id_by_payload_id(client, collection_name, article_id):\n",
    "    \"\"\"Locate the stored point by payload 'id'.\"\"\"\n",
    "    points, next_page = client.scroll(\n",
    "        collection_name=collection_name,\n",
    "        scroll_filter=rest.Filter(\n",
    "            must=[rest.FieldCondition(key=\"id\", match=rest.MatchValue(value=str(article_id)))]\n",
    "        ),\n",
    "        limit=1,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    if points:\n",
    "        return points[0].id\n",
    "    # Also try numeric payload ids, if stored as int\n",
    "    points, next_page = client.scroll(\n",
    "        collection_name=collection_name,\n",
    "        scroll_filter=rest.Filter(\n",
    "            must=[rest.FieldCondition(key=\"id\", match=rest.MatchValue(value=int(article_id)))]\n",
    "        ),\n",
    "        limit=1,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    )\n",
    "    return points[0].id if points else None\n",
    "\n",
    "def get_similar_articles_by_id(client, collection_name, article_id, k=10):\n",
    "    \"\"\"Nearest neighbors via Qdrant recommend, using the stored vector.\"\"\"\n",
    "    point_id = find_point_id_by_payload_id(client, collection_name, article_id)\n",
    "    if point_id is None:\n",
    "        return None\n",
    "    return client.recommend(collection_name=collection_name, positive=[point_id], limit=k)\n",
    "\n",
    "def get_similar_articles_by_text(client, collection_name, text, model, k=10):\n",
    "    \"\"\"Fallback: nearest neighbors by encoding full article text.\"\"\"\n",
    "    vec = encode_article_text(text, model)\n",
    "    return client.search(collection_name=collection_name, query_vector=vec, limit=k)\n",
    "\n",
    "def get_title_by_id(article_id: int) -> str:\n",
    "    # Try DataFrame first\n",
    "    row = df[df[\"id\"].astype(str) == str(article_id)]\n",
    "    if not row.empty and pd.notna(row.iloc[0][\"title\"]):\n",
    "        return str(row.iloc[0][\"title\"])\n",
    "    # Fallback: try Qdrant payload\n",
    "    try:\n",
    "        pt = client.retrieve(\n",
    "            collection_name=collection_name,\n",
    "            ids=[int(article_id)],\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        if pt and pt[0].payload and pt[0].payload.get(\"title\"):\n",
    "            return str(pt[0].payload[\"title\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"(unknown title)\"\n",
    "\n",
    "collection_name = \"wikipedia_fr\"\n",
    "\n",
    "# Example: article-to-article neighbors for two articles\n",
    "for query_article_id in [7, 9]:\n",
    "    query_title = get_title_by_id(int(query_article_id))\n",
    "    print(f\"\\nðŸ”Ž Similar articles for: {query_title} (id={query_article_id})\")\n",
    "    results = get_similar_articles_by_id(client, collection_name, query_article_id, k=10)\n",
    "\n",
    "    if results is None:\n",
    "        # Fallback to local encoding if the article vector is not stored in Qdrant\n",
    "        row = df[df[\"id\"].astype(str) == str(query_article_id)]\n",
    "        if row.empty:\n",
    "            print(f\"  â€¢ Article id={query_article_id} not found in DataFrame.\")\n",
    "            continue\n",
    "        text = row.iloc[0][\"text\"]\n",
    "        results = get_similar_articles_by_text(client, collection_name, text, model, k=10)\n",
    "\n",
    "    for r in results:\n",
    "        print(f\"  â€¢ {r.payload.get('title')} (id={r.payload.get('id')}, score={r.score:.3f})\")\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816b818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported recomputed links to recomputed_similarity_links.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6x/x7v4d0vs261d025y0qz66bdh0000gn/T/ipykernel_64326/2584421462.py:46: DeprecationWarning: `recommend` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  return client.recommend(collection_name=collection_name, positive=[point_id], limit=k)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Step 6: Export recomputed similarity links (top-k neighbors)\n",
    "# ----------------------------\n",
    "# ... existing code ...\n",
    "import csv\n",
    "\n",
    "def iter_all_point_ids(client, collection_name, batch_size=1024):\n",
    "    \"\"\"Yield all point IDs in the collection (without vectors).\"\"\"\n",
    "    next_page = None\n",
    "    while True:\n",
    "        points, next_page = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=batch_size,\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "            offset=next_page\n",
    "        )\n",
    "        if not points:\n",
    "            break\n",
    "        for p in points:\n",
    "            # Prefer payload 'id' if present, else use point id\n",
    "            payload_id = p.payload.get(\"id\")\n",
    "            yield payload_id if payload_id is not None else p.id\n",
    "        if next_page is None:\n",
    "            break\n",
    "\n",
    "def export_similarity_links(client, collection_name, k, out_csv_path):\n",
    "    \"\"\"For each article, record top-k similar neighbors to CSV.\"\"\"\n",
    "    with open(out_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"source_id\", \"target_id\", \"score\"])\n",
    "        count = 0\n",
    "        for source_id in iter_all_point_ids(client, collection_name):\n",
    "            # Resolve to actual point ID if payload_id is used\n",
    "            neighbors = get_similar_articles_by_id(client, collection_name, source_id, k=k)\n",
    "            if neighbors is None:\n",
    "                # Skip if the source article is missing as a stored vector\n",
    "                continue\n",
    "            for r in neighbors:\n",
    "                target_id = r.payload.get(\"id\", r.id)\n",
    "                if str(target_id) == str(source_id):\n",
    "                    continue\n",
    "                writer.writerow([source_id, target_id, f\"{r.score:.6f}\"])\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(f\"Processed {count} articles...\")\n",
    "    print(f\"âœ… Exported recomputed links to {out_csv_path}\")\n",
    "\n",
    "# Generate a 10-NN hyperlink graph (adjust k as needed)\n",
    "export_similarity_links(client, collection_name=\"wikipedia_fr\", k=10, out_csv_path=\"recomputed_similarity_links.csv\")\n",
    "# ... existing code ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs-433",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
