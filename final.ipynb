{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bbf8254",
   "metadata": {},
   "source": [
    "# üéØ Wikipedia Link Prediction - Complete Pipeline\n",
    "\n",
    "Production-ready pipeline for predicting Wikipedia internal links.\n",
    "\n",
    "**Components:**\n",
    "- Fast phrase extraction (n-grams matching anchor dictionary)\n",
    "- V6 linkability-based scoring (F1 ‚âà 0.52)\n",
    "- Semantic disambiguation for ambiguous anchors\n",
    "- Wikipedia-style HTML output for comparison\n",
    "\n",
    "**Usage:**\n",
    "1. Run all cells to initialize\n",
    "2. Evaluate on test set\n",
    "3. Test on specific articles\n",
    "4. Generate side-by-side HTML comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e16c9",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d158c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import html\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Set, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import requests\n",
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d55399",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Pipeline configuration - best parameters from experiments.\"\"\"\n",
    "    # Paths\n",
    "    parquet_path: str = \"articles_fr_merged.parquet\"\n",
    "    mapping_cache: str = \"url_to_id_mapping.pkl\"\n",
    "    anchor_cache: str = \"anchor_dictionary.pkl\"\n",
    "    \n",
    "    # Qdrant\n",
    "    qdrant_host: str = \"localhost\"\n",
    "    qdrant_port: int = 6333\n",
    "    collection_name: str = \"wikipedia_fr\"\n",
    "    \n",
    "    # Model\n",
    "    embedding_model: str = \"intfloat/multilingual-e5-large\"\n",
    "    \n",
    "    # Scoring (V6 best params)\n",
    "    score_threshold: float = 3.5\n",
    "    max_links_per_article: int = 500\n",
    "    \n",
    "    # Disambiguation\n",
    "    use_semantic_disambiguation: bool = True\n",
    "    disambiguation_threshold: int = 5\n",
    "    \n",
    "    # Extraction\n",
    "    max_ngram: int = 5\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ WIKIPEDIA LINK PREDICTION PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Device: {CONFIG.device}\")\n",
    "print(f\"   Threshold: {CONFIG.score_threshold}\")\n",
    "print(f\"   Disambiguation: {CONFIG.use_semantic_disambiguation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bad4fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Stop words - function words that should never be linked\n",
    "STOP_WORDS = frozenset({\n",
    "    'le', 'la', 'les', 'un', 'une', 'des', 'du', 'de', 'au', 'aux',\n",
    "    'ce', 'cette', 'ces', 'mon', 'ma', 'mes', 'ton', 'ta', 'tes',\n",
    "    'son', 'sa', 'ses', 'notre', 'nos', 'votre', 'vos', 'leur', 'leurs',\n",
    "    'je', 'tu', 'il', 'elle', 'on', 'nous', 'vous', 'ils', 'elles',\n",
    "    'qui', 'que', 'quoi', 'dont', 'o√π', 'et', 'ou', 'mais', 'donc',\n",
    "    'car', 'ni', 'si', 'dans', 'pour', 'par', 'sur', 'avec', 'sans',\n",
    "    'sous', 'entre', 'est', 'sont', 'a', 'ont', '√©t√©', '√™tre', 'avoir',\n",
    "    '√† la suite', 'au cours', 'au sein', 'en effet', 'par exemple',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f9879",
   "metadata": {},
   "source": [
    "## 2. Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6512c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LinkPrediction:\n",
    "    \"\"\"A predicted link.\"\"\"\n",
    "    phrase: str\n",
    "    target_id: int\n",
    "    target_title: str\n",
    "    score: float\n",
    "    probability: float\n",
    "    linkability: int\n",
    "    num_targets: int\n",
    "    start_pos: int = -1\n",
    "    end_pos: int = -1\n",
    "    semantic_score: float = 0.0\n",
    "\n",
    "@dataclass \n",
    "class PipelineMetrics:\n",
    "    \"\"\"Evaluation metrics.\"\"\"\n",
    "    precision: float = 0.0\n",
    "    recall: float = 0.0\n",
    "    f1: float = 0.0\n",
    "    tp: int = 0\n",
    "    fp: int = 0\n",
    "    fn: int = 0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"P={self.precision:.3f} R={self.recall:.3f} F1={self.f1:.3f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea19b60",
   "metadata": {},
   "source": [
    "## 3. Main Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561f8dc2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class WikipediaLinkPredictor:\n",
    "    \"\"\"\n",
    "    Production Wikipedia link prediction pipeline.\n",
    "    \n",
    "    Pipeline stages:\n",
    "    1. Phrase extraction (n-grams matching anchor dict)\n",
    "    2. Candidate scoring (V6 linkability-based)\n",
    "    3. Semantic disambiguation (for ambiguous anchors)\n",
    "    4. First-occurrence filtering\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config = CONFIG):\n",
    "        self.config = config\n",
    "        self._load_resources()\n",
    "    \n",
    "    def _load_resources(self):\n",
    "        \"\"\"Load all required resources.\"\"\"\n",
    "        print(\"\\nüì¶ Loading resources...\")\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Qdrant\n",
    "        self.client = QdrantClient(\n",
    "            host=self.config.qdrant_host,\n",
    "            port=self.config.qdrant_port,\n",
    "            prefer_grpc=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        print(\"   ‚úÖ Qdrant connected\")\n",
    "        \n",
    "        # Mappings\n",
    "        with open(self.config.mapping_cache, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        self.url_to_id = data['url_to_id']\n",
    "        self.id_to_title = data['id_to_title']\n",
    "        print(f\"   ‚úÖ Mappings: {len(self.id_to_title):,} articles\")\n",
    "        \n",
    "        # Anchor dictionary\n",
    "        with open(self.config.anchor_cache, 'rb') as f:\n",
    "            self.anchor_dict = pickle.load(f)\n",
    "        self.anchor_keys = frozenset(self.anchor_dict.keys())\n",
    "        \n",
    "        # Precompute totals and linkability\n",
    "        self.anchor_totals = {}\n",
    "        self.phrase_linkability = {}\n",
    "        for phrase, targets in self.anchor_dict.items():\n",
    "            total = sum(targets.values())\n",
    "            self.anchor_totals[phrase] = total\n",
    "            self.phrase_linkability[phrase] = total\n",
    "        print(f\"   ‚úÖ Anchors: {len(self.anchor_dict):,} phrases\")\n",
    "        \n",
    "        # Lazy load model\n",
    "        self._model = None\n",
    "        \n",
    "        print(f\"   ‚úÖ Loaded in {time.time()-t0:.1f}s\")\n",
    "    \n",
    "    @property\n",
    "    def model(self):\n",
    "        \"\"\"Lazy load embedding model.\"\"\"\n",
    "        if self._model is None:\n",
    "            print(\"   Loading embedding model...\")\n",
    "            self._model = SentenceTransformer(\n",
    "                self.config.embedding_model,\n",
    "                device=self.config.device\n",
    "            )\n",
    "            print(f\"   ‚úÖ Model loaded on {self.config.device}\")\n",
    "        return self._model\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PHRASE EXTRACTION\n",
    "    # ========================================================================\n",
    "    \n",
    "    def extract_phrases(self, text: str) -> List[Tuple[str, int, int]]:\n",
    "        \"\"\"Extract all phrases that exist in anchor dictionary.\"\"\"\n",
    "        words = re.findall(r'\\b[\\w\\-\\'√†√¢√§√©√®√™√´√Ø√Æ√¥√π√ª√º√ß≈ì√¶]+\\b', text.lower())\n",
    "        word_positions = [\n",
    "            (m.start(), m.end()) \n",
    "            for m in re.finditer(r'\\b[\\w\\-\\'√†√¢√§√©√®√™√´√Ø√Æ√¥√π√ª√º√ß≈ì√¶]+\\b', text.lower())\n",
    "        ]\n",
    "        \n",
    "        phrases = []\n",
    "        seen = set()\n",
    "        \n",
    "        # Longest first for better coverage\n",
    "        for n in range(self.config.max_ngram, 0, -1):\n",
    "            for i in range(len(words) - n + 1):\n",
    "                ngram = ' '.join(words[i:i+n])\n",
    "                \n",
    "                if ngram in seen or ngram in STOP_WORDS or len(ngram) <= 1:\n",
    "                    continue\n",
    "                \n",
    "                if ngram in self.anchor_keys:\n",
    "                    start = word_positions[i][0]\n",
    "                    end = word_positions[i+n-1][1]\n",
    "                    phrases.append((ngram, start, end))\n",
    "                    seen.add(ngram)\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SCORING (V6)\n",
    "    # ========================================================================\n",
    "    \n",
    "    def score_candidates(\n",
    "        self,\n",
    "        text: str,\n",
    "        phrases: List[Tuple[str, int, int]],\n",
    "        article_id: int,\n",
    "        article_title: str\n",
    "    ) -> List[LinkPrediction]:\n",
    "        \"\"\"Score all candidates using V6 linkability-based scoring.\"\"\"\n",
    "        article_title_lower = article_title.lower().strip()\n",
    "        candidates = []\n",
    "        \n",
    "        for phrase, start, end in phrases:\n",
    "            if phrase == article_title_lower:\n",
    "                continue\n",
    "            \n",
    "            targets = self.anchor_dict[phrase]\n",
    "            total = self.anchor_totals[phrase]\n",
    "            linkability = self.phrase_linkability[phrase]\n",
    "            num_targets = len(targets)\n",
    "            \n",
    "            is_cap = text[start].isupper() if start < len(text) else False\n",
    "            word_count = len(phrase.split())\n",
    "            \n",
    "            for target_id, count in targets.items():\n",
    "                if target_id == article_id:\n",
    "                    continue\n",
    "                \n",
    "                prob = count / total if total > 0 else 0\n",
    "                score = self._compute_score(\n",
    "                    prob, linkability, count, num_targets, word_count, is_cap\n",
    "                )\n",
    "                \n",
    "                if score >= self.config.score_threshold:\n",
    "                    candidates.append(LinkPrediction(\n",
    "                        phrase=phrase,\n",
    "                        target_id=target_id,\n",
    "                        target_title=self.id_to_title.get(target_id, ''),\n",
    "                        score=score,\n",
    "                        probability=prob,\n",
    "                        linkability=linkability,\n",
    "                        num_targets=num_targets,\n",
    "                        start_pos=start,\n",
    "                        end_pos=end,\n",
    "                    ))\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def _compute_score(\n",
    "        self, prob: float, linkability: int, count: int,\n",
    "        num_targets: int, word_count: int, is_cap: bool\n",
    "    ) -> float:\n",
    "        \"\"\"V6 scoring function.\"\"\"\n",
    "        score = prob\n",
    "        \n",
    "        # Probability bonuses\n",
    "        if prob >= 0.9: score *= 2.5\n",
    "        elif prob >= 0.7: score *= 2.0\n",
    "        elif prob >= 0.5: score *= 1.5\n",
    "        elif prob >= 0.3: score *= 1.2\n",
    "        \n",
    "        # Linkability (sweet spot: 20-500)\n",
    "        if linkability >= 100: score *= 1.5\n",
    "        elif linkability >= 50: score *= 1.3\n",
    "        elif linkability >= 20: score *= 1.1\n",
    "        elif linkability >= 5: score *= 1.0\n",
    "        elif linkability >= 2: score *= 0.7\n",
    "        else: score *= 0.4\n",
    "        \n",
    "        # Count bonus\n",
    "        if count >= 50: score *= 1.3\n",
    "        elif count >= 20: score *= 1.2\n",
    "        elif count >= 5: score *= 1.1\n",
    "        elif count == 1: score *= 0.6\n",
    "        \n",
    "        # Multi-word bonus\n",
    "        if word_count >= 4: score *= 1.5\n",
    "        elif word_count >= 3: score *= 1.3\n",
    "        elif word_count >= 2: score *= 1.15\n",
    "        \n",
    "        # Capitalization\n",
    "        if is_cap: score *= 1.3\n",
    "        \n",
    "        # Ambiguity penalty\n",
    "        if num_targets > 50: score *= 0.6\n",
    "        elif num_targets > 30: score *= 0.7\n",
    "        elif num_targets > 20: score *= 0.8\n",
    "        elif num_targets > 10: score *= 0.9\n",
    "        \n",
    "        # Single lowercase penalty\n",
    "        if word_count == 1 and not is_cap: score *= 0.5\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DISAMBIGUATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    def disambiguate(self, text: str, candidates: List[LinkPrediction]) -> List[LinkPrediction]:\n",
    "        \"\"\"Use semantic similarity for ambiguous anchors.\"\"\"\n",
    "        if not self.config.use_semantic_disambiguation:\n",
    "            return candidates\n",
    "        \n",
    "        # Group by phrase\n",
    "        phrase_candidates = defaultdict(list)\n",
    "        for c in candidates:\n",
    "            phrase_candidates[c.phrase].append(c)\n",
    "        \n",
    "        # Find ambiguous\n",
    "        ambiguous = [\n",
    "            p for p, cands in phrase_candidates.items()\n",
    "            if len(cands) > 1 and cands[0].num_targets >= self.config.disambiguation_threshold\n",
    "        ]\n",
    "        \n",
    "        if not ambiguous:\n",
    "            return candidates\n",
    "        \n",
    "        # Encode context\n",
    "        context_emb = self.model.encode(\n",
    "            f\"query: {text[:1000]}\",\n",
    "            normalize_embeddings=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        \n",
    "        # Compute similarities\n",
    "        for phrase in ambiguous:\n",
    "            cands = phrase_candidates[phrase]\n",
    "            target_ids = [c.target_id for c in cands]\n",
    "            \n",
    "            try:\n",
    "                points = self.client.retrieve(\n",
    "                    collection_name=self.config.collection_name,\n",
    "                    ids=target_ids,\n",
    "                    with_vectors=True\n",
    "                )\n",
    "                id_to_emb = {p.id: np.array(p.vector) for p in points}\n",
    "                \n",
    "                for c in cands:\n",
    "                    emb = id_to_emb.get(c.target_id)\n",
    "                    if emb is not None:\n",
    "                        c.semantic_score = float(np.dot(context_emb, emb))\n",
    "                        c.score *= (1 + c.semantic_score) / 2\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return [c for cands in phrase_candidates.values() for c in cands]\n",
    "    \n",
    "    # ========================================================================\n",
    "    # DEDUPLICATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    def deduplicate(self, candidates: List[LinkPrediction]) -> List[LinkPrediction]:\n",
    "        \"\"\"Keep only first occurrence of each target/phrase.\"\"\"\n",
    "        candidates.sort(key=lambda x: (-x.score, x.start_pos))\n",
    "        \n",
    "        final = []\n",
    "        seen_targets, seen_phrases = set(), set()\n",
    "        \n",
    "        for c in candidates:\n",
    "            if c.target_id in seen_targets or c.phrase in seen_phrases:\n",
    "                continue\n",
    "            final.append(c)\n",
    "            seen_targets.add(c.target_id)\n",
    "            seen_phrases.add(c.phrase)\n",
    "            if len(final) >= self.config.max_links_per_article:\n",
    "                break\n",
    "        \n",
    "        return final\n",
    "    \n",
    "    # ========================================================================\n",
    "    # MAIN PREDICTION\n",
    "    # ========================================================================\n",
    "    \n",
    "    def predict(\n",
    "        self, text: str, article_id: int = 0, article_title: str = \"\"\n",
    "    ) -> List[LinkPrediction]:\n",
    "        \"\"\"Predict links for an article.\"\"\"\n",
    "        phrases = self.extract_phrases(text)\n",
    "        candidates = self.score_candidates(text, phrases, article_id, article_title)\n",
    "        candidates = self.disambiguate(text, candidates)\n",
    "        return self.deduplicate(candidates)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EVALUATION\n",
    "    # ========================================================================\n",
    "    \n",
    "    def evaluate(self, articles: List[Dict], verbose: bool = False) -> PipelineMetrics:\n",
    "        \"\"\"Evaluate on articles with ground truth.\"\"\"\n",
    "        total_tp, total_fp, total_fn = 0, 0, 0\n",
    "        \n",
    "        for article in tqdm(articles, desc=\"Evaluating\", disable=not verbose):\n",
    "            predictions = self.predict(article['text'], article['id'], article['title'])\n",
    "            pred_ids = set(p.target_id for p in predictions)\n",
    "            gt_ids = article['gt']\n",
    "            \n",
    "            total_tp += len(pred_ids & gt_ids)\n",
    "            total_fp += len(pred_ids - gt_ids)\n",
    "            total_fn += len(gt_ids - pred_ids)\n",
    "        \n",
    "        p = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "        r = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "        \n",
    "        return PipelineMetrics(precision=p, recall=r, f1=f1, tp=total_tp, fp=total_fp, fn=total_fn)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FETCH FROM LOCAL DB\n",
    "    # ========================================================================\n",
    "    \n",
    "    def get_article_by_title(self, title: str) -> Optional[Dict]:\n",
    "        \"\"\"Fetch article from local Qdrant database by title.\"\"\"\n",
    "        print(f\"   Searching for '{title}' in database...\")\n",
    "        \n",
    "        try:\n",
    "            # Semantic search for title\n",
    "            query_emb = self.model.encode(f\"query: {title}\", normalize_embeddings=True)\n",
    "            results = self.client.query_points(\n",
    "                collection_name=self.config.collection_name,\n",
    "                query=query_emb.tolist(),\n",
    "                limit=10,\n",
    "                with_payload=True\n",
    "            ).points\n",
    "            \n",
    "            # Find best match\n",
    "            for result in results:\n",
    "                result_title = result.payload.get('title', '')\n",
    "                if title.lower() == result_title.lower():\n",
    "                    text = result.payload.get('text') or result.payload.get('text_withoutHref', '')\n",
    "                    print(f\"   ‚úÖ Found exact match: {result_title}\")\n",
    "                    return {\n",
    "                        'id': result.payload.get('id'),\n",
    "                        'title': result_title,\n",
    "                        'text': text\n",
    "                    }\n",
    "            \n",
    "            # Partial match\n",
    "            for result in results:\n",
    "                result_title = result.payload.get('title', '')\n",
    "                if title.lower() in result_title.lower() or result_title.lower() in title.lower():\n",
    "                    text = result.payload.get('text') or result.payload.get('text_withoutHref', '')\n",
    "                    print(f\"   ‚úÖ Found similar: {result_title}\")\n",
    "                    return {\n",
    "                        'id': result.payload.get('id'),\n",
    "                        'title': result_title,\n",
    "                        'text': text\n",
    "                    }\n",
    "            \n",
    "            print(f\"   ‚ùå Not found\")\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7d38e",
   "metadata": {},
   "source": [
    "## 4. HTML Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8867e0a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_comparison_html(\n",
    "    title: str,\n",
    "    text: str,\n",
    "    predictions: List[LinkPrediction],\n",
    "    original_html: str = \"\"\n",
    ") -> str:\n",
    "    \"\"\"Generate Wikipedia-style HTML with predicted links.\"\"\"\n",
    "    \n",
    "    # Insert predicted links (reverse order to preserve positions)\n",
    "    sorted_preds = sorted(predictions, key=lambda x: x.start_pos, reverse=True)\n",
    "    predicted_text = text\n",
    "    \n",
    "    for pred in sorted_preds:\n",
    "        if pred.start_pos >= 0 and pred.end_pos > pred.start_pos:\n",
    "            orig = predicted_text[pred.start_pos:pred.end_pos]\n",
    "            wiki_title = pred.target_title.replace(\" \", \"_\")\n",
    "            link = f'<a href=\"https://fr.wikipedia.org/wiki/{wiki_title}\" class=\"predicted-link\" title=\"{html.escape(pred.target_title)} (score: {pred.score:.1f})\">{orig}</a>'\n",
    "            predicted_text = predicted_text[:pred.start_pos] + link + predicted_text[pred.end_pos:]\n",
    "    \n",
    "    # Generate HTML\n",
    "    return f'''<!DOCTYPE html>\n",
    "<html lang=\"fr\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>{html.escape(title)} - Predicted Links</title>\n",
    "    <style>\n",
    "        * {{ box-sizing: border-box; }}\n",
    "        body {{ \n",
    "            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; \n",
    "            margin: 0; \n",
    "            padding: 20px;\n",
    "            background: #f8f9fa; \n",
    "        }}\n",
    "        .container {{\n",
    "            max-width: 960px;\n",
    "            margin: 0 auto;\n",
    "            background: white;\n",
    "            padding: 30px;\n",
    "            border: 1px solid #a2a9b1;\n",
    "            border-radius: 2px;\n",
    "        }}\n",
    "        h1 {{\n",
    "            font-family: 'Linux Libertine', Georgia, serif;\n",
    "            font-size: 1.8em;\n",
    "            font-weight: normal;\n",
    "            border-bottom: 1px solid #a2a9b1;\n",
    "            padding-bottom: 10px;\n",
    "            margin: 0 0 20px 0;\n",
    "        }}\n",
    "        .stats {{\n",
    "            background: linear-gradient(135deg, #667eea, #764ba2);\n",
    "            color: white;\n",
    "            padding: 20px;\n",
    "            border-radius: 8px;\n",
    "            margin-bottom: 20px;\n",
    "            display: flex;\n",
    "            justify-content: space-around;\n",
    "            text-align: center;\n",
    "        }}\n",
    "        .stat-value {{ font-size: 2em; font-weight: bold; }}\n",
    "        .stat-label {{ font-size: 0.9em; opacity: 0.9; }}\n",
    "        .content {{\n",
    "            font-size: 14px;\n",
    "            line-height: 1.7;\n",
    "            color: #202122;\n",
    "        }}\n",
    "        .predicted-link {{\n",
    "            color: #0645ad;\n",
    "            text-decoration: none;\n",
    "            background: #e8f5e9;\n",
    "            padding: 1px 3px;\n",
    "            border-radius: 2px;\n",
    "            border-bottom: 2px solid #4caf50;\n",
    "        }}\n",
    "        .predicted-link:hover {{\n",
    "            background: #c8e6c9;\n",
    "            text-decoration: underline;\n",
    "        }}\n",
    "        .link-list {{\n",
    "            margin-top: 30px;\n",
    "            padding: 20px;\n",
    "            background: #f8f9fa;\n",
    "            border-radius: 4px;\n",
    "        }}\n",
    "        .link-list h2 {{\n",
    "            font-size: 1.1em;\n",
    "            margin: 0 0 15px 0;\n",
    "            color: #333;\n",
    "        }}\n",
    "        .link-grid {{\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));\n",
    "            gap: 10px;\n",
    "        }}\n",
    "        .link-item {{\n",
    "            font-size: 12px;\n",
    "            padding: 8px 12px;\n",
    "            background: white;\n",
    "            border: 1px solid #ddd;\n",
    "            border-radius: 4px;\n",
    "        }}\n",
    "        .link-item .phrase {{ font-weight: bold; color: #1a73e8; }}\n",
    "        .link-item .target {{ color: #666; }}\n",
    "        .link-item .score {{ float: right; color: #4caf50; font-weight: bold; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>{html.escape(title)}</h1>\n",
    "        \n",
    "        <div class=\"stats\">\n",
    "            <div>\n",
    "                <div class=\"stat-value\">{len(predictions)}</div>\n",
    "                <div class=\"stat-label\">Links Predicted</div>\n",
    "            </div>\n",
    "            <div>\n",
    "                <div class=\"stat-value\">{sum(1 for p in predictions if p.score >= 5)}</div>\n",
    "                <div class=\"stat-label\">High Confidence</div>\n",
    "            </div>\n",
    "            <div>\n",
    "                <div class=\"stat-value\">{sum(p.score for p in predictions)/max(len(predictions),1):.1f}</div>\n",
    "                <div class=\"stat-label\">Avg Score</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"content\">\n",
    "            {predicted_text.replace(chr(10), '<br>')}\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"link-list\">\n",
    "            <h2>üîó All Predicted Links (sorted by score)</h2>\n",
    "            <div class=\"link-grid\">\n",
    "                {\"\".join(f'''<div class=\"link-item\">\n",
    "                    <span class=\"score\">{p.score:.1f}</span>\n",
    "                    <span class=\"phrase\">{html.escape(p.phrase)}</span><br>\n",
    "                    <span class=\"target\">‚Üí <a href=\"https://fr.wikipedia.org/wiki/{p.target_title.replace(\" \", \"_\")}\">{html.escape(p.target_title[:40])}</a></span>\n",
    "                </div>''' for p in sorted(predictions, key=lambda x: -x.score)[:50])}\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcfb5ec",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183836c6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_test_articles(pipeline, n: int = 100, min_links: int = 5) -> List[Dict]:\n",
    "    \"\"\"Load articles with ground truth for evaluation.\"\"\"\n",
    "    print(f\"\\nüìÇ Loading {n} test articles...\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    articles = []\n",
    "    offset = None\n",
    "    \n",
    "    while len(articles) < n * 3:\n",
    "        points, offset = pipeline.client.scroll(\n",
    "            collection_name=\"wikipedia_fr\",\n",
    "            limit=500, offset=offset,\n",
    "            with_payload=True, with_vectors=False\n",
    "        )\n",
    "        for point in points:\n",
    "            text = point.payload.get('text') or point.payload.get('text_withoutHref', '')\n",
    "            if text and len(text) >= 500:\n",
    "                articles.append({\n",
    "                    'id': point.payload.get('id'),\n",
    "                    'title': point.payload.get('title', ''),\n",
    "                    'text': text\n",
    "                })\n",
    "        if offset is None:\n",
    "            break\n",
    "    \n",
    "    # Load ground truth\n",
    "    article_ids = [a['id'] for a in articles]\n",
    "    df = pl.scan_parquet(CONFIG.parquet_path).filter(\n",
    "        pl.col('id').is_in(article_ids)\n",
    "    ).select(['id', 'links']).collect()\n",
    "    \n",
    "    gt_map = {}\n",
    "    for row in df.iter_rows(named=True):\n",
    "        article_id = row['id']\n",
    "        links_raw = row.get('links')\n",
    "        if links_raw is None:\n",
    "            gt_map[article_id] = frozenset()\n",
    "            continue\n",
    "        targets = set()\n",
    "        for link in links_raw:\n",
    "            href = link.get('href_decoded', '')\n",
    "            if href:\n",
    "                tid = (pipeline.url_to_id.get(href) or \n",
    "                       pipeline.url_to_id.get(href.replace(\"_\", \" \")) or \n",
    "                       pipeline.url_to_id.get(href.lower()))\n",
    "                if tid:\n",
    "                    targets.add(tid)\n",
    "        gt_map[article_id] = frozenset(targets)\n",
    "    \n",
    "    filtered = []\n",
    "    for a in articles:\n",
    "        gt = gt_map.get(a['id'], frozenset())\n",
    "        if len(gt) >= min_links:\n",
    "            a['gt'] = gt\n",
    "            filtered.append(a)\n",
    "        if len(filtered) >= n:\n",
    "            break\n",
    "    \n",
    "    print(f\"   ‚úÖ Loaded {len(filtered)} articles ({time.time()-t0:.1f}s)\")\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f3f9e9",
   "metadata": {},
   "source": [
    "## 6. Initialize Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49662b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = WikipediaLinkPredictor(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d3c2d7",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198689da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä EVALUATION ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_articles = load_test_articles(pipeline, n=100, min_links=5)\n",
    "metrics = pipeline.evaluate(test_articles, verbose=True)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"üèÜ RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"   Precision: {metrics.precision:.4f}\")\n",
    "print(f\"   Recall:    {metrics.recall:.4f}\")\n",
    "print(f\"   F1 Score:  {metrics.f1:.4f}\")\n",
    "print(f\"   TP: {metrics.tp}, FP: {metrics.fp}, FN: {metrics.fn}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b53cf8",
   "metadata": {},
   "source": [
    "## 8. Test on Specific Articles from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e58cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ TESTING ON SPECIFIC ARTICLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_titles = [\n",
    "    \"Premi√®re Guerre mondiale\",\n",
    "    \"Marie Curie\",\n",
    "    \"Tour Eiffel\",\n",
    "    \"R√©volution fran√ßaise\",\n",
    "    \"Albert Einstein\",\n",
    "    \"Paris\",\n",
    "    \"France\",\n",
    "]\n",
    "\n",
    "for title in test_titles:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìÑ {title}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Fetch from local database\n",
    "    article = pipeline.get_article_by_title(title)\n",
    "    \n",
    "    if article is None:\n",
    "        continue\n",
    "    \n",
    "    # Predict\n",
    "    t0 = time.time()\n",
    "    predictions = pipeline.predict(\n",
    "        article['text'], \n",
    "        article['id'], \n",
    "        article['title']\n",
    "    )\n",
    "    print(f\"   ‚úÖ {len(predictions)} links in {time.time()-t0:.2f}s\")\n",
    "    \n",
    "    # Top predictions\n",
    "    print(f\"\\n   Top 10:\")\n",
    "    for p in predictions[:10]:\n",
    "        print(f\"   ‚Ä¢ {p.phrase} ‚Üí {p.target_title[:40]} (s={p.score:.1f})\")\n",
    "    \n",
    "    # Save HTML\n",
    "    html_output = generate_comparison_html(article['title'], article['text'], predictions)\n",
    "    safe_name = re.sub(r'[^\\w\\-]', '_', article['title'])\n",
    "    filename = f\"predicted_{safe_name}.html\"\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_output)\n",
    "    print(f\"\\n   üíæ Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fb306b",
   "metadata": {},
   "source": [
    "## 9. Detailed Sample Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53299d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ DETAILED SAMPLE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample = test_articles[0]\n",
    "predictions = pipeline.predict(sample['text'], sample['id'], sample['title'])\n",
    "\n",
    "pred_ids = set(p.target_id for p in predictions)\n",
    "gt_ids = sample['gt']\n",
    "\n",
    "print(f\"\\nArticle: {sample['title']}\")\n",
    "print(f\"   GT: {len(gt_ids)}, Predicted: {len(predictions)}\")\n",
    "print(f\"   TP: {len(pred_ids & gt_ids)}, FP: {len(pred_ids - gt_ids)}, FN: {len(gt_ids - pred_ids)}\")\n",
    "\n",
    "print(f\"\\n   ‚úÖ Correct predictions (TP):\")\n",
    "for p in [p for p in predictions if p.target_id in gt_ids][:10]:\n",
    "    print(f\"      '{p.phrase}' ‚Üí {p.target_title[:40]}\")\n",
    "\n",
    "print(f\"\\n   ‚ùå False positives (FP):\")\n",
    "for p in [p for p in predictions if p.target_id not in gt_ids][:10]:\n",
    "    print(f\"      '{p.phrase}' ‚Üí {p.target_title[:40]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd1761",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PIPELINE COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "üìä Final Results:\n",
    "   ‚Ä¢ Precision: {metrics.precision:.4f}\n",
    "   ‚Ä¢ Recall:    {metrics.recall:.4f}\n",
    "   ‚Ä¢ F1 Score:  {metrics.f1:.4f}\n",
    "\n",
    "üìÅ Generated Files:\n",
    "   ‚Ä¢ predicted_*.html - Article predictions with links\n",
    "\n",
    "üîß Configuration:\n",
    "   ‚Ä¢ Threshold: {CONFIG.score_threshold}\n",
    "   ‚Ä¢ Disambiguation: {CONFIG.use_semantic_disambiguation}\n",
    "   ‚Ä¢ Device: {CONFIG.device}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
