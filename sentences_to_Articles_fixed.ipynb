{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ca5a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/ltran/.conda/envs/DCL_WIKI_RAG/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import polars as pl\n",
    "#If you already saved the model locally and are using docker\n",
    "client = QdrantClient(host=\"localhost\", port=6333, prefer_grpc=True, timeout=1000)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"models/multilingual-e5-large\", device=device)\n",
    "\n",
    "# Run docker container before\n",
    "# docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v /home/Loris/EPFL/MA3/ML/project2/project2Rag/qdrant_storage:/qdrant/storage qdrant/qdrant:latest\n",
    "\n",
    "# VERY IMPORTANT : to query the remote qdrand docker container over ssh, run the following in a terminale before:\n",
    "# ssh -L 6333:localhost:6333 -L 6334:localhost:6334 <your username>@dclgpusrv.epfl.ch -N\n",
    "#Make sure to have your ssh keys before\n",
    "#This forwards the qdrant ports to your local machine\n",
    "#If you are outside EPFL network, use VPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9929b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles stored: 2556402\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http import models as rest\n",
    "#get articles count from Qdrant DB \n",
    "\n",
    "collection_name = \"wikipedia_fr\"\n",
    "total = client.count(collection_name=collection_name, exact=True).count\n",
    "print(f\"Total articles stored: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2cb083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Dataset loaded as LazyFrame (not in memory)\n",
      "   Schema: Schema({'id': Int64, 'title': String, 'text': String, 'links': List(Struct({'full_url': String, 'start_idx': Int64, 'anchor': String, 'href_raw': String, 'href_decoded': String})), 'link_count': UInt32, 'text_withoutHref': String})\n",
      "\n",
      "   Link structure example:\n",
      "Antoine Meillet, nÃ© le Ã  &lt;a href=\"Moulins%20%28Allier%29\"&gt;Moulins&lt;/a&gt; (&lt;a href=\"Allier%20%28d%C3%A9partement%29\"&gt;Allier&lt;/a&gt;) et mort le Ã  &lt;a href=\"Ch%C3%A2teaumeillant\"&gt;ChÃ¢teaumeillant&lt;/a&gt; (&lt;a href=\"Cher%20%28d%C3%A9partement%29\"&gt;Cher&lt;/a&gt;), est un &lt;a href=\"Philologie\"&gt;philologue&lt;/a&gt; franÃ§ais, le principal &lt;a href=\"liste%20de%20linguistes\"&gt;linguiste&lt;/a&gt; franÃ§ais des premiÃ¨res dÃ©cennies du XXÂ siÃ¨cle.\n",
      "Biographie.\n",
      "Enfance et formation.\n",
      "Paul Jules Antoine Meillet est d'origine &lt;a href=\"Allier%20%28d%C3%A9partement%29\"&gt;bourbonnaise&lt;/a&gt;, fils d'un &lt;a href=\"notaire\"&gt;notaire&lt;/a&gt; de &lt;a href=\"Ch%C3%A2teaumeillant\"&gt;ChÃ¢teaumeillant&lt;/a&gt; (&lt;a href=\"Cher%20%28d%C3%A9partement%29\"&gt;Cher&lt;/a&gt;). Il naÃ®t Ã  &lt;a href=\"Moulins%20%28Allier%29\"&gt;Moulins&lt;/a&gt; le 11 novembre 1866.\n",
      "Il passe son enfance Ã  ChÃ¢teaumeillant, puis fait ses Ã©tudes secondaires au &lt;a href=\"lyc%C3%A9e%20Th%C3%A9odore-de-Banville\"&gt;lycÃ©e&lt;/a&gt; de Moulins.\n",
      "Ã‰tudiant Ã  partir de 1885 Ã  la &lt;a href=\"facult%C3%A9%20des%20lettres%20de%20Paris\"&gt;facultÃ© des lettres de Paris&lt;/a&gt; oÃ¹ il suit notamment les cours de &lt;a href=\"Louis%20Havet\"&gt;Louis Havet&lt;/a&gt;, il assiste Ã©galement Ã  ceux de &lt;a href=\"Michel%20Br%C3%A9al\"&gt;Michel BrÃ©al&lt;/a&gt; au &lt;a href=\"Coll%C3%A8ge%20de%20France\"&gt;CollÃ¨ge de France&lt;/a&gt; et de &lt;a href=\"Ferdinand%20de%20Saussure\"&gt;Ferdinand de Saussure&lt;/a&gt; Ã  l'&lt;a href=\"%C3%89cole%20pratique%20des%20hautes%20%C3%A9tudes\"&gt;Ã‰cole pratique des hautes Ã©tudes&lt;/a&gt;.\n",
      "En 1889, il est major de l'&lt;a href=\"agr%C3%A9gation%20de%20grammaire\"&gt;agrÃ©gation de grammaire&lt;/a&gt;. En 1891, il fait son premier sÃ©jour en &lt;a href=\"Arm%C3%A9nie\"&gt;ArmÃ©nie&lt;/a&gt;, notamment Ã  &lt;a href=\"Etchmiadzin\"&gt;Etchmiadzin&lt;/a&gt; ; son projet est d'apprendre l'armÃ©nien moderne et d'Ã©tudier d'anciens manuscrits.\n",
      "Ã€ son retour, il assure Ã  la suite de Saussure le cours de &lt;a href=\"grammaire%20compar%C3%A9e\"&gt;grammaire comparÃ©e&lt;/a&gt;, qu'il complÃ¨te Ã  partir de 1894 par une confÃ©rence sur les &lt;a href=\"langues%20persanes\"&gt;langues persanes&lt;/a&gt;.\n",
      "En 1897, il soutient sa thÃ¨se pour le &lt;a href=\"Doctorat%20%C3%A8s%20lettres%20%28France%29\"&gt;doctorat Ã¨s lettres&lt;/a&gt; \"(Recherches sur l'emploi du gÃ©nitif-accusatif en &lt;a href=\"vieux-slave\"&gt;vieux-slave&lt;/a&gt;)\".\n",
      "CarriÃ¨re.\n",
      "En 1902, il succÃ¨de au linguiste &lt;a href=\"Auguste%20Carri%C3%A8re\"&gt;Auguste CarriÃ¨re&lt;/a&gt; Ã  la chaire d'&lt;a href=\"arm%C3%A9nien\"&gt;armÃ©nien&lt;/a&gt; de l'&lt;a href=\"Institut%20national%20des%20langues%20et%20civilisations%20orientales\"&gt;Ã‰cole des langues orientales&lt;/a&gt;. En 1906, Ã  la suite de &lt;a href=\"Michel%20Br%C3%A9al\"&gt;Michel BrÃ©al&lt;/a&gt;, il prend la chaire de grammaire comparÃ©e du &lt;a href=\"Coll%C3%A8ge%20de%20France\"&gt;CollÃ¨ge de France&lt;/a&gt;, oÃ¹ il consacre ses cours Ã  l'histoire et Ã  la structure des &lt;a href=\"langues%20indo-europ%C3%A9ennes\"&gt;langues indo-europÃ©ennes&lt;/a&gt; ; il abandonne alors son enseignement Ã  l'Ã‰cole des langues orientales et se consacre dÃ©sormais Ã  la linguistique comparÃ©e au CollÃ¨ge de France, ainsi qu'Ã  l'&lt;a href=\"%C3%89cole%20pratique%20des%20hautes%20%C3%A9tudes\"&gt;Ã‰cole pratique des hautes Ã©tudes&lt;/a&gt;.\n",
      "SecrÃ©taire de la &lt;a href=\"Soci%C3%A9t%C3%A9%20de%20linguistique%20de%20Paris\"&gt;SociÃ©tÃ© de linguistique de Paris&lt;/a&gt;, il est Ã©lu Ã  l'&lt;a href=\"Acad%C3%A9mie%20des%20inscriptions%20et%20belles-lettres\"&gt;AcadÃ©mie des inscriptions et belles-lettres&lt;/a&gt; en 1924. Il prÃ©side Ã©galement l'&lt;a href=\"Institut%20d%27%C3%A9tudes%20slaves\"&gt;Institut d'Ã©tudes slaves&lt;/a&gt; de 1921 Ã  sa mort.\n",
      "Il a formÃ© toute une gÃ©nÃ©ration de linguistes franÃ§ais, parmi lesquels &lt;a href=\"%C3%89mile%20Benveniste\"&gt;Ã‰mile Benveniste&lt;/a&gt;, &lt;a href=\"Marcel%20Cohen\"&gt;Marcel Cohen&lt;/a&gt;, &lt;a href=\"Georges%20Dum%C3%A9zil\"&gt;Georges DumÃ©zil&lt;/a&gt;, &lt;a href=\"Lilias%20Homburger\"&gt;Lilias Homburger&lt;/a&gt;, &lt;a href=\"Andr%C3%A9%20Martinet\"&gt;AndrÃ© Martinet&lt;/a&gt;, &lt;a href=\"Aur%C3%A9lien%20Sauvageot\"&gt;AurÃ©lien Sauvageot&lt;/a&gt;, &lt;a href=\"Lucien%20Tesni%C3%A8re\"&gt;Lucien TesniÃ¨re&lt;/a&gt;, le &lt;a href=\"Japonisation\"&gt;japonisant&lt;/a&gt; &lt;a href=\"Charles%20Haguenauer\"&gt;Charles Haguenauer&lt;/a&gt; ou &lt;a href=\"Joseph%20Vendryes\"&gt;Joseph Vendryes&lt;/a&gt;. Antoine Meillet devait diriger la thÃ¨se de &lt;a href=\"Jean%20Paulhan\"&gt;Jean Paulhan&lt;/a&gt; sur la sÃ©mantique du proverbe et c'est lui qui dÃ©couvrit &lt;a href=\"Gustave%20Guillaume\"&gt;Gustave Guillaume&lt;/a&gt;.\n",
      "Il a influencÃ© aussi un certain nombre de linguistes Ã©trangers. Il a Ã©galement Ã©tÃ© le premier Ã  identifier le phÃ©nomÃ¨ne de la &lt;a href=\"grammaticalisation\"&gt;grammaticalisation&lt;/a&gt;.\n",
      "Selon le &lt;a href=\"Linguistique\"&gt;linguiste&lt;/a&gt; allemand &lt;a href=\"Walter%20Porzig\"&gt;Walter Porzig&lt;/a&gt;, Meillet est un Â« grand prÃ©curseur Â». Il montre, par exemple, que, dans les &lt;a href=\"Dialecte\"&gt;dialectes&lt;/a&gt; indo-europÃ©ens, les groupes indo-europÃ©ens sont le rÃ©sultat historique d'une &lt;a href=\"Variation%20linguistique\"&gt;variation diatopique&lt;/a&gt;.\n",
      "Lâ€™acte de naissance de la &lt;a href=\"sociolinguistique\"&gt;sociolinguistique&lt;/a&gt; est signÃ© par Antoine Meillet fondateur de la sociolinguistique qui sâ€™est opposÃ© au &lt;a href=\"Cours%20de%20linguistique%20g%C3%A9n%C3%A9rale\"&gt;Cours de linguistique gÃ©nÃ©rale&lt;/a&gt; de &lt;a href=\"Ferdinand%20de%20Saussure\"&gt;Ferdinand de Saussure&lt;/a&gt; dÃ¨s son apparition en 1916 en le critiquant sur plusieurs plans.\n",
      "Il meurt en 1936 Ã  &lt;a href=\"Ch%C3%A2teaumeillant\"&gt;ChÃ¢teaumeillant&lt;/a&gt; et est enterrÃ© au cimetiÃ¨re de &lt;a href=\"Moulins%20%28Allier%29\"&gt;Moulins&lt;/a&gt; dans le caveau familial.\n",
      "Ã‰tudes homÃ©riques.\n",
      "Ã€ la Sorbonne, Meillet supervise le travail de &lt;a href=\"Milman%20Parry\"&gt;Milman Parry&lt;/a&gt;. Meillet offre Ã  son Ã©tudiant l'opinion, nouvelle Ã  cette Ã©poque, que la structure formulaÃ¯que de \"&lt;a href=\"l%27Iliade\"&gt;l'Iliade&lt;/a&gt;\" serait une consÃ©quence directe de sa transmission orale. Ainsi, il le dirige vers l'Ã©tude de l'oralitÃ© dans son cadre natif et lui suggÃ¨re d'observer les mÃ©canismes d'une tradition orale vivante Ã  cÃ´tÃ© du texte classique (\"&lt;a href=\"l%27Iliade\"&gt;l'Iliade&lt;/a&gt;\") qui est censÃ© rÃ©sulter d'une telle tradition. En consÃ©quence, Meillet prÃ©sente Parry Ã  &lt;a href=\"Matija%20Murko\"&gt;Matija Murko&lt;/a&gt;, savant originaire de &lt;a href=\"Slov%C3%A9nie\"&gt;SlovÃ©nie&lt;/a&gt; qui avait longuement Ã©crit sur la tradition hÃ©roÃ¯que Ã©pique dans les &lt;a href=\"Balkans\"&gt;Balkans&lt;/a&gt;, surtout en &lt;a href=\"Bosnie-Herz%C3%A9govine\"&gt;Bosnie-HerzÃ©govine&lt;/a&gt;. Par leurs recherches, dont les rÃ©sultats sont Ã  prÃ©sent hÃ©bergÃ©s par l'universitÃ© de Harvard, Parry et son Ã©lÃ¨ve, &lt;a href=\"Albert%20Lord\"&gt;Albert Lord&lt;/a&gt;, ont profondÃ©ment renouvelÃ© les Ã©tudes homÃ©riques.\n",
      "Voir aussi.\n",
      "&lt;templatestyles src=\"Autres projets/styles.css\" /&gt;\n",
      "Sur les autres projets Wikimedia :\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Keep as LazyFrame - don't collect the full dataset!\n",
    "df_lazy = pl.scan_parquet(\"articles_fr_merged.parquet\").filter(\n",
    "    pl.col('link_count') > 0\n",
    ")\n",
    "\n",
    "# Only collect a small sample to verify structure\n",
    "sample = df_lazy.head(1).collect()\n",
    "\n",
    "print(f\"ðŸ“š Dataset loaded as LazyFrame (not in memory)\")\n",
    "print(f\"   Schema: {df_lazy.collect_schema()}\")\n",
    "print(f\"\\n   Link structure example:\")\n",
    "\n",
    "\n",
    "full_text = df_lazy.select(\"text\").head(1).collect().item()\n",
    "\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ce5997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—ºï¸  Building URL â†’ ID mapping from 'wikipedia_fr'...\n",
      "âœ… Created 12,682,764 URL mappings from 2,556,402 articles\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from urllib.parse import unquote\n",
    "\n",
    "def create_url_to_id_mapping_from_qdrant(client, collection_name: str = \"wikipedia_fr\") -> Dict[str, int]:\n",
    "    \"\"\"Build URL â†’ ID mapping using articles in Qdrant.\"\"\"\n",
    "    from urllib.parse import quote\n",
    "    \n",
    "    url_to_id = {}\n",
    "    id_to_title = {}\n",
    "    \n",
    "    print(f\"ðŸ—ºï¸  Building URL â†’ ID mapping from '{collection_name}'...\")\n",
    "    \n",
    "    offset = None\n",
    "    batch_size = 1000\n",
    "    total_processed = 0\n",
    "    \n",
    "    while True:\n",
    "        points, offset = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=batch_size,\n",
    "            offset=offset,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        \n",
    "        if not points:\n",
    "            break\n",
    "        \n",
    "        for point in points:\n",
    "            article_id = point.payload.get(\"id\")\n",
    "            title = point.payload.get(\"title\", \"\")\n",
    "            \n",
    "            if not article_id or not title:\n",
    "                continue\n",
    "            \n",
    "            id_to_title[article_id] = title\n",
    "            \n",
    "            # Create URL pattern variations\n",
    "            patterns = [\n",
    "                title,\n",
    "                title.replace(\" \", \"_\"),\n",
    "                quote(title.replace(\" \", \"_\"), safe=\"\"),\n",
    "                title.lower(),\n",
    "                title.lower().replace(\" \", \"_\"),\n",
    "                title.lower().replace(\" \", \"%20\"),\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                url_to_id[pattern] = article_id\n",
    "        \n",
    "        total_processed += len(points)\n",
    "        \n",
    "        if offset is None:\n",
    "            break\n",
    "    \n",
    "    print(f\"âœ… Created {len(url_to_id):,} URL mappings from {total_processed:,} articles\")\n",
    "    return url_to_id, id_to_title\n",
    "\n",
    "# Build mappings\n",
    "url_to_id, id_to_title = create_url_to_id_mapping_from_qdrant(client, \"wikipedia_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52156768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 12,682,764 URL mappings to url_to_id_mapping.pkl\n",
      "âœ… Saved 2,556,402 IDâ†’title mappings to id_to_title_mapping.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save mappings to disk\n",
    "with open(\"url_to_id_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(url_to_id, f)\n",
    "\n",
    "with open(\"id_to_title_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(id_to_title, f)\n",
    "\n",
    "print(f\"âœ… Saved {len(url_to_id):,} URL mappings to url_to_id_mapping.pkl\")\n",
    "print(f\"âœ… Saved {len(id_to_title):,} IDâ†’title mappings to id_to_title_mapping.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f693234d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading cached mappings...\n",
      "âœ… Loaded 12,682,764 URL mappings, 2,556,402 IDâ†’title mappings\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Load from disk if available, otherwise rebuild\n",
    "if Path(\"url_to_id_mapping.pkl\").exists() and Path(\"id_to_title_mapping.pkl\").exists():\n",
    "    print(\"ðŸ“‚ Loading cached mappings...\")\n",
    "    with open(\"url_to_id_mapping.pkl\", \"rb\") as f:\n",
    "        url_to_id = pickle.load(f)\n",
    "    with open(\"id_to_title_mapping.pkl\", \"rb\") as f:\n",
    "        id_to_title = pickle.load(f)\n",
    "    print(f\"âœ… Loaded {len(url_to_id):,} URL mappings, {len(id_to_title):,} IDâ†’title mappings\")\n",
    "else:\n",
    "    print(\"âš ï¸ No cached mappings found, rebuilding...\")\n",
    "    url_to_id, id_to_title = create_url_to_id_mapping_from_qdrant(client, \"wikipedia_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15be6759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample result:\n",
      "Original text (first 200 chars): Antoine Meillet, nÃ© le Ã  &lt;a href=\"Moulins%20%28Allier%29\"&gt;Moulins&lt;/a&gt; (&lt;a href=\"Allier%20%28d%C3%A9partement%29\"&gt;Allier&lt;/a&gt;) et mort le Ã  &lt;a href=\"Ch%C3%A2teaumeillant\"&gt;C...\n",
      "\n",
      "Clean text (first 200 chars): Antoine Meillet, nÃ© le Ã  Moulins (Allier) et mort le Ã  ChÃ¢teaumeillant (Cher), est un philologue franÃ§ais, le principal linguiste franÃ§ais des premiÃ¨res dÃ©cennies du XXÂ siÃ¨cle.\n",
      "Biographie.\n",
      "Enfance et ...\n",
      "\n",
      "Extracted links (65 total):\n",
      "  - 'Moulins' â†’ Moulins (Allier) (pos: 25)\n",
      "  - 'Allier' â†’ Allier (dÃ©partement) (pos: 34)\n",
      "  - 'ChÃ¢teaumeillant' â†’ ChÃ¢teaumeillant (pos: 55)\n",
      "  - 'Cher' â†’ Cher (dÃ©partement) (pos: 72)\n",
      "  - 'philologue' â†’ Philologie (pos: 86)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import polars as pl\n",
    "from urllib.parse import unquote\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def extract_links_and_clean_text(text: str) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Extract href links from text and return clean text with anchors only.\n",
    "    \n",
    "    Input text format:\n",
    "    'Antoine Meillet, nÃ© le Ã  <a href=\"Moulins%20%28Allier%29\">Moulins</a>...'\n",
    "    \n",
    "    Returns:\n",
    "    - clean_text: 'Antoine Meillet, nÃ© le Ã  Moulins...'\n",
    "    - links: [{'anchor': 'Moulins', 'href_raw': 'Moulins%20%28Allier%29', \n",
    "               'href_decoded': 'Moulins (Allier)', 'start_idx': 25, 'end_idx': 32}]\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\", []\n",
    "    \n",
    "    # Pattern to match <a href=\"...\">anchor</a>\n",
    "    # Handles both &lt;a href=... and <a href=... formats\n",
    "    href_pattern = r'(?:&lt;|<)a\\s+href=\"([^\"]*)\"(?:&gt;|>)(.*?)(?:&lt;|<)/a(?:&gt;|>)'\n",
    "    \n",
    "    links = []\n",
    "    clean_text = text\n",
    "    offset = 0  # Track position offset as we replace tags\n",
    "    \n",
    "    for match in re.finditer(href_pattern, text, re.IGNORECASE | re.DOTALL):\n",
    "        href_raw = match.group(1)\n",
    "        anchor = match.group(2)\n",
    "        \n",
    "        # Decode URL-encoded href\n",
    "        try:\n",
    "            href_decoded = unquote(href_raw)\n",
    "        except:\n",
    "            href_decoded = href_raw\n",
    "        \n",
    "        # Calculate position in clean text\n",
    "        # Original position minus the offset from previous replacements\n",
    "        original_start = match.start()\n",
    "        clean_start = original_start - offset\n",
    "        \n",
    "        # Store link info\n",
    "        links.append({\n",
    "            'anchor': anchor,\n",
    "            'href_raw': href_raw,\n",
    "            'href_decoded': href_decoded,\n",
    "            'start_idx': clean_start,\n",
    "            'end_idx': clean_start + len(anchor)\n",
    "        })\n",
    "        \n",
    "        # Update offset: we're removing the full tag and keeping only anchor\n",
    "        tag_length = match.end() - match.start()\n",
    "        anchor_length = len(anchor)\n",
    "        offset += tag_length - anchor_length\n",
    "    \n",
    "    # Replace all href tags with just the anchor text\n",
    "    clean_text = re.sub(href_pattern, r'\\2', text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    return clean_text, links\n",
    "\n",
    "\n",
    "def process_text_column(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a single text entry and return clean text + links.\n",
    "    For use with Polars map_elements.\n",
    "    \"\"\"\n",
    "    clean_text, links = extract_links_and_clean_text(text)\n",
    "    return {\n",
    "        'text_clean': clean_text,\n",
    "        'extracted_links': links\n",
    "    }\n",
    "\n",
    "\n",
    "# Create new LazyFrame with extracted links and clean text\n",
    "# This processes the 'text' column which contains the href tags\n",
    "\n",
    "df_with_links = (\n",
    "    df_lazy\n",
    "    .with_columns([\n",
    "        # Extract links and clean text using map_elements\n",
    "        pl.col('text').map_elements(\n",
    "            lambda x: extract_links_and_clean_text(x)[0] if x else \"\",\n",
    "            return_dtype=pl.Utf8\n",
    "        ).alias('text_clean'),\n",
    "        \n",
    "        pl.col('text').map_elements(\n",
    "            lambda x: extract_links_and_clean_text(x)[1] if x else [],\n",
    "            return_dtype=pl.List(pl.Struct([\n",
    "                pl.Field('anchor', pl.Utf8),\n",
    "                pl.Field('href_raw', pl.Utf8),\n",
    "                pl.Field('href_decoded', pl.Utf8),\n",
    "                pl.Field('start_idx', pl.Int64),\n",
    "                pl.Field('end_idx', pl.Int64)\n",
    "            ]))\n",
    "        ).alias('extracted_links')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Test on a sample\n",
    "sample = df_with_links.head(1).collect()\n",
    "print(\"Sample result:\")\n",
    "print(f\"Original text (first 200 chars): {sample['text'][0][:200]}...\")\n",
    "print(f\"\\nClean text (first 200 chars): {sample['text_clean'][0][:200]}...\")\n",
    "print(f\"\\nExtracted links ({len(sample['extracted_links'][0])} total):\")\n",
    "for link in sample['extracted_links'][0][:5]:\n",
    "    print(f\"  - '{link['anchor']}' â†’ {link['href_decoded']} (pos: {link['start_idx']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a569b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85d9fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# def extract_sentences_with_links_by_anchor(\n",
    "#     text: str,\n",
    "#     links: list\n",
    "# ) -> List[Dict]:\n",
    "#     \"\"\"Extract sentences and match links by anchor text presence (whole word match).\"\"\"\n",
    "#     if not text or not links or len(links) == 0:\n",
    "#         return []\n",
    "    \n",
    "#     # Split into sentences\n",
    "#     sentence_pattern = r'(?<=[.!?])\\s+'\n",
    "#     sentences = re.split(sentence_pattern, text)\n",
    "    \n",
    "#     results = []\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         if not sentence.strip() or len(sentence) < 20:\n",
    "#             continue\n",
    "        \n",
    "#         links_in_sent = []\n",
    "#         seen_links = set()\n",
    "        \n",
    "#         for link in links:\n",
    "#             anchor = link.get('anchor', '')\n",
    "#             href_decoded = link.get('href_decoded', '')\n",
    "            \n",
    "#             if not anchor or len(anchor) < 2:\n",
    "#                 continue\n",
    "            \n",
    "#             # Use word boundary matching to avoid substring matches\n",
    "#             # Escape special regex characters in anchor\n",
    "#             escaped_anchor = re.escape(anchor)\n",
    "#             # Match as whole word (with word boundaries)\n",
    "#             pattern = r'\\b' + escaped_anchor + r'\\b'\n",
    "            \n",
    "#             if re.search(pattern, sentence, re.IGNORECASE):\n",
    "#                 link_key = (anchor.lower(), href_decoded.lower())\n",
    "#                 if link_key not in seen_links:\n",
    "#                     seen_links.add(link_key)\n",
    "#                     links_in_sent.append({\n",
    "#                         'anchor': anchor,\n",
    "#                         'href_decoded': href_decoded,\n",
    "#                     })\n",
    "        \n",
    "#         if links_in_sent:\n",
    "#             results.append({\n",
    "#                 'sentence': sentence.strip(),\n",
    "#                 'links_in_sentence': links_in_sent,\n",
    "#                 'num_links': len(links_in_sent)\n",
    "#             })\n",
    "    \n",
    "#     return results\n",
    "\n",
    "def extract_sentences_with_links_by_position(\n",
    "    clean_text: str,\n",
    "    extracted_links: List[Dict]\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract sentences and match links by their character position in clean text.\n",
    "    \n",
    "    This ensures only links that were ACTUALLY in that sentence (in the original\n",
    "    Wikipedia markup) are included as ground truth.\n",
    "    \"\"\"\n",
    "    if not clean_text or not extracted_links or len(extracted_links) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Sort links by position\n",
    "    sorted_links = sorted(extracted_links, key=lambda x: x.get('start_idx', 0))\n",
    "    \n",
    "    # Split into sentences\n",
    "    sentence_pattern = r'(?<=[.!?])\\s+'\n",
    "    sentences = re.split(sentence_pattern, clean_text)\n",
    "    \n",
    "    results = []\n",
    "    current_pos = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if not sentence.strip() or len(sentence) < 20:\n",
    "            # Still need to track position even for skipped sentences\n",
    "            idx = clean_text.find(sentence, current_pos)\n",
    "            if idx != -1:\n",
    "                current_pos = idx + len(sentence)\n",
    "            continue\n",
    "        \n",
    "        # Find sentence boundaries in clean text\n",
    "        sent_start = clean_text.find(sentence, current_pos)\n",
    "        if sent_start == -1:\n",
    "            continue\n",
    "        sent_end = sent_start + len(sentence)\n",
    "        current_pos = sent_end\n",
    "        \n",
    "        # Find all links whose position falls within this sentence\n",
    "        links_in_sent = []\n",
    "        for link in sorted_links:\n",
    "            link_start = link.get('start_idx', -1)\n",
    "            link_end = link.get('end_idx', -1)\n",
    "            \n",
    "            # Check if link position falls within sentence boundaries\n",
    "            if sent_start <= link_start < sent_end:\n",
    "                links_in_sent.append({\n",
    "                    'anchor': link.get('anchor', ''),\n",
    "                    'href_decoded': link.get('href_decoded', ''),\n",
    "                    'href_raw': link.get('href_raw', ''),\n",
    "                    'position': link_start\n",
    "                })\n",
    "        \n",
    "        if links_in_sent:\n",
    "            results.append({\n",
    "                'sentence': sentence.strip(),\n",
    "                'links_in_sentence': links_in_sent,\n",
    "                'num_links': len(links_in_sent),\n",
    "                'start_pos': sent_start,\n",
    "                'end_pos': sent_end\n",
    "            })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faee200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Creating test dataset with position-based matching...\n",
      "   Sampling 1,000 articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2698231/569278588.py:147: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n",
      "  .collect(streaming=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loaded 1,000 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:04<00:00, 200.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Created test dataset:\n",
      "   Sentences: 51956\n",
      "   Articles processed: 1000\n",
      "   Total ground truth links: 117577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import re\n",
    "# from typing import List, Dict, Tuple\n",
    "# from urllib.parse import unquote\n",
    "\n",
    "# def create_test_dataset_optimized(\n",
    "#     df_lazy: pl.LazyFrame,\n",
    "#     url_to_id: Dict[str, int],\n",
    "#     max_sentences: int = 100,\n",
    "#     min_links_per_sentence: int = 1,\n",
    "#     sample_articles: int = 5000\n",
    "# ) -> List[Dict]:\n",
    "#     \"\"\"\n",
    "#     Create test dataset using LazyFrame streaming.\n",
    "#     \"\"\"\n",
    "#     print(f\"ðŸ” Creating test dataset (max {max_sentences} sentences)...\")\n",
    "#     print(f\"   Sampling {sample_articles:,} articles from LazyFrame...\")\n",
    "    \n",
    "#     sampled_df = (\n",
    "#         df_lazy\n",
    "#         .filter(pl.col('link_count') > 0)\n",
    "#         .filter(pl.col('text_withoutHref').is_not_null())\n",
    "#         .filter(pl.col('text_withoutHref').str.len_chars() > 100)\n",
    "#         .select(['id', 'title', 'text_withoutHref', 'links'])\n",
    "#         .head(sample_articles * 3)\n",
    "#         .collect(streaming=True)\n",
    "#         .sample(n=min(sample_articles, sample_articles * 3), shuffle=True, seed=42)\n",
    "#     )\n",
    "    \n",
    "#     print(f\"   Loaded {len(sampled_df):,} articles into memory\")\n",
    "    \n",
    "#     test_data = []\n",
    "#     articles_processed = 0\n",
    "    \n",
    "#     for row in tqdm(sampled_df.iter_rows(named=True), total=len(sampled_df), desc=\"Processing\"):\n",
    "#         if len(test_data) >= max_sentences:\n",
    "#             break\n",
    "        \n",
    "#         article_id = row[\"id\"]\n",
    "#         article_title = row[\"title\"]\n",
    "#         text = row.get(\"text_withoutHref\", \"\")\n",
    "#         links_raw = row.get(\"links\", [])\n",
    "        \n",
    "#         if links_raw is None:\n",
    "#             continue\n",
    "#         links = list(links_raw) if hasattr(links_raw, '__iter__') else []\n",
    "        \n",
    "#         if not text or len(links) == 0:\n",
    "#             continue\n",
    "        \n",
    "#         # Use anchor-based matching instead of position-based\n",
    "#         sentences_with_links = extract_sentences_with_links_by_anchor(text, links)\n",
    "        \n",
    "#         for sent_data in sentences_with_links:\n",
    "#             if len(test_data) >= max_sentences:\n",
    "#                 break\n",
    "            \n",
    "#             sentence = sent_data[\"sentence\"]\n",
    "#             links_in_sent = sent_data[\"links_in_sentence\"]\n",
    "            \n",
    "#             # Verify anchor is actually in sentence\n",
    "#             ground_truth_links = []\n",
    "#             for link in links_in_sent:\n",
    "#                 href_decoded = link[\"href_decoded\"]\n",
    "#                 anchor = link[\"anchor\"]\n",
    "                \n",
    "#                 # Double-check anchor is in sentence\n",
    "#                 if anchor.lower() not in sentence.lower():\n",
    "#                     continue\n",
    "                \n",
    "#                 if not anchor or not anchor.strip():\n",
    "#                     continue\n",
    "                \n",
    "#                 target_id = url_to_id.get(href_decoded)\n",
    "#                 if not target_id:\n",
    "#                     for variation in [\n",
    "#                         href_decoded.replace(\"_\", \" \"),\n",
    "#                         href_decoded.replace(\"%20\", \" \"),\n",
    "#                         href_decoded.lower(),\n",
    "#                         href_decoded.lower().replace(\"_\", \" \")\n",
    "#                     ]:\n",
    "#                         target_id = url_to_id.get(variation)\n",
    "#                         if target_id:\n",
    "#                             break\n",
    "                \n",
    "#                 if target_id and target_id != article_id:\n",
    "#                     ground_truth_links.append({\n",
    "#                         'anchor': anchor,\n",
    "#                         'target_id': target_id,\n",
    "#                         'href_decoded': href_decoded\n",
    "#                     })\n",
    "            \n",
    "#             if len(ground_truth_links) >= min_links_per_sentence:\n",
    "#                 test_data.append({\n",
    "#                     'source_article_id': article_id,\n",
    "#                     'source_article_title': article_title,\n",
    "#                     'sentence': sentence,\n",
    "#                     'ground_truth_links': ground_truth_links,\n",
    "#                     'num_ground_truth': len(ground_truth_links)\n",
    "#                 })\n",
    "        \n",
    "#         articles_processed += 1\n",
    "    \n",
    "#     del sampled_df\n",
    "#     import gc\n",
    "#     gc.collect()\n",
    "    \n",
    "#     print(f\"\\nâœ… Created test dataset:\")\n",
    "#     print(f\"   Sentences: {len(test_data)}\")\n",
    "#     print(f\"   Articles processed: {articles_processed}\")\n",
    "#     total_gt_links = sum(t['num_ground_truth'] for t in test_data)\n",
    "#     print(f\"   Total ground truth links: {total_gt_links}\")\n",
    "    \n",
    "#     return test_data\n",
    "\n",
    "# # Create test dataset - pass LazyFrame, not collected DataFrame\n",
    "# test_data = create_test_dataset_optimized(\n",
    "#     df_lazy,  # LazyFrame, not df\n",
    "#     url_to_id, \n",
    "#     max_sentences=1000000,\n",
    "#     min_links_per_sentence=20,\n",
    "#     sample_articles=1000  # Only load 5K articles into memory\n",
    "# )\n",
    "\n",
    "\n",
    "def create_test_dataset_with_position_matching(\n",
    "    df_lazy: pl.LazyFrame,\n",
    "    url_to_id: Dict[str, int],\n",
    "    max_sentences: int = 100,\n",
    "    min_links_per_sentence: int = 1,\n",
    "    sample_articles: int = 5000\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create test dataset using position-based link matching from raw text.\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” Creating test dataset with position-based matching...\")\n",
    "    print(f\"   Sampling {sample_articles:,} articles...\")\n",
    "    \n",
    "    # Sample articles and process text column\n",
    "    sampled_df = (\n",
    "        df_lazy\n",
    "        .filter(pl.col('link_count') > 0)\n",
    "        .filter(pl.col('text').is_not_null())\n",
    "        .filter(pl.col('text').str.len_chars() > 100)\n",
    "        .select(['id', 'title', 'text'])\n",
    "        .head(sample_articles * 3)\n",
    "        .collect(streaming=True)\n",
    "        .sample(n=min(sample_articles, sample_articles * 3), shuffle=True, seed=42)\n",
    "    )\n",
    "    \n",
    "    print(f\"   Loaded {len(sampled_df):,} articles\")\n",
    "    \n",
    "    test_data = []\n",
    "    articles_processed = 0\n",
    "    \n",
    "    for row in tqdm(sampled_df.iter_rows(named=True), total=len(sampled_df), desc=\"Processing\"):\n",
    "        if len(test_data) >= max_sentences:\n",
    "            break\n",
    "        \n",
    "        article_id = row[\"id\"]\n",
    "        article_title = row[\"title\"]\n",
    "        raw_text = row.get(\"text\", \"\")\n",
    "        \n",
    "        if not raw_text:\n",
    "            continue\n",
    "        \n",
    "        # Extract links and get clean text\n",
    "        clean_text, extracted_links = extract_links_and_clean_text(raw_text)\n",
    "        \n",
    "        if not clean_text or len(extracted_links) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Extract sentences with position-based link matching\n",
    "        sentences_with_links = extract_sentences_with_links_by_position(clean_text, extracted_links)\n",
    "        \n",
    "        for sent_data in sentences_with_links:\n",
    "            if len(test_data) >= max_sentences:\n",
    "                break\n",
    "            \n",
    "            sentence = sent_data[\"sentence\"]\n",
    "            links_in_sent = sent_data[\"links_in_sentence\"]\n",
    "            \n",
    "            # Map links to article IDs\n",
    "            ground_truth_links = []\n",
    "            for link in links_in_sent:\n",
    "                href_decoded = link[\"href_decoded\"]\n",
    "                anchor = link[\"anchor\"]\n",
    "                \n",
    "                if not anchor or not anchor.strip():\n",
    "                    continue\n",
    "                \n",
    "                target_id = url_to_id.get(href_decoded)\n",
    "                if not target_id:\n",
    "                    for variation in [\n",
    "                        href_decoded.replace(\"_\", \" \"),\n",
    "                        href_decoded.replace(\"%20\", \" \"),\n",
    "                        href_decoded.lower(),\n",
    "                        href_decoded.lower().replace(\"_\", \" \")\n",
    "                    ]:\n",
    "                        target_id = url_to_id.get(variation)\n",
    "                        if target_id:\n",
    "                            break\n",
    "                \n",
    "                if target_id and target_id != article_id:\n",
    "                    ground_truth_links.append({\n",
    "                        'anchor': anchor,\n",
    "                        'target_id': target_id,\n",
    "                        'href_decoded': href_decoded\n",
    "                    })\n",
    "            \n",
    "            if len(ground_truth_links) >= min_links_per_sentence:\n",
    "                test_data.append({\n",
    "                    'source_article_id': article_id,\n",
    "                    'source_article_title': article_title,\n",
    "                    'sentence': sentence,\n",
    "                    'ground_truth_links': ground_truth_links,\n",
    "                    'num_ground_truth': len(ground_truth_links)\n",
    "                })\n",
    "        \n",
    "        articles_processed += 1\n",
    "    \n",
    "    print(f\"\\nâœ… Created test dataset:\")\n",
    "    print(f\"   Sentences: {len(test_data)}\")\n",
    "    print(f\"   Articles processed: {articles_processed}\")\n",
    "    total_gt_links = sum(t['num_ground_truth'] for t in test_data)\n",
    "    print(f\"   Total ground truth links: {total_gt_links}\")\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "\n",
    "# Create test dataset with position-based matching\n",
    "test_data = create_test_dataset_with_position_matching(\n",
    "    df_lazy,\n",
    "    url_to_id, \n",
    "    max_sentences=10000000,\n",
    "    min_links_per_sentence=1,\n",
    "    sample_articles=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dd64c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”® Predicting links for 51956 sentences...\n",
      "   Encoding sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1624/1624 [04:26<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Querying Qdrant...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51956/51956 [18:31<00:00, 46.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Generated predictions for 51956 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_links_for_sentences(\n",
    "    client,\n",
    "    model,\n",
    "    test_data: List[Dict],\n",
    "    collection_name: str = \"wikipedia_fr\",\n",
    "    top_k: int = 10,\n",
    "    min_similarity: float = 0.5\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Predict links by finding similar articles in Qdrant.\n",
    "    \n",
    "    Approach: Encode sentence â†’ Find similar articles â†’ Suggest those articles as links\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”® Predicting links for {len(test_data)} sentences...\")\n",
    "    \n",
    "    # Extract all sentences\n",
    "    sentences = [t['sentence'] for t in test_data]\n",
    "    source_ids = [t['source_article_id'] for t in test_data]\n",
    "    \n",
    "    # Encode all sentences\n",
    "    print(\"   Encoding sentences...\")\n",
    "    embeddings = model.encode(\n",
    "        sentences,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    print(\"   Querying Qdrant...\")\n",
    "    for i, (sentence, embedding, source_id) in enumerate(tqdm(\n",
    "        zip(sentences, embeddings, source_ids), \n",
    "        total=len(sentences)\n",
    "    )):\n",
    "        # Search for similar articles\n",
    "        search_results = client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            query=embedding.tolist(),\n",
    "            limit=top_k + 1,  # Extra to account for self-match\n",
    "            score_threshold=min_similarity\n",
    "        ).points\n",
    "        \n",
    "        # Filter out source article and collect predictions\n",
    "        predicted_articles = []\n",
    "        for result in search_results:\n",
    "            article_id = result.payload.get('id')\n",
    "            \n",
    "            # Skip self-match\n",
    "            if article_id == source_id:\n",
    "                continue\n",
    "            \n",
    "            predicted_articles.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': result.payload.get('title', f'ID {article_id}'),\n",
    "                'similarity_score': result.score,\n",
    "                'text_preview': result.payload.get('text_withoutHref', '')[:150] + '...'\n",
    "            })\n",
    "        \n",
    "        # Limit to top_k after filtering\n",
    "        predicted_articles = predicted_articles[:top_k]\n",
    "        \n",
    "        predictions.append({\n",
    "            **test_data[i],\n",
    "            'predicted_articles': predicted_articles,\n",
    "            'num_predictions': len(predicted_articles)\n",
    "        })\n",
    "    \n",
    "    print(f\"âœ… Generated predictions for {len(predictions)} sentences\")\n",
    "    return predictions\n",
    "\n",
    "# Generate predictions\n",
    "predictions = predict_links_for_sentences(\n",
    "    client,\n",
    "    model,\n",
    "    test_data,\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    top_k=20,\n",
    "    min_similarity=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09b13458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Š EVALUATION RESULTS (Recall@K)\n",
      "============================================================\n",
      "\n",
      "ðŸ“ˆ Recall@K Metrics:\n",
      "   Recall@ 1: 0.093 (10,870/116,791)\n",
      "   Recall@ 5: 0.205 (23,929/116,791)\n",
      "   Recall@10: 0.258 (30,181/116,791)\n",
      "   Recall@20: 0.313 (36,539/116,791)\n",
      "\n",
      "ðŸ“‹ Dataset Stats:\n",
      "   Test sentences: 51956\n",
      "   Total ground truth links: 116,791\n",
      "\n",
      "ðŸ“Š Mean Reciprocal Rank (MRR): 0.145\n"
     ]
    }
   ],
   "source": [
    "def evaluate_predictions_recall_at_k(predictions: List[Dict], k_values: List[int] = [1, 5, 10, 20]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate predictions using Recall@K metrics.\n",
    "    \n",
    "    Recall@K = What fraction of ground truth links appear in the top-K predictions?\n",
    "    \"\"\"\n",
    "    # Initialize counters for each K\n",
    "    recall_at_k = {k: {'hits': 0, 'total': 0} for k in k_values}\n",
    "    \n",
    "    results_per_sentence = []\n",
    "    \n",
    "    for pred in predictions:\n",
    "        sentence_lower = pred['sentence'].lower()\n",
    "        \n",
    "        # Filter ground truth to only anchors actually in this sentence\n",
    "        filtered_gt = [link for link in pred['ground_truth_links']\n",
    "                       if link['anchor'].lower() in sentence_lower]\n",
    "        \n",
    "        gt_ids = set(link['target_id'] for link in filtered_gt)\n",
    "        \n",
    "        # Get predicted IDs in ranked order\n",
    "        pred_ids_ranked = [p['article_id'] for p in pred['predicted_articles']]\n",
    "        \n",
    "        # Calculate Recall@K for each K value\n",
    "        sentence_recall_at_k = {}\n",
    "        for k in k_values:\n",
    "            top_k_preds = set(pred_ids_ranked[:k])\n",
    "            hits = len(gt_ids & top_k_preds)\n",
    "            sentence_recall_at_k[k] = hits / len(gt_ids) if gt_ids else 0\n",
    "            \n",
    "            # Accumulate for overall metrics\n",
    "            recall_at_k[k]['hits'] += hits\n",
    "            recall_at_k[k]['total'] += len(gt_ids)\n",
    "        \n",
    "        results_per_sentence.append({\n",
    "            **pred,\n",
    "            'ground_truth_links': filtered_gt,\n",
    "            'num_ground_truth': len(filtered_gt),\n",
    "            'recall_at_k': sentence_recall_at_k\n",
    "        })\n",
    "    \n",
    "    # Calculate overall Recall@K\n",
    "    overall_recall_at_k = {}\n",
    "    for k in k_values:\n",
    "        if recall_at_k[k]['total'] > 0:\n",
    "            overall_recall_at_k[k] = recall_at_k[k]['hits'] / recall_at_k[k]['total']\n",
    "        else:\n",
    "            overall_recall_at_k[k] = 0\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š EVALUATION RESULTS (Recall@K)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nðŸ“ˆ Recall@K Metrics:\")\n",
    "    for k in k_values:\n",
    "        hits = recall_at_k[k]['hits']\n",
    "        total = recall_at_k[k]['total']\n",
    "        recall = overall_recall_at_k[k]\n",
    "        print(f\"   Recall@{k:2d}: {recall:.3f} ({hits:,}/{total:,})\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Dataset Stats:\")\n",
    "    print(f\"   Test sentences: {len(predictions)}\")\n",
    "    total_gt = recall_at_k[k_values[0]]['total']\n",
    "    print(f\"   Total ground truth links: {total_gt:,}\")\n",
    "    \n",
    "    # Also compute MRR (Mean Reciprocal Rank)\n",
    "    mrr_sum = 0\n",
    "    mrr_count = 0\n",
    "    for pred in predictions:\n",
    "        gt_ids = set(link['target_id'] for link in pred.get('ground_truth_links', [])\n",
    "                     if link['anchor'].lower() in pred['sentence'].lower())\n",
    "        pred_ids_ranked = [p['article_id'] for p in pred['predicted_articles']]\n",
    "        \n",
    "        for gt_id in gt_ids:\n",
    "            mrr_count += 1\n",
    "            if gt_id in pred_ids_ranked:\n",
    "                rank = pred_ids_ranked.index(gt_id) + 1\n",
    "                mrr_sum += 1.0 / rank\n",
    "    \n",
    "    mrr = mrr_sum / mrr_count if mrr_count > 0 else 0\n",
    "    print(f\"\\nðŸ“Š Mean Reciprocal Rank (MRR): {mrr:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'recall_at_k': overall_recall_at_k,\n",
    "        'mrr': mrr,\n",
    "        'results_per_sentence': results_per_sentence\n",
    "    }\n",
    "\n",
    "# Evaluate with Recall@K\n",
    "eval_results = evaluate_predictions_recall_at_k(predictions, k_values=[1, 5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ecaef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ BEST PREDICTIONS (Highest Recall@5)\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 1: Article 'Les Verts (France)'\n",
      "Sentence: 'En novembre de la mÃªme annÃ©e, le Mouvement Ã©cologique est fondÃ©.'\n",
      "Recall@1: 0.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'Mouvement Ã©cologique' â†’ Verts, Parti Ã©cologiste (rank: 3)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Novembre 1771 (sim: 0.837)\n",
      "   [âœ—] #2 AnnÃ©es 1970 en France (sim: 0.835)\n",
      "   [âœ“] #3 Verts, Parti Ã©cologiste (sim: 0.835)\n",
      "   [âœ—] #4 Novembre 1772 (sim: 0.832)\n",
      "   [âœ—] #5 Novembre 1735 (sim: 0.832)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 2: Article 'Les Verts (France)'\n",
      "Sentence: 'Le , d'autres Ã©cologistes issus du RÃ©seau des amis de la Terre (RAT) fondent Ã  BesanÃ§on la ConfÃ©dÃ©ration Ecologiste, qui est renommÃ© Les Verts - ConfÃ©dÃ©ration Ecologiste le .'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'RÃ©seau des amis de la Terre' â†’ RÃ©seau des amis de la Terre (rank: 1)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 RÃ©seau des amis de la Terre (sim: 0.881)\n",
      "   [âœ—] #2 Les Verts (Espagne) (sim: 0.850)\n",
      "   [âœ—] #3 Verts, Parti Ã©cologiste (sim: 0.847)\n",
      "   [âœ—] #4 Alliance des Ã©cologistes congolais â€“ Les Verts (sim: 0.839)\n",
      "   [âœ—] #5 SecrÃ©taire national des Ã‰cologistes (sim: 0.839)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 3: Article 'Les Verts (France)'\n",
      "Sentence: 'L'aile gauche des Verts participe alors Ã  la rÃ©flexion engagÃ©e au sein de la FÃ©dÃ©ration pour une gauche alternative (FGA).'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'FÃ©dÃ©ration pour une gauche alternative' â†’ FÃ©dÃ©ration pour une gauche alternative (rank: 1)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 FÃ©dÃ©ration pour une gauche alternative (sim: 0.848)\n",
      "   [âœ—] #2 Alternative verte zougoise (sim: 0.844)\n",
      "   [âœ—] #3 Parti dÃ©mocratique jordanien de la gauche (sim: 0.839)\n",
      "   [âœ—] #4 Verte (sim: 0.836)\n",
      "   [âœ—] #5 Nouvelle Gauche (homonymie) (sim: 0.834)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 4: Article 'Les Verts (France)'\n",
      "Sentence: 'Ã€ Paris, les Ã©lections sont marquÃ©es par l'Ã©lection de Jean-Louis Vidal, premier conseiller municipal Vert de l'histoire (remplacÃ© en 1992, dans le cadre de l'application du tourniquet, par Jean-FranÃ§ois SÃ©gard) avec 17,05 % des suffrages au second tour dans le XIVe arrondissement.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'Ã€ Paris' â†’ Ã‰lections municipales de 1989 Ã  Paris (rank: 1)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 Ã‰lections municipales de 1989 Ã  Paris (sim: 0.915)\n",
      "   [âœ—] #2 Ã‰lections municipales de 2008 Ã  Limoges (sim: 0.857)\n",
      "   [âœ—] #3 Liste des maires d'Arles (sim: 0.854)\n",
      "   [âœ—] #4 QuinziÃ¨me circonscription de Paris de 1958 Ã  1986 (sim: 0.852)\n",
      "   [âœ—] #5 Ã‰lections lÃ©gislatives de 2002 Ã  Paris (sim: 0.851)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 5: Article 'Les Verts (France)'\n",
      "Sentence: 'Pour le rÃ©fÃ©rendum sur le traitÃ© de Maastricht, qui se tient la mÃªme annÃ©e, le parti est divisÃ© en deux camps et nâ€™officialise pas de positionnement en faveur du Â« oui Â» ou du Â« non Â».'\n",
      "Recall@1: 0.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'rÃ©fÃ©rendum sur le traitÃ© de Maastricht' â†’ RÃ©fÃ©rendum franÃ§ais sur le traitÃ© de Maastricht (rank: 2)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 RÃ©fÃ©rendum nÃ©erlandais sur le traitÃ© Ã©tablissant une Constitution pour l'Europe (sim: 0.848)\n",
      "   [âœ“] #2 RÃ©fÃ©rendum franÃ§ais sur le traitÃ© de Maastricht (sim: 0.844)\n",
      "   [âœ—] #3 RÃ©fÃ©rendum sur le traitÃ© de Maastricht (sim: 0.838)\n",
      "   [âœ—] #4 RÃ©fÃ©rendum liechtensteinois de 1970 (sim: 0.838)\n",
      "   [âœ—] #5 RÃ©fÃ©rendums irlandais de 2012 (sim: 0.836)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 6: Article 'Les Verts (France)'\n",
      "Sentence: 'Ã€ Lyon, quatre adjoints sont membres des Verts (Gilles Buna, Ã‰tienne TÃªte).'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'Ã‰tienne TÃªte' â†’ Ã‰tienne TÃªte (rank: 1)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 Ã‰tienne TÃªte (sim: 0.843)\n",
      "   [âœ—] #2 Liste des sÃ©nateurs des Bouches-du-RhÃ´ne (sim: 0.836)\n",
      "   [âœ—] #3 Guy Bono (sim: 0.834)\n",
      "   [âœ—] #4 Liste des voies du 4e arrondissement de Lyon (sim: 0.826)\n",
      "   [âœ—] #5 Liste des sÃ©nateurs de l'IsÃ¨re (sim: 0.826)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 7: Article 'Les Verts (France)'\n",
      "Sentence: 'En janvier 2003, Gilles Lemaire devient secrÃ©taire national, succÃ©dant Ã  Dominique Voynet.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'Gilles Lemaire' â†’ Gilles Lemaire (rank: 1)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 Gilles Lemaire (sim: 0.851)\n",
      "   [âœ—] #2 Liste des sÃ©nateurs belges (lÃ©gislature 2003-2007) (sim: 0.831)\n",
      "   [âœ—] #3 2023 en sociologie (sim: 0.830)\n",
      "   [âœ—] #4 2023 en cyclisme (sim: 0.829)\n",
      "   [âœ—] #5 2023 en rugby Ã  XIII (sim: 0.829)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 8: Article 'Les Verts (France)'\n",
      "Sentence: 'Certaines personnalitÃ©s comme Marie-HÃ©lÃ¨ne Aubert n'interviennent dans les mÃ©dias quasiment que pour critiquer la direction et Dominique Voynet s'attaque mÃªme publiquement Ã  la lÃ©gitimitÃ© de certaines listes autonomes des Verts lors des rÃ©gionales de 2004.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'Marie-HÃ©lÃ¨ne Aubert' â†’ Marie-HÃ©lÃ¨ne Aubert (rank: 1)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 Marie-HÃ©lÃ¨ne Aubert (sim: 0.830)\n",
      "   [âœ—] #2 Dominique Voynet (sim: 0.826)\n",
      "   [âœ—] #3 Chronologie de la loi DADVSI (sim: 0.822)\n",
      "   [âœ—] #4 Ã‰lections lÃ©gislatives de 2022 dans la Haute-Vienne (sim: 0.821)\n",
      "   [âœ—] #5 Ã‰lections lÃ©gislatives de 2007 en Auvergne (sim: 0.818)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 9: Article 'Les Verts (France)'\n",
      "Sentence: 'Ainsi Denis Baupin reste adjoint au maire de Paris mais avec une dÃ©lÃ©gation de moindre importance : adjoint aux transports de 2001 Ã  2008, il devient adjoint chargÃ© du dÃ©veloppement durable, de l'environnement et du plan climat.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'Denis Baupin' â†’ Denis Baupin (rank: 1)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 Denis Baupin (sim: 0.880)\n",
      "   [âœ—] #2 Liste des conseillers de Paris (2008-2014) (sim: 0.863)\n",
      "   [âœ—] #3 Liste des conseillers de Paris (1995-2001) (sim: 0.855)\n",
      "   [âœ—] #4 RÃ©seau Vert (sim: 0.846)\n",
      "   [âœ—] #5 Adel Ziane (sim: 0.844)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 10: Article 'Les Verts (France)'\n",
      "Sentence: 'AprÃ¨s les bons rÃ©sultats Ã©lectoraux obtenus par le rassemblement Europe Ã‰cologie, Daniel Cohn-Bendit appelle, au lendemain des Ã©lections rÃ©gionales de 2010, Ã  la dissolution des Verts au sein d'Â« une nouvelle formation politique Ã  inventer Â».'\n",
      "Recall@1: 0.50 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   [âœ“] 'Europe Ã‰cologie' â†’ Europe Ã‰cologie (rank: 1)\n",
      "   [âœ“] 'Daniel Cohn-Bendit' â†’ Daniel Cohn-Bendit (rank: 3)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 Europe Ã‰cologie (sim: 0.875)\n",
      "   [âœ—] #2 Les Ã‰cologistes (sim: 0.866)\n",
      "   [âœ“] #3 Daniel Cohn-Bendit (sim: 0.857)\n",
      "   [âœ—] #4 Manifestations de 2024 contre l'extrÃªme droite en France (sim: 0.851)\n",
      "   [âœ—] #5 Ã‰cologie SolidaritÃ© (sim: 0.850)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 11: Article 'Les Verts (France)'\n",
      "Sentence: 'Les Verts pensent que l'Ã©conomie non marchande (associations, SELs, etc.) joue un rÃ´le essentiel dans la sociÃ©tÃ© qu'il convient de favoriser, par exemple en rÃ©duisant le temps de travail.'\n",
      "Recall@1: 0.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'SELs' â†’ SystÃ¨me d'Ã©change local (rank: 2)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Ã‰conomie sociale marchande (sim: 0.834)\n",
      "   [âœ“] #2 SystÃ¨me d'Ã©change local (sim: 0.825)\n",
      "   [âœ—] #3 Associations de travailleurs espÃ©rantophones (sim: 0.824)\n",
      "   [âœ—] #4 Ã‰quiterre (sim: 0.822)\n",
      "   [âœ—] #5 Les Verts (Espagne) (sim: 0.822)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 12: Article 'Les Verts (France)'\n",
      "Sentence: 'Les Verts critiquent en particulier :\n",
      "D'un point de vue gÃ©nÃ©ral, les Verts militent pour anticiper la transition (qu'ils pensent de toute faÃ§on inÃ©luctable) de notre modÃ¨le de production Ã©nergÃ©tique vers le tout renouvelable.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'transition' â†’ Transition Ã©nergÃ©tique (rank: 1)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 Transition Ã©nergÃ©tique (sim: 0.839)\n",
      "   [âœ—] #2 Technologie verte (sim: 0.830)\n",
      "   [âœ—] #3 Ã‰lectrification des usages fossiles (sim: 0.830)\n",
      "   [âœ—] #4 Liste des partis verts (sim: 0.829)\n",
      "   [âœ—] #5 Transition juste (sim: 0.828)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 13: Article 'Les Verts (France)'\n",
      "Sentence: 'Les Verts ont dans le cadre du Collectif national pour une paix juste et durable au Moyen-Orient rejoint l'initiative Â« Boycott, dÃ©sinvestissement et sanctions Â» (BDS) contre IsraÃ«l.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'Boycott, dÃ©sinvestissement et sanctions' â†’ Boycott, dÃ©sinvestissement et sanctions (rank: 1)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 Boycott, dÃ©sinvestissement et sanctions (sim: 0.853)\n",
      "   [âœ—] #2 Boycott, dÃ©sinvestissement et sanctions en France (sim: 0.850)\n",
      "   [âœ—] #3 Femmes pour l'avenir d'IsraÃ«l (sim: 0.837)\n",
      "   [âœ—] #4 HaYerukim (sim: 0.837)\n",
      "   [âœ—] #5 Liste des partis verts (sim: 0.834)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 14: Article 'Les Verts (France)'\n",
      "Sentence: 'Le , les Jeunes Verts ont changÃ© de statuts pour devenir les Jeunes Ã©cologistes et s'inscrire dans la dynamique d'Europe Ã‰cologie.'\n",
      "Recall@1: 0.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'Europe Ã‰cologie' â†’ Europe Ã‰cologie (rank: 3)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Les Jeunes Ã‰cologistes (sim: 0.857)\n",
      "   [âœ—] #2 Jeunes socialistes europÃ©ens (sim: 0.845)\n",
      "   [âœ“] #3 Europe Ã‰cologie (sim: 0.844)\n",
      "   [âœ—] #4 Les Ã‰cologistes (sim: 0.838)\n",
      "   [âœ—] #5 Parti vert europÃ©en (sim: 0.837)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 15: Article 'Boussole (constellation)'\n",
      "Sentence: 'Lacaille a attribuÃ© des dÃ©signations de Bayer Ã  dix Ã©toiles, cataloguÃ©es de Î± (Alpha) Ã  Î» (Lambda) Pyxidis, tout en ignorant les lettres grecques iota et kappa.'\n",
      "Recall@1: 0.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'dÃ©signations de Bayer' â†’ DÃ©signation de Bayer (rank: 5)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Psi Aurigae (sim: 0.860)\n",
      "   [âœ—] #2 Alpha Capricorni (sim: 0.846)\n",
      "   [âœ—] #3 DÃ©signation stellaire (sim: 0.845)\n",
      "   [âœ—] #4 Kappa Ceti (sim: 0.845)\n",
      "   [âœ“] #5 DÃ©signation de Bayer (sim: 0.842)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 16: Article 'Boussole (constellation)'\n",
      "Sentence: 'L'Ã©toile la plus brillante de la constellation est Î± Pyxidis.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'Î± Pyxidis' â†’ Alpha Pyxidis (rank: 1)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 Alpha Pyxidis (sim: 0.870)\n",
      "   [âœ—] #2 Alpha Pictoris (sim: 0.857)\n",
      "   [âœ—] #3 T Pyxidis (sim: 0.855)\n",
      "   [âœ—] #4 AntarÃ¨s (homonymie) (sim: 0.855)\n",
      "   [âœ—] #5 Eta Pyxidis (sim: 0.853)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 17: Article 'Ã‰cole pratique des hautes Ã©tudes'\n",
      "Sentence: 'Son statut de grand Ã©tablissement de type EPSCP lui permet d'admettre ses Ã©tudiants de faÃ§on sÃ©lective.'\n",
      "Recall@1: 0.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   [âœ“] 'grand Ã©tablissement' â†’ Grand Ã©tablissement (rank: 2)\n",
      "   [âœ“] 'EPSCP' â†’ Ã‰tablissement public Ã  caractÃ¨re scientifique, culturel et professionnel (rank: 4)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Magnet school (sim: 0.860)\n",
      "   [âœ“] #2 Grand Ã©tablissement (sim: 0.850)\n",
      "   [âœ—] #3 Liste des Ã©tablissements publics Ã  caractÃ¨re scientifique, culturel et professionnel (sim: 0.831)\n",
      "   [âœ“] #4 Ã‰tablissement public Ã  caractÃ¨re scientifique, culturel et professionnel (sim: 0.827)\n",
      "   [âœ—] #5 Ã‰tablissement public d'enseignement (sim: 0.826)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 18: Article 'Ã‰cole pratique des hautes Ã©tudes'\n",
      "Sentence: 'Elle est aujourd'hui composÃ©e de trois sections : Sciences de la vie et de la terre (SVT), Sciences historiques et philologiques (SHP) et Sciences religieuses (SR) et de quatre instituts (Institut europÃ©en en sciences des religions (IESR), Institut des rÃ©cifs coralliens du Pacifique (IRCP), Institut transdisciplinaire dâ€™Ã©tude du vieillissement (ITEV) et Institut des langues rares (ILARA)).'\n",
      "Recall@1: 0.50 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   [âœ“] 'Institut des rÃ©cifs coralliens du Pacifique' â†’ Institut des rÃ©cifs coralliens du Pacifique (rank: 1)\n",
      "   [âœ“] 'Institut des langues rares' â†’ Institut des langues rares (rank: 4)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 Institut des rÃ©cifs coralliens du Pacifique (sim: 0.891)\n",
      "   [âœ—] #2 Institut pour la science, la sociÃ©tÃ© et la politique (sim: 0.852)\n",
      "   [âœ—] #3 Solartechnik PrÃ¼fung Forschung (sim: 0.848)\n",
      "   [âœ“] #4 Institut des langues rares (sim: 0.847)\n",
      "   [âœ—] #5 Section d'histoire et d'archÃ©ologie de l'IEC (sim: 0.845)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 19: Article 'Ã‰cole pratique des hautes Ã©tudes'\n",
      "Sentence: 'Les directeurs d'Ã©tudes sont en effet, soit des universitaires, soit des professionnels de la recherche dans des instituts privÃ©s et publics.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'directeurs d'Ã©tudes' â†’ Directeur d'Ã©tudes (rank: 1)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 Directeur d'Ã©tudes (sim: 0.849)\n",
      "   [âœ—] #2 Directeur d'Ã©cole (sim: 0.844)\n",
      "   [âœ—] #3 Institut d'Ã©tudes europÃ©ennes (sim: 0.834)\n",
      "   [âœ—] #4 Enseignement privÃ© (sim: 0.828)\n",
      "   [âœ—] #5 Directeur de collection (sim: 0.827)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 20: Article 'Ã‰cole pratique des hautes Ã©tudes'\n",
      "Sentence: 'En 1947, Ã  la suite du dÃ©mantÃ¨lement de lâ€™ELHE, celle-ci se reforme Ã  Paris, en tant que branche de lâ€™EPHE, sous le nom dâ€™Ã‰cole des hautes Ã©tudes en sciences sociales.'\n",
      "Recall@1: 1.00 | Recall@5: 1.00 | Recall@10: 1.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   [âœ“] 'Ã‰cole des hautes Ã©tudes en sciences sociales' â†’ Ã‰cole des hautes Ã©tudes en sciences sociales (rank: 1)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ“] #1 Ã‰cole des hautes Ã©tudes en sciences sociales (sim: 0.883)\n",
      "   [âœ—] #2 Ã‰cole libre des hautes Ã©tudes (sim: 0.858)\n",
      "   [âœ—] #3 PSE - Ã‰cole d'Ã©conomie de Paris (sim: 0.850)\n",
      "   [âœ—] #4 Ã‰cole des hautes Ã©tudes sociales (sim: 0.848)\n",
      "   [âœ—] #5 Haute Ã‰cole en Hainaut (sim: 0.845)\n",
      "\n",
      "================================================================================\n",
      "âŒ WORST PREDICTIONS (Lowest Recall@5, with predictions)\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 1: Article 'Charlemagne'\n",
      "Sentence: 'Au XXÂ siÃ¨cle, en Allemagne, sous le rÃ©gime national-socialiste, Himmler et les SS vitupÃ©rÃ¨rent lâ€™action nÃ©faste de Charlemagne quâ€™ils rendaient responsable de la christianisation des Germains et du massacre des Saxons, reprenant l'image du Â« Boucher des Saxons Â».'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   'Himmler' â†’ Heinrich Himmler (not found)\n",
      "   'Saxons' â†’ Saxons (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Schutzstaffel (sim: 0.845)\n",
      "   [âœ—] #2 Origines de l'antisÃ©mitisme nazi (sim: 0.842)\n",
      "   [âœ—] #3 Massacre de Distomo (sim: 0.842)\n",
      "   [âœ—] #4 Shoah dans les manuels scolaires (sim: 0.841)\n",
      "   [âœ—] #5 Walter Brehmer (sim: 0.840)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 2: Article 'Charlemagne'\n",
      "Sentence: 'De plus, Ã  la fin de la Seconde Guerre mondiale, afin de favoriser le recrutement de volontaires franÃ§ais, le nom de Charlemagne, hÃ©ros et conquÃ©rant revendiquÃ© par les deux nations franÃ§aise et allemande, fut donnÃ© Ã  la division SS dite Division Charlemagne.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'division SS' â†’ Waffen-SS (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 33e division SS Charlemagne (sim: 0.848)\n",
      "   [âœ—] #2 22e division SS Maria Theresia (sim: 0.838)\n",
      "   [âœ—] #3 2e division lÃ©gÃ¨re de chasseurs (sim: 0.837)\n",
      "   [âœ—] #4 2e division SS Das Reich (sim: 0.834)\n",
      "   [âœ—] #5 Armeegruppe Fretter-Pico (sim: 0.831)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 3: Article 'Charlemagne'\n",
      "Sentence: 'La figure de Charlemagne est idÃ©alisÃ©e dans la culture mÃ©diÃ©vale, notamment au travers des chansons de geste, dans lesquelles il fait partie des Neuf Preux.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.50\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   'chansons de geste' â†’ Chanson de geste (not found)\n",
      "   'Neuf Preux' â†’ Neuf Preux (rank: 7)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Liste des saints du IXe siÃ¨cle (sim: 0.836)\n",
      "   [âœ—] #2 Chevalier noir (homonymie) (sim: 0.835)\n",
      "   [âœ—] #3 PiÃ¨ce de 100 francs Charlemagne (sim: 0.833)\n",
      "   [âœ—] #4 PÃ¨lerinage de Charlemagne (sim: 0.831)\n",
      "   [âœ—] #5 KarlamagnÃºs saga (sim: 0.830)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 4: Article 'Charlemagne'\n",
      "Sentence: 'Ã‰ginhard, dans \"Le couronnement de Charlemagne\".'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'Ã‰ginhard' â†’ Ã‰ginhard (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Yoshimura ChÅgi (sim: 0.861)\n",
      "   [âœ—] #2 Les SorciÃ¨res (sim: 0.854)\n",
      "   [âœ—] #3 145 av. J.-C. (sim: 0.845)\n",
      "   [âœ—] #4 Liste des Ã©pisodes de Kenshin le vagabond (sim: 0.845)\n",
      "   [âœ—] #5 Personnages de Genshin Impact (sim: 0.845)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 5: Article 'Charlemagne'\n",
      "Sentence: 'Ces poÃ¨mes ont Ã©tÃ© regroupÃ©s dÃ¨s le Moyen Ã‚ge dans un cycle (ou Â« geste Â») appelÃ© \"cycle du Roi\".'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'cycle du Roi' â†’ LittÃ©rature franÃ§aise du Moyen Ã‚ge (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 La Ceinture de chastetÃ© (sim: 0.840)\n",
      "   [âœ—] #2 Chanson de geste (sim: 0.837)\n",
      "   [âœ—] #3 Chevaliers de la table ronde (cercle littÃ©raire) (sim: 0.834)\n",
      "   [âœ—] #4 Galien le RestorÃ© (sim: 0.833)\n",
      "   [âœ—] #5 Histoire de la poÃ©sie franÃ§aise (sim: 0.833)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 6: Article 'Charlemagne'\n",
      "Sentence: 'Charlemagne est avant tout reprÃ©sentÃ© dans des enluminures, comme l'attestent les \"Grandes Chroniques de France\" dont les thÃ¨mes du couronnement, du roi guerrier et du dÃ©fenseur de la chrÃ©tientÃ© sont les plus fÃ©conds, ou des manuscrits du XVÂ siÃ¨cle, tel celui du \"Miroir des Saxons\", qui voient une multiplication des thÃ¨mes iconographiques.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.33\n",
      "\n",
      "âœ… Ground Truth (3):\n",
      "   'enluminure' â†’ Enluminure (not found)\n",
      "   'Grandes Chroniques de France' â†’ Grandes Chroniques de France (rank: 8)\n",
      "   'Miroir des Saxons' â†’ Miroir des Saxons (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Grandes Chroniques de France de Charles V (sim: 0.853)\n",
      "   [âœ—] #2 Grandes Chroniques de France (Jean Fouquet) (sim: 0.848)\n",
      "   [âœ—] #3 SiÃ¨cle des LumiÃ¨res (sim: 0.847)\n",
      "   [âœ—] #4 Le Couronnement (sim: 0.839)\n",
      "   [âœ—] #5 Liste des images canoniquement couronnÃ©es en France (sim: 0.839)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 7: Article 'Charlemagne'\n",
      "Sentence: 'Quant au terme Â« fleurie Â», il serait en fait une mauvaise traduction du terme Â« flori Â» qui signifie \"blanc\" en ancien franÃ§ais.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'ancien franÃ§ais' â†’ Ancien franÃ§ais (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Chloris (sim: 0.872)\n",
      "   [âœ—] #2 Fresney (sim: 0.861)\n",
      "   [âœ—] #3 ZachariÃ¤ (sim: 0.853)\n",
      "   [âœ—] #4 Greenleaf (sim: 0.848)\n",
      "   [âœ—] #5 Black and White (sim: 0.846)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 8: Article 'Charlemagne'\n",
      "Sentence: 'L'hymne national de la principautÃ© d'Andorre rappelle la lÃ©gende selon laquelle l'Andorre aurait Ã©tÃ© crÃ©Ã©e par Charlemagne.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   'hymne national' â†’ Hymne national (not found)\n",
      "   'principautÃ© d'Andorre' â†’ Andorre (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 El Gran Carlemany (sim: 0.883)\n",
      "   [âœ—] #2 Chronologie d'Andorre (sim: 0.846)\n",
      "   [âœ—] #3 Miss Andorre (sim: 0.840)\n",
      "   [âœ—] #4 Oh Land of Beauty (sim: 0.836)\n",
      "   [âœ—] #5 Asturias, Patria querida (sim: 0.836)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 9: Article 'Charlemagne'\n",
      "Sentence: 'La vie de Charlemagne est antÃ©rieure Ã  l'apparition de l'hÃ©raldique, mais sa notoriÃ©tÃ© lui a valu l'attribution d'armes qui, du fait de l'anachronisme, relÃ¨vent des armoiries imaginaires.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   'hÃ©raldique' â†’ HÃ©raldique (not found)\n",
      "   'armoiries imaginaires' â†’ Armoiries imaginaires (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 ArmÃ© (hÃ©raldique) (sim: 0.829)\n",
      "   [âœ—] #2 Blason (hÃ©raldique) (sim: 0.824)\n",
      "   [âœ—] #3 HÃ©raldique ecclÃ©siastique (sim: 0.823)\n",
      "   [âœ—] #4 Armorial au lion (sim: 0.823)\n",
      "   [âœ—] #5 HÃ©raldique napolÃ©onienne (sim: 0.822)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 10: Article 'Charlemagne'\n",
      "Sentence: 'La premiÃ¨re description de ces armes se trouve dans les \"Enfances Ogier\", composÃ©es vers 1275 par Adenet le Roi, mÃ©nestrel et poÃ¨te Ã  la cour des ducs de Brabant.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'Adenet le Roi' â†’ Adenet le Roi (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Liste des odonates du canton de GenÃ¨ve (sim: 0.852)\n",
      "   [âœ—] #2 Blason de la Dordogne (sim: 0.848)\n",
      "   [âœ—] #3 2015 dans la poÃ©sie (sim: 0.846)\n",
      "   [âœ—] #4 Royal Armouries Ms. I.33 (sim: 0.843)\n",
      "   [âœ—] #5 Livre II des Fables de La Fontaine (sim: 0.842)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 11: Article 'Charlemagne'\n",
      "Sentence: 'Lors de la rÃ©ception solennelle de Charles Quint par , Ã  Paris, le , ces armes furent prÃ©sentÃ©es comme le symbole du rapprochement entre le royaume de France et le Saint-Empire.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   'Charles Quint' â†’ Charles Quint (not found)\n",
      "   'Paris' â†’ Paris (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Anneau royal (sim: 0.845)\n",
      "   [âœ—] #2 Ã‰perons du sacre (sim: 0.843)\n",
      "   [âœ—] #3 Regalia du royaume de France (sim: 0.843)\n",
      "   [âœ—] #4 Cinq-Mars rendant son Ã©pÃ©e Ã  Louis XIII (sim: 0.839)\n",
      "   [âœ—] #5 L'Anneau de l'empereur Charles Quint (sim: 0.838)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 12: Article 'Charlemagne'\n",
      "Sentence: 'HiÃ©rosme de Bara, dans son ouvrage \"Le Blason des Armoiries\", le blasonne ainsi : Â« Party, le premier, moitiÃ© de l'Empire qui est d'or, Ã  une demie aigle esployÃ©e de sable, membrÃ©e &amp; diadesmÃ©e de gueulles ; le deuxiesme de France, qui est d'azur, semÃ© de fleur de lys d'or Â».'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'HiÃ©rosme de Bara' â†’ HiÃ©rosme de Bara (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Armoiries du comtÃ© de Bourgogne (sim: 0.858)\n",
      "   [âœ—] #2 Blason d'Anjou (sim: 0.847)\n",
      "   [âœ—] #3 Jean II de Trazegnies (sim: 0.847)\n",
      "   [âœ—] #4 Armoiries de la France (sim: 0.845)\n",
      "   [âœ—] #5 Armoiries de l'Allemagne (sim: 0.843)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 13: Article 'Charlemagne'\n",
      "Sentence: 'Il Ã©crit que Â« Charlemagne Roy de France et Empereur d'Occident portoit d'azur, Ã  un aigle Ã©ployÃ© d'or, diadÃ©mÃ©, languÃ©, &amp; armÃ© de gueules, l'estomach chargÃ© de l'escu de France, qui estoit d'azur, aux fleurs de lis sans nombre, d'or : &amp; telles armes furent portÃ©es par les empereurs FranÃ§ois ses descendants, iusques Ã  ce que ceux de la maison de Saxe usurpÃ¨rent l'Empire sur les FranÃ§ois, car alors ils changÃ¨rent les Ã©maux anciens de l'Empire, &amp; prirent le mÃ©tal, &amp; la couleur des armes de leur Othon, surnommÃ© le grand, qui portoit selon sa naissance, fascÃ© d'or, &amp; de sable de six piÃ¨ces, blasonnant les armes de l'Empire, d'or &amp; de sable, armÃ©, lampassÃ©, &amp; couronnÃ© d'un diadÃ¨me de gueules.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   'maison de Saxe' â†’ Ottoniens (not found)\n",
      "   'Othon' â†’ Otton Ier (empereur du Saint-Empire) (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Buste de Charlemagne (sim: 0.887)\n",
      "   [âœ—] #2 ChÃ¢sse de Charlemagne (sim: 0.880)\n",
      "   [âœ—] #3 SystÃ¨me monÃ©taire du royaume de France (sim: 0.876)\n",
      "   [âœ—] #4 Armoiries du comtÃ© de Bourgogne (sim: 0.873)\n",
      "   [âœ—] #5 Blason de la Guyenne (sim: 0.873)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 14: Article 'Charlemagne'\n",
      "Sentence: 'Â» On suppose que c'est en partie de ce blasonnement donnÃ© pour Charlemagne que NapolÃ©on s'inspira pour dÃ©finir les armoiries de l'Empire FranÃ§ais dans le dÃ©cret du 21 messidor (10 juillet 1804).'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'Empire FranÃ§ais' â†’ Premier Empire (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Armorial de l'Italie napolÃ©onienne (sim: 0.864)\n",
      "   [âœ—] #2 HÃ©raldique napolÃ©onienne (sim: 0.864)\n",
      "   [âœ—] #3 DÃ©corations crÃ©Ã©es ou reÃ§ues par NapolÃ©on Ier (sim: 0.859)\n",
      "   [âœ—] #4 Armorial des communes de l'Empire (sim: 0.850)\n",
      "   [âœ—] #5 Couronne de NapolÃ©on Ier (sim: 0.850)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 15: Article 'Charlemagne'\n",
      "Sentence: 'En revanche, l'usage toponymique de \"Karl der GroÃŸe\" est assez rare dans les pays germanophones : une \"Karl-der-GroÃŸe-StraÃŸe\" Ã  Barum-St.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'Barum' â†’ Barum (arrondissement de Lunebourg) (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Karl (sim: 0.837)\n",
      "   [âœ—] #2 Rue Karl-Marx (sim: 0.821)\n",
      "   [âœ—] #3 Grande Rue (sim: 0.820)\n",
      "   [âœ—] #4 KarlstraÃŸe (Karlsruhe) (sim: 0.818)\n",
      "   [âœ—] #5 Rue des HÃ©ros (sim: 0.818)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 16: Article 'Charlemagne'\n",
      "Sentence: 'Dionys (Basse-Saxe, district de Lunebourg).'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   'Basse-Saxe' â†’ Basse-Saxe (not found)\n",
      "   'Lunebourg' â†’ Lunebourg (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Arrondissement de Lunebourg (sim: 0.881)\n",
      "   [âœ—] #2 LÃ¼tetsburg (sim: 0.866)\n",
      "   [âœ—] #3 LÃ¼nne (sim: 0.863)\n",
      "   [âœ—] #4 LÃ¼dersburg (sim: 0.862)\n",
      "   [âœ—] #5 Hohnstorf (Elbe) (sim: 0.862)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 17: Article 'Charlemagne'\n",
      "Sentence: 'Ã€ Zurich un \"Zentrum Karl der Grosse\" (graphie suisse avec deux \"s\") sert comme plateforme pour le discours politique et sociÃ©tal.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'Zurich' â†’ Zurich (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Zurich (grande rÃ©gion) (sim: 0.825)\n",
      "   [âœ—] #2 Collegium Carolinum (Zurich) (sim: 0.824)\n",
      "   [âœ—] #3 Centre culturel suisse (sim: 0.823)\n",
      "   [âœ—] #4 Project Censored (sim: 0.817)\n",
      "   [âœ—] #5 Paradeplatz (Zurich) (sim: 0.814)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 18: Article 'Charlemagne'\n",
      "Sentence: 'Charlemagne est incarnÃ© par l'acteur franÃ§ais Christian Brendel.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (1):\n",
      "   'Christian Brendel' â†’ Christian Brendel (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Bruno Clairefond (sim: 0.851)\n",
      "   [âœ—] #2 Jean-Christophe BrÃ©tigniÃ¨re (sim: 0.851)\n",
      "   [âœ—] #3 PiÃ¨ce de 100 francs Charlemagne (sim: 0.850)\n",
      "   [âœ—] #4 Vincent Branchet (sim: 0.849)\n",
      "   [âœ—] #5 Jacques Bertrand (acteur) (sim: 0.848)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 19: Article 'Charlemagne'\n",
      "Sentence: 'L'Ã©mission \"Secrets d'Histoire\" du sur France 2, intitulÃ©e \"SacrÃ© Charlemagne !\", lui Ã©tait consacrÃ©e.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   'Secrets d'Histoire' â†’ Secrets d'Histoire (not found)\n",
      "   'France 2' â†’ France 2 (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 22 av. J.-C. (sim: 0.844)\n",
      "   [âœ—] #2 2 av. J.-C. (sim: 0.843)\n",
      "   [âœ—] #3 212 av. J.-C. (sim: 0.841)\n",
      "   [âœ—] #4 122 av. J.-C. (sim: 0.840)\n",
      "   [âœ—] #5 242 av. J.-C. (sim: 0.840)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Example 20: Article 'Liste de biologistes'\n",
      "Sentence: 'Cette liste, non-exhaustive, rassemble des biologistes, classÃ©s par ordre chronologique de naissance.'\n",
      "Recall@1: 0.00 | Recall@5: 0.00 | Recall@10: 0.00\n",
      "\n",
      "âœ… Ground Truth (2):\n",
      "   'non-exhaustive' â†’ ExhaustivitÃ© (not found)\n",
      "   'biologiste' â†’ Biologiste (not found)\n",
      "\n",
      "ðŸ”® Top 5 Predictions:\n",
      "   [âœ—] #1 Liste de climatologues (sim: 0.890)\n",
      "   [âœ—] #2 Liste de philosophes des sciences (sim: 0.887)\n",
      "   [âœ—] #3 Liste d'auteurs et autrices Ã©cofÃ©ministes (sim: 0.886)\n",
      "   [âœ—] #4 Liste d'Ã©crivains quÃ©bÃ©cois par annÃ©e de naissance (sim: 0.879)\n",
      "   [âœ—] #5 Liste de personnalitÃ©s nÃ©es Ã  New York (sim: 0.874)\n"
     ]
    }
   ],
   "source": [
    "def display_detailed_analysis(eval_results: Dict, num_samples: int = 5, k_display: int = 5):\n",
    "    \"\"\"Show detailed examples of predictions vs ground truth using Recall@K.\"\"\"\n",
    "    \n",
    "    results = eval_results['results_per_sentence']\n",
    "    \n",
    "    # Sort by Recall@k_display to show best and worst\n",
    "    sorted_results = sorted(results, key=lambda x: x['recall_at_k'].get(k_display, 0), reverse=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ðŸŽ¯ BEST PREDICTIONS (Highest Recall@{k_display})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, result in enumerate(sorted_results[:num_samples]):\n",
    "        recall_k = result['recall_at_k'].get(k_display, 0)\n",
    "        if recall_k == 0:\n",
    "            continue\n",
    "        print(f\"\\n{'â”€'*80}\")\n",
    "        print(f\"Example {i+1}: Article '{result['source_article_title']}'\")\n",
    "        print(f\"Sentence: '{result['sentence']}'\")\n",
    "        print(f\"Recall@1: {result['recall_at_k'].get(1, 0):.2f} | Recall@5: {result['recall_at_k'].get(5, 0):.2f} | Recall@10: {result['recall_at_k'].get(10, 0):.2f}\")\n",
    "        \n",
    "        # Get ground truth IDs and predicted IDs\n",
    "        gt_ids = set(gt['target_id'] for gt in result['ground_truth_links'])\n",
    "        pred_ids_ranked = [p['article_id'] for p in result['predicted_articles']]\n",
    "        top_k_pred_ids = set(pred_ids_ranked[:k_display])\n",
    "        \n",
    "        print(f\"\\nâœ… Ground Truth ({len(result['ground_truth_links'])}):\")\n",
    "        for gt in result['ground_truth_links']:\n",
    "            target_title = id_to_title.get(gt['target_id'], f\"ID {gt['target_id']}\")\n",
    "            matched = \"âœ“\" if gt['target_id'] in top_k_pred_ids else \" \"\n",
    "            # Show rank if found\n",
    "            if gt['target_id'] in pred_ids_ranked:\n",
    "                rank = pred_ids_ranked.index(gt['target_id']) + 1\n",
    "                print(f\"   [{matched}] '{gt['anchor']}' â†’ {target_title} (rank: {rank})\")\n",
    "            else:\n",
    "                print(f\"   [{matched}] '{gt['anchor']}' â†’ {target_title} (not in top {len(pred_ids_ranked)})\")\n",
    "        \n",
    "        print(f\"\\nðŸ”® Top {min(k_display, result['num_predictions'])} Predictions:\")\n",
    "        for j, pred in enumerate(result['predicted_articles'][:k_display]):\n",
    "            matched = \"âœ“\" if pred['article_id'] in gt_ids else \"âœ—\"\n",
    "            print(f\"   [{matched}] #{j+1} {pred['article_title']} (sim: {pred['similarity_score']:.3f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"âŒ WORST PREDICTIONS (Lowest Recall@{k_display}, with predictions)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    worst = [r for r in sorted_results if r['num_predictions'] > 0][-num_samples:]\n",
    "    \n",
    "    for i, result in enumerate(worst):\n",
    "        print(f\"\\n{'â”€'*80}\")\n",
    "        print(f\"Example {i+1}: Article '{result['source_article_title']}'\")\n",
    "        print(f\"Sentence: '{result['sentence']}'\")\n",
    "        print(f\"Recall@1: {result['recall_at_k'].get(1, 0):.2f} | Recall@5: {result['recall_at_k'].get(5, 0):.2f} | Recall@10: {result['recall_at_k'].get(10, 0):.2f}\")\n",
    "        \n",
    "        gt_ids = set(gt['target_id'] for gt in result['ground_truth_links'])\n",
    "        pred_ids_ranked = [p['article_id'] for p in result['predicted_articles']]\n",
    "        \n",
    "        print(f\"\\nâœ… Ground Truth ({len(result['ground_truth_links'])}):\")\n",
    "        for gt in result['ground_truth_links']:\n",
    "            target_title = id_to_title.get(gt['target_id'], f\"ID {gt['target_id']}\")\n",
    "            if gt['target_id'] in pred_ids_ranked:\n",
    "                rank = pred_ids_ranked.index(gt['target_id']) + 1\n",
    "                print(f\"   '{gt['anchor']}' â†’ {target_title} (rank: {rank})\")\n",
    "            else:\n",
    "                print(f\"   '{gt['anchor']}' â†’ {target_title} (not found)\")\n",
    "        \n",
    "        print(f\"\\nðŸ”® Top {min(k_display, result['num_predictions'])} Predictions:\")\n",
    "        for j, pred in enumerate(result['predicted_articles'][:k_display]):\n",
    "            matched = \"âœ“\" if pred['article_id'] in gt_ids else \"âœ—\"\n",
    "            print(f\"   [{matched}] #{j+1} {pred['article_title']} (sim: {pred['similarity_score']:.3f})\")\n",
    "\n",
    "# Display analysis\n",
    "display_detailed_analysis(eval_results, num_samples=20, k_display=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8c28985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Š SUMMARY STATISTICS (Recall@K)\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Hit Rate (sentences with â‰¥1 correct in top-K):\n",
      "   Hit@1:  10870/51956 (20.9%)\n",
      "   Hit@5:  19659/51956 (37.8%)\n",
      "   Hit@10: 23119/51956 (44.5%)\n",
      "\n",
      "ðŸ“ˆ Average Per-Sentence Recall@K:\n",
      "   Avg Recall@1:  0.118\n",
      "   Avg Recall@5:  0.239\n",
      "   Avg Recall@10: 0.293\n",
      "   Avg Recall@20: 0.346\n",
      "\n",
      "ðŸ“Š Overall Metrics:\n",
      "   MRR: 0.145\n",
      "   Recall@1: 0.093\n",
      "   Recall@5: 0.205\n",
      "   Recall@10: 0.258\n",
      "   Recall@20: 0.313\n",
      "\n",
      "ðŸ“‹ Prediction Distribution:\n",
      "   Avg predictions per sentence: 20.0\n",
      "   Avg ground truth per sentence: 2.3\n",
      "\n",
      "ðŸ” Coverage:\n",
      "   Sentences with 0 predictions: 0\n",
      "   Sentences with 10+ predictions: 51956\n"
     ]
    }
   ],
   "source": [
    "def print_summary_statistics(eval_results: Dict):\n",
    "    \"\"\"Print summary statistics about the evaluation using Recall@K.\"\"\"\n",
    "    \n",
    "    results = eval_results['results_per_sentence']\n",
    "    \n",
    "    # Count sentences with at least one correct prediction in top-K\n",
    "    sentences_with_hit_at_1 = sum(1 for r in results if r['recall_at_k'].get(1, 0) > 0)\n",
    "    sentences_with_hit_at_5 = sum(1 for r in results if r['recall_at_k'].get(5, 0) > 0)\n",
    "    sentences_with_hit_at_10 = sum(1 for r in results if r['recall_at_k'].get(10, 0) > 0)\n",
    "    \n",
    "    # Average Recall@K per sentence\n",
    "    avg_recall_at_1 = np.mean([r['recall_at_k'].get(1, 0) for r in results])\n",
    "    avg_recall_at_5 = np.mean([r['recall_at_k'].get(5, 0) for r in results])\n",
    "    avg_recall_at_10 = np.mean([r['recall_at_k'].get(10, 0) for r in results])\n",
    "    avg_recall_at_20 = np.mean([r['recall_at_k'].get(20, 0) for r in results])\n",
    "    \n",
    "    # Distribution of predictions\n",
    "    pred_counts = [r['num_predictions'] for r in results]\n",
    "    gt_counts = [r['num_ground_truth'] for r in results]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ“Š SUMMARY STATISTICS (Recall@K)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Hit Rate (sentences with â‰¥1 correct in top-K):\")\n",
    "    print(f\"   Hit@1:  {sentences_with_hit_at_1}/{len(results)} ({100*sentences_with_hit_at_1/len(results):.1f}%)\")\n",
    "    print(f\"   Hit@5:  {sentences_with_hit_at_5}/{len(results)} ({100*sentences_with_hit_at_5/len(results):.1f}%)\")\n",
    "    print(f\"   Hit@10: {sentences_with_hit_at_10}/{len(results)} ({100*sentences_with_hit_at_10/len(results):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Average Per-Sentence Recall@K:\")\n",
    "    print(f\"   Avg Recall@1:  {avg_recall_at_1:.3f}\")\n",
    "    print(f\"   Avg Recall@5:  {avg_recall_at_5:.3f}\")\n",
    "    print(f\"   Avg Recall@10: {avg_recall_at_10:.3f}\")\n",
    "    print(f\"   Avg Recall@20: {avg_recall_at_20:.3f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Overall Metrics:\")\n",
    "    print(f\"   MRR: {eval_results['mrr']:.3f}\")\n",
    "    for k, recall in eval_results['recall_at_k'].items():\n",
    "        print(f\"   Recall@{k}: {recall:.3f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Prediction Distribution:\")\n",
    "    print(f\"   Avg predictions per sentence: {np.mean(pred_counts):.1f}\")\n",
    "    print(f\"   Avg ground truth per sentence: {np.mean(gt_counts):.1f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ” Coverage:\")\n",
    "    print(f\"   Sentences with 0 predictions: {sum(1 for p in pred_counts if p == 0)}\")\n",
    "    print(f\"   Sentences with 10+ predictions: {sum(1 for p in pred_counts if p >= 10)}\")\n",
    "\n",
    "print_summary_statistics(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DCL_WIKI_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
