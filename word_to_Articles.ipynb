{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67daa0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localhome/ltran/.conda/envs/DCL_WIKI_RAG/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "#If you already saved the model locally and are using docker\n",
    "client = QdrantClient(host=\"localhost\", port=6333, prefer_grpc=True, timeout=1000)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"models/multilingual-e5-large\", device=device)\n",
    "\n",
    "# Run docker container before\n",
    "# docker run -d --name qdrant -p 6333:6333 -p 6334:6334 -v /home/Loris/EPFL/MA3/ML/project2/project2Rag/qdrant_storage:/qdrant/storage qdrant/qdrant:latest\n",
    "\n",
    "# VERY IMPORTANT : to query the remote qdrand docker container over ssh, run the following in a terminale before:\n",
    "# ssh -L 6333:localhost:6333 -L 6334:localhost:6334 <your username>@dclgpusrv.epfl.ch -N\n",
    "#Make sure to have your ssh keys before\n",
    "#This forwards the qdrant ports to your local machine\n",
    "#If you are outside EPFL network, use VPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc50e4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Extracting ALL unique href_decoded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2373837/1703159120.py:40: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n",
      "  .collect(streaming=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 1,078,890 unique href_decoded\n",
      "   Representing 51,065,598 links\n",
      "   Min occurrences: 5\n",
      "\n",
      "ðŸ”® Processing 1,078,890 href_decoded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   4%|â–Ž         | 625/16858 [06:19<2:37:41,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 40,000/1,078,890 (3.7%)\n",
      "   ðŸ“ˆ Running accuracy: 37.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   7%|â–‹         | 1250/16858 [12:25<2:29:34,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 80,000/1,078,890 (7.4%)\n",
      "   ðŸ“ˆ Running accuracy: 39.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  11%|â–ˆ         | 1875/16858 [18:22<2:17:34,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 120,000/1,078,890 (11.1%)\n",
      "   ðŸ“ˆ Running accuracy: 40.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  15%|â–ˆâ–        | 2500/16858 [24:13<2:09:03,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 160,000/1,078,890 (14.8%)\n",
      "   ðŸ“ˆ Running accuracy: 41.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  19%|â–ˆâ–Š        | 3125/16858 [30:01<2:07:28,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 200,000/1,078,890 (18.5%)\n",
      "   ðŸ“ˆ Running accuracy: 41.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  22%|â–ˆâ–ˆâ–       | 3750/16858 [35:50<2:02:24,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 240,000/1,078,890 (22.2%)\n",
      "   ðŸ“ˆ Running accuracy: 41.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 4375/16858 [41:46<1:55:10,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 280,000/1,078,890 (26.0%)\n",
      "   ðŸ“ˆ Running accuracy: 42.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  30%|â–ˆâ–ˆâ–‰       | 5000/16858 [47:39<1:48:10,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 320,000/1,078,890 (29.7%)\n",
      "   ðŸ“ˆ Running accuracy: 42.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5625/16858 [53:27<1:42:05,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 360,000/1,078,890 (33.4%)\n",
      "   ðŸ“ˆ Running accuracy: 42.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 6250/16858 [59:15<1:38:25,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 400,000/1,078,890 (37.1%)\n",
      "   ðŸ“ˆ Running accuracy: 42.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6875/16858 [1:05:02<1:30:21,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 440,000/1,078,890 (40.8%)\n",
      "   ðŸ“ˆ Running accuracy: 43.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7500/16858 [1:10:51<1:26:06,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 480,000/1,078,890 (44.5%)\n",
      "   ðŸ“ˆ Running accuracy: 43.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 8125/16858 [1:16:39<1:20:51,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 520,000/1,078,890 (48.2%)\n",
      "   ðŸ“ˆ Running accuracy: 43.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 8750/16858 [1:22:28<1:14:08,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 560,000/1,078,890 (51.9%)\n",
      "   ðŸ“ˆ Running accuracy: 43.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 9375/16858 [1:28:19<1:07:14,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 600,000/1,078,890 (55.6%)\n",
      "   ðŸ“ˆ Running accuracy: 43.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 10000/16858 [1:34:08<1:02:39,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 640,000/1,078,890 (59.3%)\n",
      "   ðŸ“ˆ Running accuracy: 43.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10625/16858 [1:39:54<56:04,  1.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 680,000/1,078,890 (63.0%)\n",
      "   ðŸ“ˆ Running accuracy: 43.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 11250/16858 [1:45:41<51:34,  1.81it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 720,000/1,078,890 (66.7%)\n",
      "   ðŸ“ˆ Running accuracy: 44.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 11875/16858 [1:51:27<44:33,  1.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 760,000/1,078,890 (70.4%)\n",
      "   ðŸ“ˆ Running accuracy: 44.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 12500/16858 [1:57:13<39:18,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 800,000/1,078,890 (74.2%)\n",
      "   ðŸ“ˆ Running accuracy: 44.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 13125/16858 [2:03:02<33:49,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 840,000/1,078,890 (77.9%)\n",
      "   ðŸ“ˆ Running accuracy: 44.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13750/16858 [2:08:49<28:49,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 880,000/1,078,890 (81.6%)\n",
      "   ðŸ“ˆ Running accuracy: 44.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 14375/16858 [2:14:35<22:31,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 920,000/1,078,890 (85.3%)\n",
      "   ðŸ“ˆ Running accuracy: 44.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 15000/16858 [2:20:20<17:12,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 960,000/1,078,890 (89.0%)\n",
      "   ðŸ“ˆ Running accuracy: 44.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 15625/16858 [2:26:06<11:14,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 1,000,000/1,078,890 (92.7%)\n",
      "   ðŸ“ˆ Running accuracy: 44.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 16250/16858 [2:31:55<05:38,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ðŸ“Š Progress: 1,040,000/1,078,890 (96.4%)\n",
      "   ðŸ“ˆ Running accuracy: 44.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16858/16858 [2:37:34<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“Š FULL DATASET EVALUATION RESULTS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“ˆ Unweighted (by unique href):\n",
      "   Total: 1,078,890\n",
      "   âœ… Exact: 477,158 (44.23%)\n",
      "   âŒ Mismatch: 601,732 (55.77%)\n",
      "\n",
      "ðŸ“ˆ Weighted (by link frequency):\n",
      "   Total links: 51,065,598\n",
      "   âœ… Exact: 18,541,916 (36.31%)\n",
      "   âŒ Mismatch: 32,523,682 (63.69%)\n",
      "\n",
      "ðŸ“Š Similarity Distribution:\n",
      "   Mean: 0.8665\n",
      "   Median: 0.8670\n",
      "   Std: 0.0221\n",
      "\n",
      "ðŸ“Š Accuracy by Link Frequency:\n",
      "   Rare (5-100)        :  44.6% (450,672/1,010,996)\n",
      "   Medium (100-1K)     :  39.6% (24,887/62,868)\n",
      "   Common (1K-10K)     :  32.4% (1,547/4,780)\n",
      "   Very Common (10K+)  :  21.1% (52/246)\n",
      "\n",
      "ðŸ’¾ Results saved to full_href_evaluation_2.parquet\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def evaluate_full_dataset(\n",
    "    client,\n",
    "    model,\n",
    "    parquet_path: str = \"articles_fr_merged.parquet\",\n",
    "    collection_name: str = \"wikipedia_fr\",\n",
    "    batch_size: int = 64,\n",
    "    min_link_count: int = 5,  # Only hrefs appearing 5+ times (more reliable)\n",
    "    max_hrefs: int = None,    # None = all hrefs\n",
    "    checkpoint_every: int = 5000\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate on full dataset with checkpointing.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“‚ Extracting ALL unique href_decoded...\")\n",
    "    \n",
    "    # Extract all unique hrefs\n",
    "    unique_hrefs = (\n",
    "        pl.scan_parquet(parquet_path)\n",
    "        .select(['links'])\n",
    "        .filter(pl.col('links').is_not_null())\n",
    "        .with_columns(\n",
    "            pl.col('links').list.eval(\n",
    "                pl.element().struct.field('href_decoded')\n",
    "            ).alias('href_list')\n",
    "        )\n",
    "        .select(['href_list'])\n",
    "        .explode('href_list')\n",
    "        .rename({'href_list': 'href_decoded'})\n",
    "        .filter(pl.col('href_decoded').is_not_null())\n",
    "        .filter(pl.col('href_decoded').str.len_chars() > 1)\n",
    "        .filter(~pl.col('href_decoded').str.starts_with('#'))  # Skip anchor fragments\n",
    "        .group_by('href_decoded')\n",
    "        .agg(pl.len().alias('count'))\n",
    "        .filter(pl.col('count') >= min_link_count)\n",
    "        .sort('count', descending=True)\n",
    "        .collect(streaming=True)\n",
    "    )\n",
    "    \n",
    "    if max_hrefs:\n",
    "        unique_hrefs = unique_hrefs.head(max_hrefs)\n",
    "    \n",
    "    total_hrefs = len(unique_hrefs)\n",
    "    total_links = unique_hrefs['count'].sum()\n",
    "    \n",
    "    print(f\"âœ… Found {total_hrefs:,} unique href_decoded\")\n",
    "    print(f\"   Representing {total_links:,} links\")\n",
    "    print(f\"   Min occurrences: {min_link_count}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    href_list = unique_hrefs['href_decoded'].to_list()\n",
    "    count_list = unique_hrefs['count'].to_list()\n",
    "    \n",
    "    results = []\n",
    "    exact_count = 0\n",
    "    mismatch_count = 0\n",
    "    \n",
    "    print(f\"\\nðŸ”® Processing {total_hrefs:,} href_decoded...\")\n",
    "    \n",
    "    for i in tqdm(range(0, total_hrefs, batch_size), desc=\"Evaluating\"):\n",
    "        batch_hrefs = href_list[i:i + batch_size]\n",
    "        batch_counts = count_list[i:i + batch_size]\n",
    "        \n",
    "        # Batch encode\n",
    "        queries = [f\"query: {href}\" for href in batch_hrefs]\n",
    "        embeddings = model.encode(\n",
    "            queries,\n",
    "            normalize_embeddings=True,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Query Qdrant\n",
    "        for href, count, emb in zip(batch_hrefs, batch_counts, embeddings):\n",
    "            try:\n",
    "                res = client.query_points(\n",
    "                    collection_name=collection_name,\n",
    "                    query=emb.tolist(),\n",
    "                    limit=1,\n",
    "                    score_threshold=0.3\n",
    "                ).points\n",
    "                \n",
    "                if res:\n",
    "                    pred_title = res[0].payload.get('title', '') or ''\n",
    "                    similarity = res[0].score\n",
    "                    is_exact = href.lower().strip() == pred_title.lower().strip()\n",
    "                else:\n",
    "                    pred_title = ''\n",
    "                    similarity = 0.0\n",
    "                    is_exact = False\n",
    "                \n",
    "                results.append({\n",
    "                    'href_decoded': href,\n",
    "                    'count': count,\n",
    "                    'predicted_title': pred_title,\n",
    "                    'similarity': similarity,\n",
    "                    'is_exact': is_exact\n",
    "                })\n",
    "                \n",
    "                if is_exact:\n",
    "                    exact_count += 1\n",
    "                else:\n",
    "                    mismatch_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'href_decoded': href,\n",
    "                    'count': count,\n",
    "                    'predicted_title': '',\n",
    "                    'similarity': 0.0,\n",
    "                    'is_exact': False\n",
    "                })\n",
    "                mismatch_count += 1\n",
    "        \n",
    "        # Checkpoint progress\n",
    "        if (i + batch_size) % checkpoint_every == 0:\n",
    "            processed = min(i + batch_size, total_hrefs)\n",
    "            current_acc = 100 * exact_count / processed\n",
    "            print(f\"\\n   ðŸ“Š Progress: {processed:,}/{total_hrefs:,} ({100*processed/total_hrefs:.1f}%)\")\n",
    "            print(f\"   ðŸ“ˆ Running accuracy: {current_acc:.2f}%\")\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del embeddings\n",
    "        if i % (batch_size * 50) == 0:\n",
    "            gc.collect()\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pl.DataFrame(\n",
    "        results,\n",
    "        schema={\n",
    "            'href_decoded': pl.Utf8,\n",
    "            'count': pl.Int64,\n",
    "            'predicted_title': pl.Utf8,\n",
    "            'similarity': pl.Float64,\n",
    "            'is_exact': pl.Boolean\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Final statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š FULL DATASET EVALUATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_exact = results_df.filter(pl.col('is_exact'))['count'].sum()\n",
    "    total_mismatch = results_df.filter(~pl.col('is_exact'))['count'].sum()\n",
    "    \n",
    "    unweighted_acc = 100 * exact_count / total_hrefs\n",
    "    weighted_acc = 100 * total_exact / total_links\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Unweighted (by unique href):\")\n",
    "    print(f\"   Total: {total_hrefs:,}\")\n",
    "    print(f\"   âœ… Exact: {exact_count:,} ({unweighted_acc:.2f}%)\")\n",
    "    print(f\"   âŒ Mismatch: {mismatch_count:,} ({100-unweighted_acc:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Weighted (by link frequency):\")\n",
    "    print(f\"   Total links: {total_links:,}\")\n",
    "    print(f\"   âœ… Exact: {total_exact:,} ({weighted_acc:.2f}%)\")\n",
    "    print(f\"   âŒ Mismatch: {total_mismatch:,} ({100-weighted_acc:.2f}%)\")\n",
    "    \n",
    "    # Similarity stats\n",
    "    valid_sims = results_df.filter(pl.col('similarity') > 0)['similarity']\n",
    "    print(f\"\\nðŸ“Š Similarity Distribution:\")\n",
    "    print(f\"   Mean: {valid_sims.mean():.4f}\")\n",
    "    print(f\"   Median: {valid_sims.median():.4f}\")\n",
    "    print(f\"   Std: {valid_sims.std():.4f}\")\n",
    "    \n",
    "    # By frequency buckets\n",
    "    print(f\"\\nðŸ“Š Accuracy by Link Frequency:\")\n",
    "    for min_c, max_c, label in [(5, 100, \"Rare (5-100)\"), (100, 1000, \"Medium (100-1K)\"), \n",
    "                                 (1000, 10000, \"Common (1K-10K)\"), (10000, None, \"Very Common (10K+)\")]:\n",
    "        if max_c:\n",
    "            bucket = results_df.filter((pl.col('count') >= min_c) & (pl.col('count') < max_c))\n",
    "        else:\n",
    "            bucket = results_df.filter(pl.col('count') >= min_c)\n",
    "        \n",
    "        if len(bucket) > 0:\n",
    "            bucket_exact = bucket.filter(pl.col('is_exact')).height\n",
    "            bucket_acc = 100 * bucket_exact / len(bucket)\n",
    "            print(f\"   {label:20}: {bucket_acc:>5.1f}% ({bucket_exact:,}/{len(bucket):,})\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Run full evaluation (adjust max_hrefs if needed)\n",
    "full_results = evaluate_full_dataset(\n",
    "    client,\n",
    "    model,\n",
    "    parquet_path=\"articles_fr_merged.parquet\",\n",
    "    collection_name=\"wikipedia_fr\",\n",
    "    batch_size=64,\n",
    "    min_link_count=5,      # Only hrefs appearing 5+ times\n",
    "    max_hrefs=None,       # Start with 50K, increase to None for full\n",
    "    checkpoint_every=10000\n",
    ")\n",
    "\n",
    "# Save results\n",
    "full_results.write_parquet(\"full_href_evaluation_2.parquet\")\n",
    "print(f\"\\nðŸ’¾ Results saved to full_href_evaluation_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b139ec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading results...\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š 1. OVERALL STATISTICS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ˆ Unweighted (by unique href_decoded):\n",
      "   Total unique hrefs: 1,078,890\n",
      "   âœ… Exact matches:   477,158 (44.23%)\n",
      "   âŒ Mismatches:      601,732 (55.77%)\n",
      "\n",
      "ðŸ“ˆ Weighted (by link frequency):\n",
      "   Total links: 51,065,598\n",
      "   âœ… Exact matches:   18,541,916 (36.31%)\n",
      "   âŒ Mismatches:      32,523,682 (63.69%)\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š 2. SIMILARITY SCORE DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Overall (n=1,078,890):\n",
      "   Mean:   0.8665\n",
      "   Median: 0.8670\n",
      "   Std:    0.0221\n",
      "   Min:    0.7642\n",
      "   Max:    0.9606\n",
      "\n",
      "ðŸ“Š By Match Type:\n",
      "   Exact matches:\n",
      "      Mean: 0.8743, Median: 0.8739\n",
      "   Mismatches:\n",
      "      Mean: 0.8603, Median: 0.8597\n",
      "\n",
      "ðŸ“Š Similarity Histogram:\n",
      "   [0.75-0.80):    274 (  2.6% exact) \n",
      "   [0.80-0.85): 257,450 ( 17.7% exact) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   [0.85-0.90): 753,056 ( 51.6% exact) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   [0.90-0.95): 68,094 ( 63.2% exact) â–ˆâ–ˆâ–ˆ\n",
      "   [0.95-1.00):     16 ( 43.8% exact) \n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š 3. ACCURACY BY LINK FREQUENCY\n",
      "================================================================================\n",
      "\n",
      "Frequency Bucket             Total    Exact    Acc %        Links\n",
      "-----------------------------------------------------------------\n",
      "Very Rare (5-10)           461,546  206,642    44.8%    3,027,399\n",
      "Rare (10-50)               480,068  214,569    44.7%    9,755,772\n",
      "Uncommon (50-100)           69,382   29,461    42.5%    4,784,369\n",
      "Medium (100-500)            56,359   22,640    40.2%   11,239,602\n",
      "Common (500-1K)              6,509    2,247    34.5%    4,499,675\n",
      "Frequent (1K-5K)             4,361    1,425    32.7%    8,518,968\n",
      "Very Frequent (5K-10K)         419      122    29.1%    2,884,658\n",
      "High Frequency (10K-50K)       225       48    21.3%    4,423,219\n",
      "Extremely High (50K+)           21        4    19.0%    1,931,936\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š 4. TOP EXACT MATCHES (by frequency)\n",
      "================================================================================\n",
      "\n",
      "href_decoded                                sim      count\n",
      "------------------------------------------------------------\n",
      "Allemagne                                 0.861     89,684\n",
      "MÃ©tÃ©o-France                              0.862     67,412\n",
      "Angleterre                                0.881     57,622\n",
      "Londres                                   0.869     55,039\n",
      "Japon                                     0.859     49,474\n",
      "pÃ©rihÃ©lie                                 0.880     43,977\n",
      "Ã©cliptique                                0.876     41,467\n",
      "excentricitÃ© orbitale                     0.893     40,855\n",
      "Union internationale pour la conservati   0.876     38,050\n",
      "Rome                                      0.846     37,013\n",
      "base de donnÃ©es                           0.881     34,790\n",
      "Institut national de l'information gÃ©og   0.866     33,814\n",
      "amplitude thermique                       0.869     33,597\n",
      "Climat de la France                       0.878     33,424\n",
      "orthodromie                               0.871     33,359\n",
      "normale climatique                        0.884     32,238\n",
      "SuÃ¨de                                     0.877     28,898\n",
      "Araneae                                   0.868     25,056\n",
      "latin                                     0.858     23,940\n",
      "magnitude absolue                         0.884     22,775\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š 5. TOP MISMATCHES (by frequency - most impactful errors)\n",
      "================================================================================\n",
      "\n",
      "href_decoded                   predicted                        sim      count\n",
      "--------------------------------------------------------------------------------\n",
      "Ã‰tats-Unis                     Zwartemeer                      0.86    280,764\n",
      "France                         Frankreich                      0.86    255,746\n",
      "Paris                          Paris (Arkansas)                0.86    134,006\n",
      "espÃ¨ce                         Cynorkis exilis                 0.86    104,776\n",
      "Seconde Guerre mondiale        Octobre 1939 (guerre mondiale   0.88     90,193\n",
      "Famille (biologie)             Linaria                         0.88     82,311\n",
      "ceinture d'astÃ©roÃ¯des          Ceinture d'astÃ©roÃ¯des de HD 6   0.89     78,752\n",
      "Pologne                        PÄ…tnÃ³w                          0.91     75,903\n",
      "Canada                         Kennebec                        0.87     72,077\n",
      "Royaume-Uni                    Berkley                         0.89     69,159\n",
      "Europe                         Europe au Ier millÃ©naire av.    0.85     67,881\n",
      "Espagne                        Liste des pays ayant l'espagn   0.86     63,412\n",
      "Belgique                       Buzet                           0.87     61,709\n",
      "Italie                         Histoire de l'Italie            0.85     61,549\n",
      "PremiÃ¨re Guerre mondiale       Pays impliquÃ©s dans la PremiÃ¨   0.87     57,205\n",
      "football                       American Football               0.86     53,514\n",
      "New York                       Ã‰tat de New York                0.85     53,222\n",
      "dÃ©partement franÃ§ais           Directoire du dÃ©partement       0.86     44,388\n",
      "astÃ©roÃ¯de                      (1218) Aster                    0.89     43,680\n",
      "Russie                         Kozielsk                        0.88     42,749\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š 6. MISMATCH PATTERN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Mismatch Categories (sampled from first 10K):\n",
      "   Total mismatches: 601,732\n",
      "   Partial matches (substring): ~3,790\n",
      "\n",
      "ðŸ“Š Partial Match Examples:\n",
      "   'Paris' â†’ 'Paris (Arkansas)' (n=134,006)\n",
      "   'ceinture d'astÃ©roÃ¯des' â†’ 'Ceinture d'astÃ©roÃ¯des de HD 69' (n=78,752)\n",
      "   'Europe' â†’ 'Europe au Ier millÃ©naire av. J' (n=67,881)\n",
      "   'Italie' â†’ 'Histoire de l'Italie' (n=61,549)\n",
      "   'PremiÃ¨re Guerre mondiale' â†’ 'Pays impliquÃ©s dans la PremiÃ¨r' (n=57,205)\n",
      "   'football' â†’ 'American Football' (n=53,514)\n",
      "   'New York' â†’ 'Ã‰tat de New York' (n=53,222)\n",
      "   'Californie' â†’ 'GÃ©ographie de la Californie' (n=39,828)\n",
      "   'Chine' â†’ 'Papier de Chine' (n=36,704)\n",
      "   'Australie' â†’ 'FÃ©dÃ©ration australienne' (n=36,176)\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š 7. HIGH SIMILARITY MISMATCHES (sim > 0.88)\n",
      "================================================================================\n",
      "\n",
      "href_decoded                   predicted                        sim    count\n",
      "------------------------------------------------------------------------------\n",
      "Prix de la Banque de SuÃ¨de en  Prix Nobel d'Ã©conomie          0.961      331\n",
      "Coupe du monde de skeleton 20  Coupe du monde de skeleton 20  0.959        5\n",
      "Trois-Villes                   Iruri                          0.959       26\n",
      "prix de la Banque de SuÃ¨de en  Prix Nobel d'Ã©conomie          0.957       75\n",
      "Liste des membres de la 17e K  17e lÃ©gislature de la Knesset  0.955        6\n",
      "Coupe du monde de skeleton 20  Coupe du monde de skeleton 20  0.954        8\n",
      "Chronologie de la numismatiqu  XVIIe siÃ¨cle en numismatique   0.953       46\n",
      "Coupe du monde de skeleton 20  Coupe du monde de skeleton 20  0.953        6\n",
      "MaÃ®tre d'armes                 The Swordsman                  0.950       33\n",
      "Championnats d'Europe d'athlÃ©  RÃ©sultats dÃ©taillÃ©s des champ  0.950        7\n",
      "Coupe du monde de bobsleigh 2  Coupe du monde de bobsleigh 2  0.946        5\n",
      "VTT cross-country masculin au  Cross-country VTT masculin au  0.945       10\n",
      "Championnats d'Europe d'athlÃ©  RÃ©sultats dÃ©taillÃ©s des champ  0.945       21\n",
      "Championnats d'Europe d'athlÃ©  RÃ©sultats dÃ©taillÃ©s des champ  0.944       93\n",
      "respiration de Cheyne-Stokes   Cheynes-Stokes                 0.944        5\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š 8. LOW SIMILARITY EXACT MATCHES (sim < 0.85)\n",
      "================================================================================\n",
      "\n",
      "href_decoded                                          sim    count\n",
      "--------------------------------------------------------------------\n",
      "Ever Fallen in Love (With Someone You Shouldn't'v   0.790        6\n",
      "Mark Hellinger Theatre                              0.790       15\n",
      "Glasgow High School Former Pupils                   0.791        5\n",
      "All-Star Batman and Robin, the Boy Wonder           0.792        5\n",
      "D. and W. Henderson and Company                     0.794       11\n",
      "John, I'm Only Dancing                              0.795        7\n",
      "I Ain't Gonna Stand for It                          0.796        5\n",
      "Sweet Dreams (Are Made of This)                     0.800       23\n",
      "His Band and the Street Choir                       0.800        5\n",
      "On First Looking into Chapman's Homer               0.801        6\n",
      "Aleksander Aamodt Kilde                             0.801       72\n",
      "Familiar to Millions                                0.804        8\n",
      "Gimme! Gimme! Gimme! (A Man After Midnight)         0.804       16\n",
      "The Stars (Are Out Tonight)                         0.804        6\n",
      "Balai Pustaka                                       0.805        6\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š 9. SUMMARY\n",
      "================================================================================\n",
      "\n",
      "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "    â”‚  SEMANTIC SIMILARITY LINK PREDICTION EVALUATION                 â”‚\n",
      "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "    â”‚  Total unique href_decoded:   1,078,890                        â”‚\n",
      "    â”‚  Total links represented:    51,065,598                        â”‚\n",
      "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "    â”‚  UNWEIGHTED ACCURACY:             44.23%                       â”‚\n",
      "    â”‚  WEIGHTED ACCURACY:               36.31%                       â”‚\n",
      "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "    â”‚  Mean similarity (exact):        0.8743                        â”‚\n",
      "    â”‚  Mean similarity (mismatch):     0.8603                        â”‚\n",
      "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "def display_detailed_results(parquet_path: str = \"full_href_evaluation.parquet\"):\n",
    "    \"\"\"\n",
    "    Load and display comprehensive results from saved evaluation.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“‚ Loading results...\")\n",
    "    df = pl.read_parquet(parquet_path)\n",
    "    \n",
    "    total_hrefs = len(df)\n",
    "    total_links = df['count'].sum()\n",
    "    exact_df = df.filter(pl.col('is_exact'))\n",
    "    mismatch_df = df.filter(~pl.col('is_exact'))\n",
    "    \n",
    "    # ============================================================\n",
    "    # 1. OVERALL STATISTICS\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š 1. OVERALL STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    exact_count = exact_df.height\n",
    "    mismatch_count = mismatch_df.height\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Unweighted (by unique href_decoded):\")\n",
    "    print(f\"   Total unique hrefs: {total_hrefs:,}\")\n",
    "    print(f\"   âœ… Exact matches:   {exact_count:,} ({100*exact_count/total_hrefs:.2f}%)\")\n",
    "    print(f\"   âŒ Mismatches:      {mismatch_count:,} ({100*mismatch_count/total_hrefs:.2f}%)\")\n",
    "    \n",
    "    exact_links = exact_df['count'].sum()\n",
    "    mismatch_links = mismatch_df['count'].sum()\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Weighted (by link frequency):\")\n",
    "    print(f\"   Total links: {total_links:,}\")\n",
    "    print(f\"   âœ… Exact matches:   {exact_links:,} ({100*exact_links/total_links:.2f}%)\")\n",
    "    print(f\"   âŒ Mismatches:      {mismatch_links:,} ({100*mismatch_links/total_links:.2f}%)\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 2. SIMILARITY DISTRIBUTION\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š 2. SIMILARITY SCORE DISTRIBUTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    valid_df = df.filter(pl.col('similarity') > 0)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Overall (n={len(valid_df):,}):\")\n",
    "    print(f\"   Mean:   {valid_df['similarity'].mean():.4f}\")\n",
    "    print(f\"   Median: {valid_df['similarity'].median():.4f}\")\n",
    "    print(f\"   Std:    {valid_df['similarity'].std():.4f}\")\n",
    "    print(f\"   Min:    {valid_df['similarity'].min():.4f}\")\n",
    "    print(f\"   Max:    {valid_df['similarity'].max():.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š By Match Type:\")\n",
    "    for label, subset in [(\"Exact matches\", exact_df), (\"Mismatches\", mismatch_df)]:\n",
    "        subset_valid = subset.filter(pl.col('similarity') > 0)\n",
    "        if len(subset_valid) > 0:\n",
    "            print(f\"   {label}:\")\n",
    "            print(f\"      Mean: {subset_valid['similarity'].mean():.4f}, Median: {subset_valid['similarity'].median():.4f}\")\n",
    "    \n",
    "    # Similarity histogram\n",
    "    print(f\"\\nðŸ“Š Similarity Histogram:\")\n",
    "    for low, high in [(0.7, 0.75), (0.75, 0.8), (0.8, 0.85), (0.85, 0.9), (0.9, 0.95), (0.95, 1.0)]:\n",
    "        bucket = valid_df.filter((pl.col('similarity') >= low) & (pl.col('similarity') < high))\n",
    "        exact_in_bucket = bucket.filter(pl.col('is_exact')).height\n",
    "        total_in_bucket = bucket.height\n",
    "        if total_in_bucket > 0:\n",
    "            acc = 100 * exact_in_bucket / total_in_bucket\n",
    "            bar = \"â–ˆ\" * int(total_in_bucket / len(valid_df) * 50)\n",
    "            print(f\"   [{low:.2f}-{high:.2f}): {total_in_bucket:>6,} ({acc:>5.1f}% exact) {bar}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 3. ACCURACY BY FREQUENCY BUCKET\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š 3. ACCURACY BY LINK FREQUENCY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    buckets = [\n",
    "        (5, 10, \"Very Rare (5-10)\"),\n",
    "        (10, 50, \"Rare (10-50)\"),\n",
    "        (50, 100, \"Uncommon (50-100)\"),\n",
    "        (100, 500, \"Medium (100-500)\"),\n",
    "        (500, 1000, \"Common (500-1K)\"),\n",
    "        (1000, 5000, \"Frequent (1K-5K)\"),\n",
    "        (5000, 10000, \"Very Frequent (5K-10K)\"),\n",
    "        (10000, 50000, \"High Frequency (10K-50K)\"),\n",
    "        (50000, None, \"Extremely High (50K+)\")\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'Frequency Bucket':<25} {'Total':>8} {'Exact':>8} {'Acc %':>8} {'Links':>12}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for min_c, max_c, label in buckets:\n",
    "        if max_c:\n",
    "            bucket = df.filter((pl.col('count') >= min_c) & (pl.col('count') < max_c))\n",
    "        else:\n",
    "            bucket = df.filter(pl.col('count') >= min_c)\n",
    "        \n",
    "        if bucket.height > 0:\n",
    "            bucket_exact = bucket.filter(pl.col('is_exact')).height\n",
    "            bucket_acc = 100 * bucket_exact / bucket.height\n",
    "            bucket_links = bucket['count'].sum()\n",
    "            print(f\"{label:<25} {bucket.height:>8,} {bucket_exact:>8,} {bucket_acc:>7.1f}% {bucket_links:>12,}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 4. TOP EXACT MATCHES\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š 4. TOP EXACT MATCHES (by frequency)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    top_exact = exact_df.sort('count', descending=True).head(20)\n",
    "    print(f\"\\n{'href_decoded':<40} {'sim':>6} {'count':>10}\")\n",
    "    print(\"-\" * 60)\n",
    "    for row in top_exact.iter_rows(named=True):\n",
    "        print(f\"{row['href_decoded'][:39]:<40} {row['similarity']:>6.3f} {row['count']:>10,}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 5. TOP MISMATCHES (Most Impactful Errors)\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š 5. TOP MISMATCHES (by frequency - most impactful errors)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    top_mismatch = mismatch_df.sort('count', descending=True).head(20)\n",
    "    print(f\"\\n{'href_decoded':<30} {'predicted':<30} {'sim':>5} {'count':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for row in top_mismatch.iter_rows(named=True):\n",
    "        href = row['href_decoded'][:29]\n",
    "        pred = row['predicted_title'][:29] if row['predicted_title'] else 'N/A'\n",
    "        print(f\"{href:<30} {pred:<30} {row['similarity']:>5.2f} {row['count']:>10,}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 6. MISMATCH ANALYSIS (FIXED - no regex)\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š 6. MISMATCH PATTERN ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    mismatch_with_pred = mismatch_df.filter(pl.col('predicted_title') != '')\n",
    "    \n",
    "    # Count partial matches manually (avoid regex issues)\n",
    "    partial_count = 0\n",
    "    partial_examples = []\n",
    "    \n",
    "    for row in mismatch_with_pred.head(10000).iter_rows(named=True):  # Sample for speed\n",
    "        href_lower = row['href_decoded'].lower()\n",
    "        pred_lower = row['predicted_title'].lower() if row['predicted_title'] else ''\n",
    "        \n",
    "        if href_lower in pred_lower or pred_lower in href_lower:\n",
    "            partial_count += 1\n",
    "            if len(partial_examples) < 10:\n",
    "                partial_examples.append(row)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Mismatch Categories (sampled from first 10K):\")\n",
    "    print(f\"   Total mismatches: {mismatch_count:,}\")\n",
    "    print(f\"   Partial matches (substring): ~{partial_count:,}\")\n",
    "    \n",
    "    if partial_examples:\n",
    "        print(f\"\\nðŸ“Š Partial Match Examples:\")\n",
    "        for row in partial_examples:\n",
    "            print(f\"   '{row['href_decoded'][:30]}' â†’ '{row['predicted_title'][:30]}' (n={row['count']:,})\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 7. HIGH SIMILARITY MISMATCHES\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š 7. HIGH SIMILARITY MISMATCHES (sim > 0.88)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    high_sim_mismatch = mismatch_df.filter(pl.col('similarity') > 0.88).sort('similarity', descending=True).head(15)\n",
    "    print(f\"\\n{'href_decoded':<30} {'predicted':<30} {'sim':>5} {'count':>8}\")\n",
    "    print(\"-\" * 78)\n",
    "    for row in high_sim_mismatch.iter_rows(named=True):\n",
    "        href = row['href_decoded'][:29]\n",
    "        pred = row['predicted_title'][:29] if row['predicted_title'] else 'N/A'\n",
    "        print(f\"{href:<30} {pred:<30} {row['similarity']:>5.3f} {row['count']:>8,}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 8. LOW SIMILARITY EXACT MATCHES\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š 8. LOW SIMILARITY EXACT MATCHES (sim < 0.85)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    low_sim_exact = exact_df.filter(pl.col('similarity') < 0.85).sort('similarity').head(15)\n",
    "    print(f\"\\n{'href_decoded':<50} {'sim':>6} {'count':>8}\")\n",
    "    print(\"-\" * 68)\n",
    "    for row in low_sim_exact.iter_rows(named=True):\n",
    "        print(f\"{row['href_decoded'][:49]:<50} {row['similarity']:>6.3f} {row['count']:>8,}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # 9. SUMMARY\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š 9. SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    mismatch_sim_mean = mismatch_df.filter(pl.col('similarity') > 0)['similarity'].mean()\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚  SEMANTIC SIMILARITY LINK PREDICTION EVALUATION                 â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚  Total unique href_decoded:  {total_hrefs:>10,}                        â”‚\n",
    "    â”‚  Total links represented:    {total_links:>10,}                        â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚  UNWEIGHTED ACCURACY:        {100*exact_count/total_hrefs:>10.2f}%                       â”‚\n",
    "    â”‚  WEIGHTED ACCURACY:          {100*exact_links/total_links:>10.2f}%                       â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚  Mean similarity (exact):    {exact_df['similarity'].mean():>10.4f}                        â”‚\n",
    "    â”‚  Mean similarity (mismatch): {mismatch_sim_mean:>10.4f}                        â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \"\"\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Load and display results\n",
    "df = display_detailed_results(\"full_href_evaluation.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DCL_WIKI_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
